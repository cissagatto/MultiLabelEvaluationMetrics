<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 14.6.1"/>
    <title>src.evaluation API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note {color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../src.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;src</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="variable" href="#system">system</a>
            </li>
            <li>
                    <a class="variable" href="#current_directory">current_directory</a>
            </li>
            <li>
                    <a class="function" href="#multilabel_label_problem_measures">multilabel_label_problem_measures</a>
            </li>
            <li>
                    <a class="function" href="#multilabel_bipartition_measures">multilabel_bipartition_measures</a>
            </li>
            <li>
                    <a class="function" href="#multilabel_curves_measures">multilabel_curves_measures</a>
            </li>
            <li>
                    <a class="function" href="#multilabel_ranking_measures">multilabel_ranking_measures</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
<a href="./../src.html">src</a><wbr>.evaluation    </h1>

                
                        <input id="mod-evaluation-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-evaluation-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos">  1</span></a><span class="c1">##############################################################################</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos">  2</span></a><span class="c1"># Copyright (C) 2024                                                         #</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos">  3</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos">  4</span></a><span class="c1"># CC BY-NC-SA 4.0                                                            #</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos">  5</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos">  6</span></a><span class="c1"># Canonical URL https://creativecommons.org/licenses/by-nc-sa/4.0/           #</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos">  7</span></a><span class="c1"># Attribution-NonCommercial-ShareAlike 4.0 International CC BY-NC-SA 4.0     #</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos">  8</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos">  9</span></a><span class="c1"># Prof. Elaine Cecilia Gatto | Prof. Ricardo Cerri | Prof. Mauri Ferrandin   #</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos"> 10</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos"> 11</span></a><span class="c1"># Federal University of São Carlos - UFSCar - https://www2.ufscar.br         #</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos"> 12</span></a><span class="c1"># Campus São Carlos - Computer Department - DC - https://site.dc.ufscar.br   #</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos"> 13</span></a><span class="c1"># Post Graduate Program in Computer Science - PPGCC                          # </span>
</span><span id="L-14"><a href="#L-14"><span class="linenos"> 14</span></a><span class="c1"># http://ppgcc.dc.ufscar.br - Bioinformatics and Machine Learning Group      #</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos"> 15</span></a><span class="c1"># BIOMAL - http://www.biomal.ufscar.br                                       #</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos"> 16</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos"> 17</span></a><span class="c1"># You are free to:                                                           #</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos"> 18</span></a><span class="c1">#     Share — copy and redistribute the material in any medium or format     #</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos"> 19</span></a><span class="c1">#     Adapt — remix, transform, and build upon the material                  #</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos"> 20</span></a><span class="c1">#     The licensor cannot revoke these freedoms as long as you follow the    #</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos"> 21</span></a><span class="c1">#       license terms.                                                       #</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos"> 22</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos"> 23</span></a><span class="c1"># Under the following terms:                                                 #</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos"> 24</span></a><span class="c1">#   Attribution — You must give appropriate credit , provide a link to the   #</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos"> 25</span></a><span class="c1">#     license, and indicate if changes were made . You may do so in any      #</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos"> 26</span></a><span class="c1">#     reasonable manner, but not in any way that suggests the licensor       #</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos"> 27</span></a><span class="c1">#     endorses you or your use.                                              #</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos"> 28</span></a><span class="c1">#   NonCommercial — You may not use the material for commercial purposes     #</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos"> 29</span></a><span class="c1">#   ShareAlike — If you remix, transform, or build upon the material, you    #</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos"> 30</span></a><span class="c1">#     must distribute your contributions under the same license as the       #</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos"> 31</span></a><span class="c1">#     original.                                                              #</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos"> 32</span></a><span class="c1">#   No additional restrictions — You may not apply legal terms or            #</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos"> 33</span></a><span class="c1">#     technological measures that legally restrict others from doing         #</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos"> 34</span></a><span class="c1">#     anything the license permits.                                          #</span>
</span><span id="L-35"><a href="#L-35"><span class="linenos"> 35</span></a><span class="c1">#                                                                            #</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos"> 36</span></a><span class="c1">##############################################################################</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos"> 37</span></a>
</span><span id="L-38"><a href="#L-38"><span class="linenos"> 38</span></a>
</span><span id="L-39"><a href="#L-39"><span class="linenos"> 39</span></a>
</span><span id="L-40"><a href="#L-40"><span class="linenos"> 40</span></a>
</span><span id="L-41"><a href="#L-41"><span class="linenos"> 41</span></a>
</span><span id="L-42"><a href="#L-42"><span class="linenos"> 42</span></a><span class="c1">########################################################################</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos"> 43</span></a><span class="c1">#                                                                      #</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos"> 44</span></a><span class="c1">########################################################################</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos"> 45</span></a><span class="kn">import</span> <span class="nn">sys</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos"> 46</span></a><span class="kn">import</span> <span class="nn">platform</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos"> 47</span></a><span class="kn">import</span> <span class="nn">os</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos"> 48</span></a>
</span><span id="L-49"><a href="#L-49"><span class="linenos"> 49</span></a><span class="n">system</span> <span class="o">=</span> <span class="n">platform</span><span class="o">.</span><span class="n">system</span><span class="p">()</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos"> 50</span></a><span class="k">if</span> <span class="n">system</span> <span class="o">==</span> <span class="s1">&#39;Windows&#39;</span><span class="p">:</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos"> 51</span></a>    <span class="n">user_profile</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;USERPROFILE&#39;</span><span class="p">]</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos"> 52</span></a>    <span class="n">FolderRoot</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">user_profile</span><span class="p">,</span> <span class="s1">&#39;Documents&#39;</span><span class="p">,</span> <span class="s1">&#39;MultiLabelEvaluationMetrics&#39;</span><span class="p">,</span> <span class="s1">&#39;src&#39;</span><span class="p">)</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos"> 53</span></a><span class="k">elif</span> <span class="n">system</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;Linux&#39;</span><span class="p">,</span> <span class="s1">&#39;Darwin&#39;</span><span class="p">]:</span>  <span class="c1"># &#39;Darwin&#39; is the system name for macOS</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos"> 54</span></a>    <span class="n">FolderRoot</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/MultiLabelEvaluationMetrics/src&#39;</span><span class="p">)</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos"> 55</span></a><span class="k">else</span><span class="p">:</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos"> 56</span></a>    <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Unsupported operating system&#39;</span><span class="p">)</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos"> 57</span></a>
</span><span id="L-58"><a href="#L-58"><span class="linenos"> 58</span></a><span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">FolderRoot</span><span class="p">)</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos"> 59</span></a><span class="n">current_directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos"> 60</span></a><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">)</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos"> 61</span></a>
</span><span id="L-62"><a href="#L-62"><span class="linenos"> 62</span></a>
</span><span id="L-63"><a href="#L-63"><span class="linenos"> 63</span></a><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos"> 64</span></a><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos"> 65</span></a><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-66"><a href="#L-66"><span class="linenos"> 66</span></a>    <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">hamming_loss</span><span class="p">,</span> <span class="n">zero_one_loss</span><span class="p">,</span>
</span><span id="L-67"><a href="#L-67"><span class="linenos"> 67</span></a>    <span class="n">average_precision_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span>
</span><span id="L-68"><a href="#L-68"><span class="linenos"> 68</span></a>    <span class="n">recall_score</span><span class="p">,</span> <span class="n">jaccard_score</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">precision_recall_curve</span><span class="p">,</span>
</span><span id="L-69"><a href="#L-69"><span class="linenos"> 69</span></a>    <span class="n">precision_recall_fscore_support</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">coverage_error</span><span class="p">,</span> 
</span><span id="L-70"><a href="#L-70"><span class="linenos"> 70</span></a>    <span class="n">label_ranking_loss</span>
</span><span id="L-71"><a href="#L-71"><span class="linenos"> 71</span></a><span class="p">)</span>
</span><span id="L-72"><a href="#L-72"><span class="linenos"> 72</span></a>
</span><span id="L-73"><a href="#L-73"><span class="linenos"> 73</span></a><span class="kn">import</span> <span class="nn">confusion_matrix</span> <span class="k">as</span> <span class="nn">cm</span>
</span><span id="L-74"><a href="#L-74"><span class="linenos"> 74</span></a><span class="kn">import</span> <span class="nn">measures</span> <span class="k">as</span> <span class="nn">ms</span>
</span><span id="L-75"><a href="#L-75"><span class="linenos"> 75</span></a>
</span><span id="L-76"><a href="#L-76"><span class="linenos"> 76</span></a>
</span><span id="L-77"><a href="#L-77"><span class="linenos"> 77</span></a>
</span><span id="L-78"><a href="#L-78"><span class="linenos"> 78</span></a><span class="c1">########################################################################</span>
</span><span id="L-79"><a href="#L-79"><span class="linenos"> 79</span></a><span class="c1">#                                                                      #</span>
</span><span id="L-80"><a href="#L-80"><span class="linenos"> 80</span></a><span class="c1">########################################################################</span>
</span><span id="L-81"><a href="#L-81"><span class="linenos"> 81</span></a><span class="k">def</span> <span class="nf">multilabel_label_problem_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="L-82"><a href="#L-82"><span class="linenos"> 82</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-83"><a href="#L-83"><span class="linenos"> 83</span></a><span class="sd">    Calculates measures for label prediction problems in multi-label classification.</span>
</span><span id="L-84"><a href="#L-84"><span class="linenos"> 84</span></a>
</span><span id="L-85"><a href="#L-85"><span class="linenos"> 85</span></a><span class="sd">    Parameters:</span>
</span><span id="L-86"><a href="#L-86"><span class="linenos"> 86</span></a><span class="sd">    ----------</span>
</span><span id="L-87"><a href="#L-87"><span class="linenos"> 87</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="L-88"><a href="#L-88"><span class="linenos"> 88</span></a><span class="sd">    pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</span>
</span><span id="L-89"><a href="#L-89"><span class="linenos"> 89</span></a>
</span><span id="L-90"><a href="#L-90"><span class="linenos"> 90</span></a><span class="sd">    Returns:</span>
</span><span id="L-91"><a href="#L-91"><span class="linenos"> 91</span></a><span class="sd">    -------</span>
</span><span id="L-92"><a href="#L-92"><span class="linenos"> 92</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="L-93"><a href="#L-93"><span class="linenos"> 93</span></a><span class="sd">        A DataFrame containing all the calculated metrics.</span>
</span><span id="L-94"><a href="#L-94"><span class="linenos"> 94</span></a>
</span><span id="L-95"><a href="#L-95"><span class="linenos"> 95</span></a><span class="sd">    Metrics Calculated:</span>
</span><span id="L-96"><a href="#L-96"><span class="linenos"> 96</span></a><span class="sd">    -------------------</span>
</span><span id="L-97"><a href="#L-97"><span class="linenos"> 97</span></a><span class="sd">    - Constant Label Problem (CLP)</span>
</span><span id="L-98"><a href="#L-98"><span class="linenos"> 98</span></a><span class="sd">    - Wrong Label Problem (WLP)</span>
</span><span id="L-99"><a href="#L-99"><span class="linenos"> 99</span></a><span class="sd">    - Missing Label Problem (MLP)</span>
</span><span id="L-100"><a href="#L-100"><span class="linenos">100</span></a>
</span><span id="L-101"><a href="#L-101"><span class="linenos">101</span></a><span class="sd">    Interpretation:</span>
</span><span id="L-102"><a href="#L-102"><span class="linenos">102</span></a><span class="sd">    ----------</span>
</span><span id="L-103"><a href="#L-103"><span class="linenos">103</span></a><span class="sd">    1. **Wrong Label Problem (WLP)**</span>
</span><span id="L-104"><a href="#L-104"><span class="linenos">104</span></a><span class="sd">        Definition: Measures the number of labels that are predicted but should not be. The ideal value is zero.        </span>
</span><span id="L-105"><a href="#L-105"><span class="linenos">105</span></a><span class="sd">        - **Low WLP**: Indicates fewer incorrect predictions of labels.</span>
</span><span id="L-106"><a href="#L-106"><span class="linenos">106</span></a><span class="sd">        - **High WLP**: Indicates that the classifier often predicts incorrect labels.</span>
</span><span id="L-107"><a href="#L-107"><span class="linenos">107</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="L-108"><a href="#L-108"><span class="linenos">108</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="L-109"><a href="#L-109"><span class="linenos">109</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="L-110"><a href="#L-110"><span class="linenos">110</span></a>
</span><span id="L-111"><a href="#L-111"><span class="linenos">111</span></a><span class="sd">    2. **Missing Label Problem (MLP)**</span>
</span><span id="L-112"><a href="#L-112"><span class="linenos">112</span></a><span class="sd">        Definition: Measures the proportion of labels that should have been predicted but were not. The ideal value is zero.        </span>
</span><span id="L-113"><a href="#L-113"><span class="linenos">113</span></a><span class="sd">        - **Low MLP**: Indicates that most of the relevant labels are predicted.</span>
</span><span id="L-114"><a href="#L-114"><span class="linenos">114</span></a><span class="sd">        - **High MLP**: Indicates that many relevant labels are missing in the predictions.</span>
</span><span id="L-115"><a href="#L-115"><span class="linenos">115</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="L-116"><a href="#L-116"><span class="linenos">116</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="L-117"><a href="#L-117"><span class="linenos">117</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="L-118"><a href="#L-118"><span class="linenos">118</span></a>
</span><span id="L-119"><a href="#L-119"><span class="linenos">119</span></a><span class="sd">    3. **Constant Label Problem (CLP)**</span>
</span><span id="L-120"><a href="#L-120"><span class="linenos">120</span></a><span class="sd">        Definition: Measures the occurrence where the same label is predicted for all instances. The ideal value is zero.        </span>
</span><span id="L-121"><a href="#L-121"><span class="linenos">121</span></a><span class="sd">        - **Low CLP**: Indicates that predictions vary and are more closely aligned with true labels.</span>
</span><span id="L-122"><a href="#L-122"><span class="linenos">122</span></a><span class="sd">        - **High CLP**: Indicates that the classifier predicts the same label for all instances.</span>
</span><span id="L-123"><a href="#L-123"><span class="linenos">123</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="L-124"><a href="#L-124"><span class="linenos">124</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="L-125"><a href="#L-125"><span class="linenos">125</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="L-126"><a href="#L-126"><span class="linenos">126</span></a><span class="sd">  </span>
</span><span id="L-127"><a href="#L-127"><span class="linenos">127</span></a>
</span><span id="L-128"><a href="#L-128"><span class="linenos">128</span></a><span class="sd">    Example Usage:</span>
</span><span id="L-129"><a href="#L-129"><span class="linenos">129</span></a><span class="sd">    --------------</span>
</span><span id="L-130"><a href="#L-130"><span class="linenos">130</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_label_problem_measures(true_labels, pred_labels)</span>
</span><span id="L-131"><a href="#L-131"><span class="linenos">131</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="L-132"><a href="#L-132"><span class="linenos">132</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-133"><a href="#L-133"><span class="linenos">133</span></a>
</span><span id="L-134"><a href="#L-134"><span class="linenos">134</span></a>    <span class="n">matrix_confusion</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">mlem_confusion_matrix</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="L-135"><a href="#L-135"><span class="linenos">135</span></a>
</span><span id="L-136"><a href="#L-136"><span class="linenos">136</span></a>    <span class="n">clp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_clp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="L-137"><a href="#L-137"><span class="linenos">137</span></a>    <span class="n">mlp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_mlp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="L-138"><a href="#L-138"><span class="linenos">138</span></a>    <span class="n">wlp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_wlp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="L-139"><a href="#L-139"><span class="linenos">139</span></a>
</span><span id="L-140"><a href="#L-140"><span class="linenos">140</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="L-141"><a href="#L-141"><span class="linenos">141</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>    
</span><span id="L-142"><a href="#L-142"><span class="linenos">142</span></a>        <span class="s1">&#39;clp&#39;</span><span class="p">:</span> <span class="n">clp</span><span class="p">,</span>
</span><span id="L-143"><a href="#L-143"><span class="linenos">143</span></a>        <span class="s1">&#39;mlp&#39;</span><span class="p">:</span> <span class="n">mlp</span><span class="p">,</span>
</span><span id="L-144"><a href="#L-144"><span class="linenos">144</span></a>        <span class="s1">&#39;wlp&#39;</span><span class="p">:</span> <span class="n">wlp</span>
</span><span id="L-145"><a href="#L-145"><span class="linenos">145</span></a>    <span class="p">}</span>
</span><span id="L-146"><a href="#L-146"><span class="linenos">146</span></a>
</span><span id="L-147"><a href="#L-147"><span class="linenos">147</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="L-148"><a href="#L-148"><span class="linenos">148</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="L-149"><a href="#L-149"><span class="linenos">149</span></a>
</span><span id="L-150"><a href="#L-150"><span class="linenos">150</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="L-151"><a href="#L-151"><span class="linenos">151</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="L-152"><a href="#L-152"><span class="linenos">152</span></a>
</span><span id="L-153"><a href="#L-153"><span class="linenos">153</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span><span id="L-154"><a href="#L-154"><span class="linenos">154</span></a>
</span><span id="L-155"><a href="#L-155"><span class="linenos">155</span></a>
</span><span id="L-156"><a href="#L-156"><span class="linenos">156</span></a>
</span><span id="L-157"><a href="#L-157"><span class="linenos">157</span></a><span class="c1">########################################################################</span>
</span><span id="L-158"><a href="#L-158"><span class="linenos">158</span></a><span class="c1">#                                                                      #</span>
</span><span id="L-159"><a href="#L-159"><span class="linenos">159</span></a><span class="c1">########################################################################</span>
</span><span id="L-160"><a href="#L-160"><span class="linenos">160</span></a><span class="k">def</span> <span class="nf">multilabel_bipartition_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="L-161"><a href="#L-161"><span class="linenos">161</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-162"><a href="#L-162"><span class="linenos">162</span></a><span class="sd">    Calculates various evaluation metrics for multi-label classification.</span>
</span><span id="L-163"><a href="#L-163"><span class="linenos">163</span></a>
</span><span id="L-164"><a href="#L-164"><span class="linenos">164</span></a><span class="sd">    Parameters:</span>
</span><span id="L-165"><a href="#L-165"><span class="linenos">165</span></a><span class="sd">    ----------</span>
</span><span id="L-166"><a href="#L-166"><span class="linenos">166</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="L-167"><a href="#L-167"><span class="linenos">167</span></a><span class="sd">    pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</span>
</span><span id="L-168"><a href="#L-168"><span class="linenos">168</span></a>
</span><span id="L-169"><a href="#L-169"><span class="linenos">169</span></a><span class="sd">    Returns:</span>
</span><span id="L-170"><a href="#L-170"><span class="linenos">170</span></a><span class="sd">    -------</span>
</span><span id="L-171"><a href="#L-171"><span class="linenos">171</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="L-172"><a href="#L-172"><span class="linenos">172</span></a><span class="sd">        A DataFrame containing all the calculated metrics.</span>
</span><span id="L-173"><a href="#L-173"><span class="linenos">173</span></a>
</span><span id="L-174"><a href="#L-174"><span class="linenos">174</span></a><span class="sd">    Metrics Calculated:</span>
</span><span id="L-175"><a href="#L-175"><span class="linenos">175</span></a><span class="sd">    -------------------</span>
</span><span id="L-176"><a href="#L-176"><span class="linenos">176</span></a><span class="sd">    - Accuracy</span>
</span><span id="L-177"><a href="#L-177"><span class="linenos">177</span></a><span class="sd">    - Hamming Loss</span>
</span><span id="L-178"><a href="#L-178"><span class="linenos">178</span></a><span class="sd">    - Zero-One Loss</span>
</span><span id="L-179"><a href="#L-179"><span class="linenos">179</span></a><span class="sd">    - F1 Score (macro, micro, weighted, samples)</span>
</span><span id="L-180"><a href="#L-180"><span class="linenos">180</span></a><span class="sd">    - Precision (macro, micro, weighted, samples)</span>
</span><span id="L-181"><a href="#L-181"><span class="linenos">181</span></a><span class="sd">    - Recall (macro, micro, weighted, samples)</span>
</span><span id="L-182"><a href="#L-182"><span class="linenos">182</span></a><span class="sd">    - Precision Recall F1 Support (macro, micro, weighted, samples)</span>
</span><span id="L-183"><a href="#L-183"><span class="linenos">183</span></a><span class="sd">    - Jaccard Score (macro, micro, weighted, samples)</span>
</span><span id="L-184"><a href="#L-184"><span class="linenos">184</span></a>
</span><span id="L-185"><a href="#L-185"><span class="linenos">185</span></a><span class="sd">    Interpretation:</span>
</span><span id="L-186"><a href="#L-186"><span class="linenos">186</span></a><span class="sd">    ----------------</span>
</span><span id="L-187"><a href="#L-187"><span class="linenos">187</span></a><span class="sd">    1. **Accuracy**</span>
</span><span id="L-188"><a href="#L-188"><span class="linenos">188</span></a><span class="sd">        Definition: The proportion of correctly predicted labels (both positive and negative) over </span>
</span><span id="L-189"><a href="#L-189"><span class="linenos">189</span></a><span class="sd">        the total number of labels.</span>
</span><span id="L-190"><a href="#L-190"><span class="linenos">190</span></a><span class="sd">        - **High Accuracy**: Indicates that the classifier correctly predicted a high proportion of labels.</span>
</span><span id="L-191"><a href="#L-191"><span class="linenos">191</span></a><span class="sd">        - **Low Accuracy**: Indicates that the classifier made many incorrect predictions.</span>
</span><span id="L-192"><a href="#L-192"><span class="linenos">192</span></a><span class="sd">        - **Reference**: [Wikipedia: Accuracy and Precision](https://en.wikipedia.org/wiki/Accuracy_and_precision)</span>
</span><span id="L-193"><a href="#L-193"><span class="linenos">193</span></a>
</span><span id="L-194"><a href="#L-194"><span class="linenos">194</span></a><span class="sd">    2. **Hamming Loss**</span>
</span><span id="L-195"><a href="#L-195"><span class="linenos">195</span></a><span class="sd">        Definition: The fraction of labels that are incorrectly predicted, either due to false</span>
</span><span id="L-196"><a href="#L-196"><span class="linenos">196</span></a><span class="sd">        positives or false negatives, normalized by the total number of labels.    </span>
</span><span id="L-197"><a href="#L-197"><span class="linenos">197</span></a><span class="sd">        - **Low Hamming Loss**: Indicates fewer incorrect predictions.</span>
</span><span id="L-198"><a href="#L-198"><span class="linenos">198</span></a><span class="sd">        - **High Hamming Loss**: Indicates many incorrect predictions.</span>
</span><span id="L-199"><a href="#L-199"><span class="linenos">199</span></a><span class="sd">        - **Reference**: [Hamming Loss on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html)</span>
</span><span id="L-200"><a href="#L-200"><span class="linenos">200</span></a>
</span><span id="L-201"><a href="#L-201"><span class="linenos">201</span></a><span class="sd">    3. **Subset Accuracy**</span>
</span><span id="L-202"><a href="#L-202"><span class="linenos">202</span></a><span class="sd">        Definition: The proportion of instances for which the classifier predicted all the labels </span>
</span><span id="L-203"><a href="#L-203"><span class="linenos">203</span></a><span class="sd">        exactly right (i.e., the predicted label set matches the true label set exactly).</span>
</span><span id="L-204"><a href="#L-204"><span class="linenos">204</span></a><span class="sd">        - **High Subset Accuracy**: Indicates that the classifier correctly predicted all labels for </span>
</span><span id="L-205"><a href="#L-205"><span class="linenos">205</span></a><span class="sd">          many instances.</span>
</span><span id="L-206"><a href="#L-206"><span class="linenos">206</span></a><span class="sd">        - **Low Subset Accuracy**: Indicates that the classifier often missed some labels or included </span>
</span><span id="L-207"><a href="#L-207"><span class="linenos">207</span></a><span class="sd">          incorrect labels.</span>
</span><span id="L-208"><a href="#L-208"><span class="linenos">208</span></a><span class="sd">        - **Reference**: [Subset Accuracy on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)</span>
</span><span id="L-209"><a href="#L-209"><span class="linenos">209</span></a>
</span><span id="L-210"><a href="#L-210"><span class="linenos">210</span></a><span class="sd">    4. **Zero-One Loss**</span>
</span><span id="L-211"><a href="#L-211"><span class="linenos">211</span></a><span class="sd">        Definition: The fraction of instances where the classifier’s prediction does not match the </span>
</span><span id="L-212"><a href="#L-212"><span class="linenos">212</span></a><span class="sd">        true label set (i.e., the prediction is not an exact match).    </span>
</span><span id="L-213"><a href="#L-213"><span class="linenos">213</span></a><span class="sd">        - **Low Zero-One Loss**: Indicates that the classifier makes fewer predictions that do not match </span>
</span><span id="L-214"><a href="#L-214"><span class="linenos">214</span></a><span class="sd">          the true labels exactly.</span>
</span><span id="L-215"><a href="#L-215"><span class="linenos">215</span></a><span class="sd">        - **High Zero-One Loss**: Indicates that the classifier often makes incorrect predictions.</span>
</span><span id="L-216"><a href="#L-216"><span class="linenos">216</span></a><span class="sd">        - **Reference**: [Zero-One Loss on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html)</span>
</span><span id="L-217"><a href="#L-217"><span class="linenos">217</span></a>
</span><span id="L-218"><a href="#L-218"><span class="linenos">218</span></a><span class="sd">    5. **Precision (Macro)**</span>
</span><span id="L-219"><a href="#L-219"><span class="linenos">219</span></a><span class="sd">        Definition: The average precision score calculated for each label independently and then </span>
</span><span id="L-220"><a href="#L-220"><span class="linenos">220</span></a><span class="sd">        averaged, treating all labels equally.    </span>
</span><span id="L-221"><a href="#L-221"><span class="linenos">221</span></a><span class="sd">        - **High Macro Precision**: Indicates good performance across all labels individually.</span>
</span><span id="L-222"><a href="#L-222"><span class="linenos">222</span></a><span class="sd">        - **Low Macro Precision**: Indicates poor performance on some labels.</span>
</span><span id="L-223"><a href="#L-223"><span class="linenos">223</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="L-224"><a href="#L-224"><span class="linenos">224</span></a>
</span><span id="L-225"><a href="#L-225"><span class="linenos">225</span></a><span class="sd">    6. **Precision (Micro)**</span>
</span><span id="L-226"><a href="#L-226"><span class="linenos">226</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives </span>
</span><span id="L-227"><a href="#L-227"><span class="linenos">227</span></a><span class="sd">        and false positives, aggregated across all labels.    </span>
</span><span id="L-228"><a href="#L-228"><span class="linenos">228</span></a><span class="sd">        - **High Micro Precision**: Indicates good overall performance when considering all labels collectively.</span>
</span><span id="L-229"><a href="#L-229"><span class="linenos">229</span></a><span class="sd">        - **Low Micro Precision**: Indicates many false positives relative to true positives.</span>
</span><span id="L-230"><a href="#L-230"><span class="linenos">230</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="L-231"><a href="#L-231"><span class="linenos">231</span></a>
</span><span id="L-232"><a href="#L-232"><span class="linenos">232</span></a><span class="sd">    7. **Precision (Weighted)**</span>
</span><span id="L-233"><a href="#L-233"><span class="linenos">233</span></a><span class="sd">        Definition: The precision score calculated for each label, weighted by the number of true </span>
</span><span id="L-234"><a href="#L-234"><span class="linenos">234</span></a><span class="sd">        instances for each label, and then averaged.</span>
</span><span id="L-235"><a href="#L-235"><span class="linenos">235</span></a><span class="sd">        - **High Weighted Precision**: Indicates good performance when accounting for the number of </span>
</span><span id="L-236"><a href="#L-236"><span class="linenos">236</span></a><span class="sd">          instances for each label.</span>
</span><span id="L-237"><a href="#L-237"><span class="linenos">237</span></a><span class="sd">        - **Low Weighted Precision**: Indicates varying performance across labels.</span>
</span><span id="L-238"><a href="#L-238"><span class="linenos">238</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="L-239"><a href="#L-239"><span class="linenos">239</span></a>
</span><span id="L-240"><a href="#L-240"><span class="linenos">240</span></a><span class="sd">    8. **Precision (Samples)**</span>
</span><span id="L-241"><a href="#L-241"><span class="linenos">241</span></a><span class="sd">        Definition: The precision score computed for each instance individually, then averaged.    </span>
</span><span id="L-242"><a href="#L-242"><span class="linenos">242</span></a><span class="sd">        - **High Sample Precision**: Indicates good performance on average across different instances.</span>
</span><span id="L-243"><a href="#L-243"><span class="linenos">243</span></a><span class="sd">        - **Low Sample Precision**: Indicates that the classifier often makes incorrect predictions for some instances.</span>
</span><span id="L-244"><a href="#L-244"><span class="linenos">244</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="L-245"><a href="#L-245"><span class="linenos">245</span></a>
</span><span id="L-246"><a href="#L-246"><span class="linenos">246</span></a><span class="sd">    9. **Recall (Macro)**</span>
</span><span id="L-247"><a href="#L-247"><span class="linenos">247</span></a><span class="sd">        Definition: The average recall score calculated for each label independently and then </span>
</span><span id="L-248"><a href="#L-248"><span class="linenos">248</span></a><span class="sd">        averaged, treating all labels equally.    </span>
</span><span id="L-249"><a href="#L-249"><span class="linenos">249</span></a><span class="sd">        - **High Macro Recall**: Indicates good identification of relevant labels across all labels individually.</span>
</span><span id="L-250"><a href="#L-250"><span class="linenos">250</span></a><span class="sd">        - **Low Macro Recall**: Indicates that the classifier misses many relevant labels.</span>
</span><span id="L-251"><a href="#L-251"><span class="linenos">251</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="L-252"><a href="#L-252"><span class="linenos">252</span></a>
</span><span id="L-253"><a href="#L-253"><span class="linenos">253</span></a><span class="sd">    10. **Recall (Micro)**</span>
</span><span id="L-254"><a href="#L-254"><span class="linenos">254</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives </span>
</span><span id="L-255"><a href="#L-255"><span class="linenos">255</span></a><span class="sd">        and false negatives, aggregated across all labels.    </span>
</span><span id="L-256"><a href="#L-256"><span class="linenos">256</span></a><span class="sd">        - **High Micro Recall**: Indicates good overall identification of relevant labels.</span>
</span><span id="L-257"><a href="#L-257"><span class="linenos">257</span></a><span class="sd">        - **Low Micro Recall**: Indicates that the classifier misses many relevant labels.</span>
</span><span id="L-258"><a href="#L-258"><span class="linenos">258</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="L-259"><a href="#L-259"><span class="linenos">259</span></a>
</span><span id="L-260"><a href="#L-260"><span class="linenos">260</span></a><span class="sd">    11. **Recall (Weighted)**</span>
</span><span id="L-261"><a href="#L-261"><span class="linenos">261</span></a><span class="sd">        Definition: The recall score calculated for each label, weighted by the number of true </span>
</span><span id="L-262"><a href="#L-262"><span class="linenos">262</span></a><span class="sd">        instances for each label, and then averaged.    </span>
</span><span id="L-263"><a href="#L-263"><span class="linenos">263</span></a><span class="sd">        - **High Weighted Recall**: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="L-264"><a href="#L-264"><span class="linenos">264</span></a><span class="sd">        - **Low Weighted Recall**: Indicates varying performance in identifying relevant labels.</span>
</span><span id="L-265"><a href="#L-265"><span class="linenos">265</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="L-266"><a href="#L-266"><span class="linenos">266</span></a>
</span><span id="L-267"><a href="#L-267"><span class="linenos">267</span></a><span class="sd">    12. **Recall (Samples)**</span>
</span><span id="L-268"><a href="#L-268"><span class="linenos">268</span></a><span class="sd">        Definition: The recall score computed for each instance individually, then averaged.    </span>
</span><span id="L-269"><a href="#L-269"><span class="linenos">269</span></a><span class="sd">        - **High Sample Recall**: Indicates good identification of relevant labels on average across instances.</span>
</span><span id="L-270"><a href="#L-270"><span class="linenos">270</span></a><span class="sd">        - **Low Sample Recall**: Indicates that the classifier misses many relevant labels for some instances.</span>
</span><span id="L-271"><a href="#L-271"><span class="linenos">271</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="L-272"><a href="#L-272"><span class="linenos">272</span></a>
</span><span id="L-273"><a href="#L-273"><span class="linenos">273</span></a><span class="sd">    13. **F1 Score (Macro)**</span>
</span><span id="L-274"><a href="#L-274"><span class="linenos">274</span></a><span class="sd">        Definition: The average F1 score calculated for each label independently and then averaged, </span>
</span><span id="L-275"><a href="#L-275"><span class="linenos">275</span></a><span class="sd">        treating all labels equally.    </span>
</span><span id="L-276"><a href="#L-276"><span class="linenos">276</span></a><span class="sd">        - **High Macro F1**: Indicates a good balance between precision and recall across all labels.</span>
</span><span id="L-277"><a href="#L-277"><span class="linenos">277</span></a><span class="sd">        - **Low Macro F1**: Indicates poor balance between precision and recall.</span>
</span><span id="L-278"><a href="#L-278"><span class="linenos">278</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="L-279"><a href="#L-279"><span class="linenos">279</span></a>
</span><span id="L-280"><a href="#L-280"><span class="linenos">280</span></a><span class="sd">    14. **F1 Score (Micro)**</span>
</span><span id="L-281"><a href="#L-281"><span class="linenos">281</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives, </span>
</span><span id="L-282"><a href="#L-282"><span class="linenos">282</span></a><span class="sd">        false positives, and false negatives, aggregated across all labels.</span>
</span><span id="L-283"><a href="#L-283"><span class="linenos">283</span></a><span class="sd">        - **High Micro F1**: Indicates good overall balance between precision and recall.</span>
</span><span id="L-284"><a href="#L-284"><span class="linenos">284</span></a><span class="sd">        - **Low Micro F1**: Indicates poor overall balance between precision and recall.</span>
</span><span id="L-285"><a href="#L-285"><span class="linenos">285</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="L-286"><a href="#L-286"><span class="linenos">286</span></a>
</span><span id="L-287"><a href="#L-287"><span class="linenos">287</span></a><span class="sd">    15. **F1 Score (Weighted)**</span>
</span><span id="L-288"><a href="#L-288"><span class="linenos">288</span></a><span class="sd">        Definition: The F1 score calculated for each label, weighted by the number of true instances</span>
</span><span id="L-289"><a href="#L-289"><span class="linenos">289</span></a><span class="sd">          for each label, and then averaged.    </span>
</span><span id="L-290"><a href="#L-290"><span class="linenos">290</span></a><span class="sd">        - **High Weighted F1**: Indicates good performance considering the number of instances for each label.</span>
</span><span id="L-291"><a href="#L-291"><span class="linenos">291</span></a><span class="sd">        - **Low Weighted F1**: Indicates varying performance across labels.</span>
</span><span id="L-292"><a href="#L-292"><span class="linenos">292</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="L-293"><a href="#L-293"><span class="linenos">293</span></a>
</span><span id="L-294"><a href="#L-294"><span class="linenos">294</span></a><span class="sd">    16. **F1 Score (Samples)**</span>
</span><span id="L-295"><a href="#L-295"><span class="linenos">295</span></a><span class="sd">        Definition: The F1 score computed for each instance individually, then averaged.</span>
</span><span id="L-296"><a href="#L-296"><span class="linenos">296</span></a><span class="sd">        - **High Sample F1**: Indicates good balance between precision and recall for each instance.</span>
</span><span id="L-297"><a href="#L-297"><span class="linenos">297</span></a><span class="sd">        - **Low Sample F1**: Indicates that the balance between precision and recall varies significantly across instances.</span>
</span><span id="L-298"><a href="#L-298"><span class="linenos">298</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="L-299"><a href="#L-299"><span class="linenos">299</span></a>
</span><span id="L-300"><a href="#L-300"><span class="linenos">300</span></a><span class="sd">    17. **Jaccard Score (Macro)**</span>
</span><span id="L-301"><a href="#L-301"><span class="linenos">301</span></a><span class="sd">        Definition: The average Jaccard score computed for each label independently and then </span>
</span><span id="L-302"><a href="#L-302"><span class="linenos">302</span></a><span class="sd">        averaged, treating all labels equally.</span>
</span><span id="L-303"><a href="#L-303"><span class="linenos">303</span></a><span class="sd">        - **High Macro Jaccard Score**: Indicates good performance across all labels individually.</span>
</span><span id="L-304"><a href="#L-304"><span class="linenos">304</span></a><span class="sd">        - **Low Macro Jaccard Score**: Indicates poor performance on some labels.</span>
</span><span id="L-305"><a href="#L-305"><span class="linenos">305</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="L-306"><a href="#L-306"><span class="linenos">306</span></a>
</span><span id="L-307"><a href="#L-307"><span class="linenos">307</span></a><span class="sd">    18. **Jaccard Score (Micro)**</span>
</span><span id="L-308"><a href="#L-308"><span class="linenos">308</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives,</span>
</span><span id="L-309"><a href="#L-309"><span class="linenos">309</span></a><span class="sd">        false positives, and false negatives, aggregated across all labels.</span>
</span><span id="L-310"><a href="#L-310"><span class="linenos">310</span></a><span class="sd">        - **High Micro Jaccard Score**: Indicates good overall performance in terms of similarity and diversity.</span>
</span><span id="L-311"><a href="#L-311"><span class="linenos">311</span></a><span class="sd">        - **Low Micro Jaccard Score**: Indicates poor performance in capturing similarities and differences across labels.</span>
</span><span id="L-312"><a href="#L-312"><span class="linenos">312</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="L-313"><a href="#L-313"><span class="linenos">313</span></a>
</span><span id="L-314"><a href="#L-314"><span class="linenos">314</span></a><span class="sd">    19. **Jaccard Score (Weighted)**</span>
</span><span id="L-315"><a href="#L-315"><span class="linenos">315</span></a><span class="sd">        Definition: The Jaccard score calculated for each label, weighted by the number of true </span>
</span><span id="L-316"><a href="#L-316"><span class="linenos">316</span></a><span class="sd">        instances for each label, and then averaged.</span>
</span><span id="L-317"><a href="#L-317"><span class="linenos">317</span></a><span class="sd">        - **High Weighted Jaccard Score**: Indicates good performance considering the number of instances for each label.</span>
</span><span id="L-318"><a href="#L-318"><span class="linenos">318</span></a><span class="sd">        - **Low Weighted Jaccard Score**: Indicates varying performance across labels.</span>
</span><span id="L-319"><a href="#L-319"><span class="linenos">319</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="L-320"><a href="#L-320"><span class="linenos">320</span></a>
</span><span id="L-321"><a href="#L-321"><span class="linenos">321</span></a><span class="sd">    20. **Jaccard Score (Samples)**</span>
</span><span id="L-322"><a href="#L-322"><span class="linenos">322</span></a><span class="sd">        Definition: The Jaccard score computed for each instance individually, then averaged.</span>
</span><span id="L-323"><a href="#L-323"><span class="linenos">323</span></a><span class="sd">        - **High Sample Jaccard Score**: Indicates good performance on average for each instance.</span>
</span><span id="L-324"><a href="#L-324"><span class="linenos">324</span></a><span class="sd">        - **Low Sample Jaccard Score**: Indicates varying performance across instances.</span>
</span><span id="L-325"><a href="#L-325"><span class="linenos">325</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="L-326"><a href="#L-326"><span class="linenos">326</span></a>
</span><span id="L-327"><a href="#L-327"><span class="linenos">327</span></a><span class="sd">    Example Usage:</span>
</span><span id="L-328"><a href="#L-328"><span class="linenos">328</span></a><span class="sd">    --------------</span>
</span><span id="L-329"><a href="#L-329"><span class="linenos">329</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_bipartition_measures(true_labels, pred_labels)</span>
</span><span id="L-330"><a href="#L-330"><span class="linenos">330</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="L-331"><a href="#L-331"><span class="linenos">331</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-332"><a href="#L-332"><span class="linenos">332</span></a>
</span><span id="L-333"><a href="#L-333"><span class="linenos">333</span></a>    <span class="c1"># Basic metrics</span>
</span><span id="L-334"><a href="#L-334"><span class="linenos">334</span></a>    <span class="n">accuracy_mlem</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_accuracy</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="L-335"><a href="#L-335"><span class="linenos">335</span></a>    <span class="n">hamming_l</span> <span class="o">=</span> <span class="n">hamming_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">true_labels</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred_labels</span><span class="p">))</span>    
</span><span id="L-336"><a href="#L-336"><span class="linenos">336</span></a>    <span class="n">zol</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">true_labels</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred_labels</span><span class="p">))</span>    
</span><span id="L-337"><a href="#L-337"><span class="linenos">337</span></a>    <span class="n">sa</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_subset_accuracy</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="L-338"><a href="#L-338"><span class="linenos">338</span></a>
</span><span id="L-339"><a href="#L-339"><span class="linenos">339</span></a>    <span class="c1"># Precision Scores</span>
</span><span id="L-340"><a href="#L-340"><span class="linenos">340</span></a>    <span class="n">precision_macro</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>    
</span><span id="L-341"><a href="#L-341"><span class="linenos">341</span></a>    <span class="n">precision_micro</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-342"><a href="#L-342"><span class="linenos">342</span></a>    <span class="n">precision_weighted</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-343"><a href="#L-343"><span class="linenos">343</span></a>    <span class="n">precision_samples</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-344"><a href="#L-344"><span class="linenos">344</span></a>    <span class="n">precision_none</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-345"><a href="#L-345"><span class="linenos">345</span></a>    
</span><span id="L-346"><a href="#L-346"><span class="linenos">346</span></a>    <span class="c1"># Recall Scores</span>
</span><span id="L-347"><a href="#L-347"><span class="linenos">347</span></a>    <span class="n">recall_macro</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>  
</span><span id="L-348"><a href="#L-348"><span class="linenos">348</span></a>    <span class="n">recall_micro</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-349"><a href="#L-349"><span class="linenos">349</span></a>    <span class="n">recall_weighted</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-350"><a href="#L-350"><span class="linenos">350</span></a>    <span class="n">recall_samples</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-351"><a href="#L-351"><span class="linenos">351</span></a>    <span class="n">recall_none</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-352"><a href="#L-352"><span class="linenos">352</span></a>       
</span><span id="L-353"><a href="#L-353"><span class="linenos">353</span></a>    <span class="c1"># F1 Scores</span>
</span><span id="L-354"><a href="#L-354"><span class="linenos">354</span></a>    <span class="n">f1_macro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-355"><a href="#L-355"><span class="linenos">355</span></a>    <span class="n">f1_micro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-356"><a href="#L-356"><span class="linenos">356</span></a>    <span class="n">f1_weighted</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-357"><a href="#L-357"><span class="linenos">357</span></a>    <span class="n">f1_samples</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-358"><a href="#L-358"><span class="linenos">358</span></a>    <span class="n">f1_none</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="L-359"><a href="#L-359"><span class="linenos">359</span></a>
</span><span id="L-360"><a href="#L-360"><span class="linenos">360</span></a>    <span class="c1"># Jaccard Scores</span>
</span><span id="L-361"><a href="#L-361"><span class="linenos">361</span></a>    <span class="n">jaccard_macro</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-362"><a href="#L-362"><span class="linenos">362</span></a>    <span class="n">jaccard_micro</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-363"><a href="#L-363"><span class="linenos">363</span></a>    <span class="n">jaccard_weighted</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-364"><a href="#L-364"><span class="linenos">364</span></a>    <span class="n">jaccard_samples</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="L-365"><a href="#L-365"><span class="linenos">365</span></a>    <span class="n">jaccard_none</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="L-366"><a href="#L-366"><span class="linenos">366</span></a>
</span><span id="L-367"><a href="#L-367"><span class="linenos">367</span></a>    <span class="c1"># Precision, Recall, F1, and Support Scores</span>
</span><span id="L-368"><a href="#L-368"><span class="linenos">368</span></a>    <span class="n">rpf_macro</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-369"><a href="#L-369"><span class="linenos">369</span></a>    <span class="n">rpf_micro</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-370"><a href="#L-370"><span class="linenos">370</span></a>    <span class="n">rpf_weighted</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="L-371"><a href="#L-371"><span class="linenos">371</span></a>    <span class="n">rpf_samples</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="L-372"><a href="#L-372"><span class="linenos">372</span></a>    <span class="n">rpf_none</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="L-373"><a href="#L-373"><span class="linenos">373</span></a>
</span><span id="L-374"><a href="#L-374"><span class="linenos">374</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="L-375"><a href="#L-375"><span class="linenos">375</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-376"><a href="#L-376"><span class="linenos">376</span></a>        <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">accuracy_mlem</span><span class="p">,</span>        
</span><span id="L-377"><a href="#L-377"><span class="linenos">377</span></a>        <span class="s1">&#39;f1_macro&#39;</span><span class="p">:</span> <span class="n">f1_macro</span><span class="p">,</span>
</span><span id="L-378"><a href="#L-378"><span class="linenos">378</span></a>        <span class="s1">&#39;f1_micro&#39;</span><span class="p">:</span> <span class="n">f1_micro</span><span class="p">,</span>
</span><span id="L-379"><a href="#L-379"><span class="linenos">379</span></a>        <span class="s1">&#39;f1_weighted&#39;</span><span class="p">:</span> <span class="n">f1_weighted</span><span class="p">,</span>
</span><span id="L-380"><a href="#L-380"><span class="linenos">380</span></a>        <span class="s1">&#39;f1_samples&#39;</span><span class="p">:</span> <span class="n">f1_samples</span><span class="p">,</span> 
</span><span id="L-381"><a href="#L-381"><span class="linenos">381</span></a>        <span class="s1">&#39;hamming_loss&#39;</span><span class="p">:</span> <span class="n">hamming_l</span><span class="p">,</span>              
</span><span id="L-382"><a href="#L-382"><span class="linenos">382</span></a>        <span class="s1">&#39;jaccard_macro&#39;</span><span class="p">:</span> <span class="n">jaccard_macro</span><span class="p">,</span>
</span><span id="L-383"><a href="#L-383"><span class="linenos">383</span></a>        <span class="s1">&#39;jaccard_micro&#39;</span><span class="p">:</span> <span class="n">jaccard_micro</span><span class="p">,</span>
</span><span id="L-384"><a href="#L-384"><span class="linenos">384</span></a>        <span class="s1">&#39;jaccard_weighted&#39;</span><span class="p">:</span> <span class="n">jaccard_weighted</span><span class="p">,</span>
</span><span id="L-385"><a href="#L-385"><span class="linenos">385</span></a>        <span class="s1">&#39;jaccard_samples&#39;</span><span class="p">:</span> <span class="n">jaccard_samples</span><span class="p">,</span>
</span><span id="L-386"><a href="#L-386"><span class="linenos">386</span></a>        <span class="s1">&#39;precision_macro&#39;</span><span class="p">:</span> <span class="n">precision_macro</span><span class="p">,</span>
</span><span id="L-387"><a href="#L-387"><span class="linenos">387</span></a>        <span class="s1">&#39;precision_micro&#39;</span><span class="p">:</span> <span class="n">precision_micro</span><span class="p">,</span>
</span><span id="L-388"><a href="#L-388"><span class="linenos">388</span></a>        <span class="s1">&#39;precision_weighted&#39;</span><span class="p">:</span> <span class="n">precision_weighted</span><span class="p">,</span>
</span><span id="L-389"><a href="#L-389"><span class="linenos">389</span></a>        <span class="s1">&#39;precision_samples&#39;</span><span class="p">:</span> <span class="n">precision_samples</span><span class="p">,</span>        
</span><span id="L-390"><a href="#L-390"><span class="linenos">390</span></a>        <span class="s1">&#39;recall_macro&#39;</span><span class="p">:</span> <span class="n">recall_macro</span><span class="p">,</span>
</span><span id="L-391"><a href="#L-391"><span class="linenos">391</span></a>        <span class="s1">&#39;recall_micro&#39;</span><span class="p">:</span> <span class="n">recall_micro</span><span class="p">,</span>
</span><span id="L-392"><a href="#L-392"><span class="linenos">392</span></a>        <span class="s1">&#39;recall_weighted&#39;</span><span class="p">:</span> <span class="n">recall_weighted</span><span class="p">,</span>
</span><span id="L-393"><a href="#L-393"><span class="linenos">393</span></a>        <span class="s1">&#39;recall_samples&#39;</span><span class="p">:</span> <span class="n">recall_samples</span><span class="p">,</span>  
</span><span id="L-394"><a href="#L-394"><span class="linenos">394</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_macro&#39;: rpf_macro,</span>
</span><span id="L-395"><a href="#L-395"><span class="linenos">395</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_micro&#39;: rpf_micro,</span>
</span><span id="L-396"><a href="#L-396"><span class="linenos">396</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_weighted&#39;: rpf_weighted,</span>
</span><span id="L-397"><a href="#L-397"><span class="linenos">397</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_samples&#39;: rpf_samples,</span>
</span><span id="L-398"><a href="#L-398"><span class="linenos">398</span></a>        <span class="s1">&#39;zero_one_loss&#39;</span><span class="p">:</span> <span class="n">zol</span>     
</span><span id="L-399"><a href="#L-399"><span class="linenos">399</span></a>    <span class="p">}</span>
</span><span id="L-400"><a href="#L-400"><span class="linenos">400</span></a>
</span><span id="L-401"><a href="#L-401"><span class="linenos">401</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="L-402"><a href="#L-402"><span class="linenos">402</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="L-403"><a href="#L-403"><span class="linenos">403</span></a>
</span><span id="L-404"><a href="#L-404"><span class="linenos">404</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="L-405"><a href="#L-405"><span class="linenos">405</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="L-406"><a href="#L-406"><span class="linenos">406</span></a>
</span><span id="L-407"><a href="#L-407"><span class="linenos">407</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span><span id="L-408"><a href="#L-408"><span class="linenos">408</span></a>
</span><span id="L-409"><a href="#L-409"><span class="linenos">409</span></a>
</span><span id="L-410"><a href="#L-410"><span class="linenos">410</span></a>
</span><span id="L-411"><a href="#L-411"><span class="linenos">411</span></a>
</span><span id="L-412"><a href="#L-412"><span class="linenos">412</span></a><span class="c1">########################################################################</span>
</span><span id="L-413"><a href="#L-413"><span class="linenos">413</span></a><span class="c1">#                                                                      #</span>
</span><span id="L-414"><a href="#L-414"><span class="linenos">414</span></a><span class="c1">########################################################################</span>
</span><span id="L-415"><a href="#L-415"><span class="linenos">415</span></a><span class="k">def</span> <span class="nf">multilabel_curves_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="L-416"><a href="#L-416"><span class="linenos">416</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-417"><a href="#L-417"><span class="linenos">417</span></a><span class="sd">    Calculates various evaluation metrics related to ranking curves for multi-label classification.</span>
</span><span id="L-418"><a href="#L-418"><span class="linenos">418</span></a>
</span><span id="L-419"><a href="#L-419"><span class="linenos">419</span></a><span class="sd">    Parameters:</span>
</span><span id="L-420"><a href="#L-420"><span class="linenos">420</span></a><span class="sd">    ----------</span>
</span><span id="L-421"><a href="#L-421"><span class="linenos">421</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="L-422"><a href="#L-422"><span class="linenos">422</span></a><span class="sd">    pred_scores (pd.DataFrame): The DataFrame containing the predicted probabilities for each label.</span>
</span><span id="L-423"><a href="#L-423"><span class="linenos">423</span></a>
</span><span id="L-424"><a href="#L-424"><span class="linenos">424</span></a><span class="sd">    Returns:</span>
</span><span id="L-425"><a href="#L-425"><span class="linenos">425</span></a><span class="sd">    -------</span>
</span><span id="L-426"><a href="#L-426"><span class="linenos">426</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="L-427"><a href="#L-427"><span class="linenos">427</span></a><span class="sd">        A DataFrame containing the computed curve-based metrics.</span>
</span><span id="L-428"><a href="#L-428"><span class="linenos">428</span></a>
</span><span id="L-429"><a href="#L-429"><span class="linenos">429</span></a><span class="sd">    Metrics Computed:</span>
</span><span id="L-430"><a href="#L-430"><span class="linenos">430</span></a><span class="sd">    ------------------</span>
</span><span id="L-431"><a href="#L-431"><span class="linenos">431</span></a><span class="sd">    - Average Precision (AP) Score (Macro, Micro, Weighted, Samples)</span>
</span><span id="L-432"><a href="#L-432"><span class="linenos">432</span></a><span class="sd">    - ROC AUC Score (Macro, Micro, Weighted, Samples)</span>
</span><span id="L-433"><a href="#L-433"><span class="linenos">433</span></a>
</span><span id="L-434"><a href="#L-434"><span class="linenos">434</span></a><span class="sd">    Interpretation:</span>
</span><span id="L-435"><a href="#L-435"><span class="linenos">435</span></a><span class="sd">    ----------------</span>
</span><span id="L-436"><a href="#L-436"><span class="linenos">436</span></a><span class="sd">    1. **Average Precision (AP) Score**</span>
</span><span id="L-437"><a href="#L-437"><span class="linenos">437</span></a><span class="sd">        Definition: Measures the quality of the ranking of predicted probabilities. It summarizes the </span>
</span><span id="L-438"><a href="#L-438"><span class="linenos">438</span></a><span class="sd">        precision-recall curve by calculating the average precision over all instances.</span>
</span><span id="L-439"><a href="#L-439"><span class="linenos">439</span></a><span class="sd">        - **AP Macro**: The average precision score calculated for each label independently and then </span>
</span><span id="L-440"><a href="#L-440"><span class="linenos">440</span></a><span class="sd">          averaged, treating all labels equally.</span>
</span><span id="L-441"><a href="#L-441"><span class="linenos">441</span></a><span class="sd">          - High AP Macro: Indicates good performance across all labels, regardless of class imbalance.</span>
</span><span id="L-442"><a href="#L-442"><span class="linenos">442</span></a><span class="sd">        - **AP Micro**: The average precision score calculated by aggregating the contributions of all labels </span>
</span><span id="L-443"><a href="#L-443"><span class="linenos">443</span></a><span class="sd">          to compute the average precision.</span>
</span><span id="L-444"><a href="#L-444"><span class="linenos">444</span></a><span class="sd">          - High AP Micro: Indicates good overall performance when considering the aggregate precision.</span>
</span><span id="L-445"><a href="#L-445"><span class="linenos">445</span></a><span class="sd">        - **AP Weighted**: The average precision score calculated for each label, weighted by the number of </span>
</span><span id="L-446"><a href="#L-446"><span class="linenos">446</span></a><span class="sd">          true instances for each label, and then averaged.</span>
</span><span id="L-447"><a href="#L-447"><span class="linenos">447</span></a><span class="sd">          - High AP Weighted: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="L-448"><a href="#L-448"><span class="linenos">448</span></a><span class="sd">        - **AP Samples**: The average precision score computed for each instance individually and then averaged.</span>
</span><span id="L-449"><a href="#L-449"><span class="linenos">449</span></a><span class="sd">          - High AP Samples: Indicates good performance on average across different instances.</span>
</span><span id="L-450"><a href="#L-450"><span class="linenos">450</span></a>
</span><span id="L-451"><a href="#L-451"><span class="linenos">451</span></a><span class="sd">    2. **ROC AUC Score**</span>
</span><span id="L-452"><a href="#L-452"><span class="linenos">452</span></a><span class="sd">        Definition: Measures the area under the Receiver Operating Characteristic (ROC) curve, summarizing the </span>
</span><span id="L-453"><a href="#L-453"><span class="linenos">453</span></a><span class="sd">        trade-off between true positive rate and false positive rate.</span>
</span><span id="L-454"><a href="#L-454"><span class="linenos">454</span></a><span class="sd">        - **ROC AUC Macro**: The ROC AUC score calculated for each label independently and then averaged, </span>
</span><span id="L-455"><a href="#L-455"><span class="linenos">455</span></a><span class="sd">          treating all labels equally.</span>
</span><span id="L-456"><a href="#L-456"><span class="linenos">456</span></a><span class="sd">          - High ROC AUC Macro: Indicates good performance across all labels, regardless of class imbalance.</span>
</span><span id="L-457"><a href="#L-457"><span class="linenos">457</span></a><span class="sd">        - **ROC AUC Micro**: The ROC AUC score calculated by aggregating the contributions of all labels to </span>
</span><span id="L-458"><a href="#L-458"><span class="linenos">458</span></a><span class="sd">          compute the average ROC AUC.</span>
</span><span id="L-459"><a href="#L-459"><span class="linenos">459</span></a><span class="sd">          - High ROC AUC Micro: Indicates good overall performance when considering the aggregate true positive </span>
</span><span id="L-460"><a href="#L-460"><span class="linenos">460</span></a><span class="sd">            rate and false positive rate.</span>
</span><span id="L-461"><a href="#L-461"><span class="linenos">461</span></a><span class="sd">        - **ROC AUC Weighted**: The ROC AUC score calculated for each label, weighted by the number of true </span>
</span><span id="L-462"><a href="#L-462"><span class="linenos">462</span></a><span class="sd">          instances for each label, and then averaged.</span>
</span><span id="L-463"><a href="#L-463"><span class="linenos">463</span></a><span class="sd">          - High ROC AUC Weighted: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="L-464"><a href="#L-464"><span class="linenos">464</span></a><span class="sd">        - **ROC AUC Samples**: The ROC AUC score computed for each instance individually and then averaged.</span>
</span><span id="L-465"><a href="#L-465"><span class="linenos">465</span></a><span class="sd">          - High ROC AUC Samples: Indicates good performance on average across different instances.</span>
</span><span id="L-466"><a href="#L-466"><span class="linenos">466</span></a>
</span><span id="L-467"><a href="#L-467"><span class="linenos">467</span></a><span class="sd">    Example Usage:</span>
</span><span id="L-468"><a href="#L-468"><span class="linenos">468</span></a><span class="sd">    --------------</span>
</span><span id="L-469"><a href="#L-469"><span class="linenos">469</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_curves_measures(true_labels, pred_scores)</span>
</span><span id="L-470"><a href="#L-470"><span class="linenos">470</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="L-471"><a href="#L-471"><span class="linenos">471</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-472"><a href="#L-472"><span class="linenos">472</span></a>
</span><span id="L-473"><a href="#L-473"><span class="linenos">473</span></a>    <span class="c1"># Average Precision Scores</span>
</span><span id="L-474"><a href="#L-474"><span class="linenos">474</span></a>    <span class="n">average_precision_macro</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
</span><span id="L-475"><a href="#L-475"><span class="linenos">475</span></a>    <span class="n">average_precision_micro</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
</span><span id="L-476"><a href="#L-476"><span class="linenos">476</span></a>    <span class="n">average_precision_weighted</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
</span><span id="L-477"><a href="#L-477"><span class="linenos">477</span></a>    <span class="n">average_precision_samples</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>    
</span><span id="L-478"><a href="#L-478"><span class="linenos">478</span></a>    
</span><span id="L-479"><a href="#L-479"><span class="linenos">479</span></a>    <span class="c1"># ROC AUC Scores</span>
</span><span id="L-480"><a href="#L-480"><span class="linenos">480</span></a>    <span class="n">roc_auc_macro</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
</span><span id="L-481"><a href="#L-481"><span class="linenos">481</span></a>    <span class="n">roc_auc_micro</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
</span><span id="L-482"><a href="#L-482"><span class="linenos">482</span></a>    <span class="n">roc_auc_weighted</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
</span><span id="L-483"><a href="#L-483"><span class="linenos">483</span></a>    <span class="n">roc_auc_samples</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>      
</span><span id="L-484"><a href="#L-484"><span class="linenos">484</span></a>
</span><span id="L-485"><a href="#L-485"><span class="linenos">485</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="L-486"><a href="#L-486"><span class="linenos">486</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="L-487"><a href="#L-487"><span class="linenos">487</span></a>        <span class="s1">&#39;auprc_macro&#39;</span><span class="p">:</span> <span class="n">average_precision_macro</span><span class="p">,</span>
</span><span id="L-488"><a href="#L-488"><span class="linenos">488</span></a>        <span class="s1">&#39;auprc_micro&#39;</span><span class="p">:</span> <span class="n">average_precision_micro</span><span class="p">,</span>
</span><span id="L-489"><a href="#L-489"><span class="linenos">489</span></a>        <span class="s1">&#39;auprc_weighted&#39;</span><span class="p">:</span> <span class="n">average_precision_weighted</span><span class="p">,</span>
</span><span id="L-490"><a href="#L-490"><span class="linenos">490</span></a>        <span class="s1">&#39;auprc_samples&#39;</span><span class="p">:</span> <span class="n">average_precision_samples</span><span class="p">,</span>
</span><span id="L-491"><a href="#L-491"><span class="linenos">491</span></a>        <span class="s1">&#39;roc_auc_macro&#39;</span><span class="p">:</span> <span class="n">roc_auc_macro</span><span class="p">,</span>
</span><span id="L-492"><a href="#L-492"><span class="linenos">492</span></a>        <span class="s1">&#39;roc_auc_micro&#39;</span><span class="p">:</span> <span class="n">roc_auc_micro</span><span class="p">,</span>
</span><span id="L-493"><a href="#L-493"><span class="linenos">493</span></a>        <span class="s1">&#39;roc_auc_weighted&#39;</span><span class="p">:</span> <span class="n">roc_auc_weighted</span><span class="p">,</span>
</span><span id="L-494"><a href="#L-494"><span class="linenos">494</span></a>        <span class="s1">&#39;roc_auc_samples&#39;</span><span class="p">:</span> <span class="n">roc_auc_samples</span>
</span><span id="L-495"><a href="#L-495"><span class="linenos">495</span></a>    <span class="p">}</span>
</span><span id="L-496"><a href="#L-496"><span class="linenos">496</span></a>
</span><span id="L-497"><a href="#L-497"><span class="linenos">497</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="L-498"><a href="#L-498"><span class="linenos">498</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="L-499"><a href="#L-499"><span class="linenos">499</span></a>
</span><span id="L-500"><a href="#L-500"><span class="linenos">500</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="L-501"><a href="#L-501"><span class="linenos">501</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="L-502"><a href="#L-502"><span class="linenos">502</span></a>
</span><span id="L-503"><a href="#L-503"><span class="linenos">503</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span><span id="L-504"><a href="#L-504"><span class="linenos">504</span></a>
</span><span id="L-505"><a href="#L-505"><span class="linenos">505</span></a>
</span><span id="L-506"><a href="#L-506"><span class="linenos">506</span></a>    
</span><span id="L-507"><a href="#L-507"><span class="linenos">507</span></a>
</span><span id="L-508"><a href="#L-508"><span class="linenos">508</span></a><span class="c1">########################################################################</span>
</span><span id="L-509"><a href="#L-509"><span class="linenos">509</span></a><span class="c1">#                                                                      #</span>
</span><span id="L-510"><a href="#L-510"><span class="linenos">510</span></a><span class="c1">########################################################################</span>
</span><span id="L-511"><a href="#L-511"><span class="linenos">511</span></a><span class="k">def</span> <span class="nf">multilabel_ranking_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="L-512"><a href="#L-512"><span class="linenos">512</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="L-513"><a href="#L-513"><span class="linenos">513</span></a><span class="sd">    Calculates various ranking-based evaluation metrics for multi-label classification.</span>
</span><span id="L-514"><a href="#L-514"><span class="linenos">514</span></a>
</span><span id="L-515"><a href="#L-515"><span class="linenos">515</span></a><span class="sd">    Parameters:</span>
</span><span id="L-516"><a href="#L-516"><span class="linenos">516</span></a><span class="sd">    ----------</span>
</span><span id="L-517"><a href="#L-517"><span class="linenos">517</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels for each instance.</span>
</span><span id="L-518"><a href="#L-518"><span class="linenos">518</span></a><span class="sd">    pred_scores (pd.DataFrame): The DataFrame containing the predicted scores for each label.</span>
</span><span id="L-519"><a href="#L-519"><span class="linenos">519</span></a>
</span><span id="L-520"><a href="#L-520"><span class="linenos">520</span></a><span class="sd">    Returns:</span>
</span><span id="L-521"><a href="#L-521"><span class="linenos">521</span></a><span class="sd">    -------</span>
</span><span id="L-522"><a href="#L-522"><span class="linenos">522</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="L-523"><a href="#L-523"><span class="linenos">523</span></a><span class="sd">        A DataFrame containing the computed ranking-based metrics.</span>
</span><span id="L-524"><a href="#L-524"><span class="linenos">524</span></a>
</span><span id="L-525"><a href="#L-525"><span class="linenos">525</span></a><span class="sd">    Metrics Computed:</span>
</span><span id="L-526"><a href="#L-526"><span class="linenos">526</span></a><span class="sd">    ------------------</span>
</span><span id="L-527"><a href="#L-527"><span class="linenos">527</span></a><span class="sd">    - Average Precision</span>
</span><span id="L-528"><a href="#L-528"><span class="linenos">528</span></a><span class="sd">    - Coverage Error</span>
</span><span id="L-529"><a href="#L-529"><span class="linenos">529</span></a><span class="sd">    - Is Error</span>
</span><span id="L-530"><a href="#L-530"><span class="linenos">530</span></a><span class="sd">    - Margin Loss</span>
</span><span id="L-531"><a href="#L-531"><span class="linenos">531</span></a><span class="sd">    - Ranking Error</span>
</span><span id="L-532"><a href="#L-532"><span class="linenos">532</span></a><span class="sd">    - Ranking Loss</span>
</span><span id="L-533"><a href="#L-533"><span class="linenos">533</span></a>
</span><span id="L-534"><a href="#L-534"><span class="linenos">534</span></a><span class="sd">    Interpretation:</span>
</span><span id="L-535"><a href="#L-535"><span class="linenos">535</span></a><span class="sd">    ----------------</span>
</span><span id="L-536"><a href="#L-536"><span class="linenos">536</span></a><span class="sd">    1. **Average Precision**</span>
</span><span id="L-537"><a href="#L-537"><span class="linenos">537</span></a><span class="sd">        Definition: Measures the quality of the ranking of predicted labels. It is the average of the </span>
</span><span id="L-538"><a href="#L-538"><span class="linenos">538</span></a><span class="sd">        precision scores calculated at each position in the ranked list of predictions, weighted by </span>
</span><span id="L-539"><a href="#L-539"><span class="linenos">539</span></a><span class="sd">        the number of relevant items found.</span>
</span><span id="L-540"><a href="#L-540"><span class="linenos">540</span></a><span class="sd">        - A value of 1.0 indicates perfect ranking where all relevant labels are ranked above all </span>
</span><span id="L-541"><a href="#L-541"><span class="linenos">541</span></a><span class="sd">          irrelevant labels for every instance.</span>
</span><span id="L-542"><a href="#L-542"><span class="linenos">542</span></a><span class="sd">        - Lower values indicate that the model is not effectively ranking all relevant labels before </span>
</span><span id="L-543"><a href="#L-543"><span class="linenos">543</span></a><span class="sd">          irrelevant ones.</span>
</span><span id="L-544"><a href="#L-544"><span class="linenos">544</span></a>
</span><span id="L-545"><a href="#L-545"><span class="linenos">545</span></a><span class="sd">    2. **Coverage Error**</span>
</span><span id="L-546"><a href="#L-546"><span class="linenos">546</span></a><span class="sd">        Definition: Measures the average number of labels that need to be checked before finding all </span>
</span><span id="L-547"><a href="#L-547"><span class="linenos">547</span></a><span class="sd">        relevant labels for each instance.</span>
</span><span id="L-548"><a href="#L-548"><span class="linenos">548</span></a><span class="sd">        - A value of 3.5 indicates that, on average, you need to check 3.5 labels to find all relevant </span>
</span><span id="L-549"><a href="#L-549"><span class="linenos">549</span></a><span class="sd">          labels.</span>
</span><span id="L-550"><a href="#L-550"><span class="linenos">550</span></a><span class="sd">        - Lower values are preferable as they suggest that fewer labels need to be checked to find all </span>
</span><span id="L-551"><a href="#L-551"><span class="linenos">551</span></a><span class="sd">          relevant ones, indicating better model performance.</span>
</span><span id="L-552"><a href="#L-552"><span class="linenos">552</span></a>
</span><span id="L-553"><a href="#L-553"><span class="linenos">553</span></a><span class="sd">    3. **Is Error**</span>
</span><span id="L-554"><a href="#L-554"><span class="linenos">554</span></a><span class="sd">        Definition: Indicates whether there is any discrepancy between the predicted ranking and the true </span>
</span><span id="L-555"><a href="#L-555"><span class="linenos">555</span></a><span class="sd">        ranking. </span>
</span><span id="L-556"><a href="#L-556"><span class="linenos">556</span></a><span class="sd">        - A value of 1.0 suggests that there is an error in the ranking, meaning that the predicted </span>
</span><span id="L-557"><a href="#L-557"><span class="linenos">557</span></a><span class="sd">          ranking does not match the true ranking exactly.</span>
</span><span id="L-558"><a href="#L-558"><span class="linenos">558</span></a><span class="sd">        - A value of 0.0 indicates that the predicted ranking matches the true ranking exactly.</span>
</span><span id="L-559"><a href="#L-559"><span class="linenos">559</span></a>
</span><span id="L-560"><a href="#L-560"><span class="linenos">560</span></a><span class="sd">    4. **Margin Loss**</span>
</span><span id="L-561"><a href="#L-561"><span class="linenos">561</span></a><span class="sd">        Definition: Measures the average number of positions by which positive labels are ranked below </span>
</span><span id="L-562"><a href="#L-562"><span class="linenos">562</span></a><span class="sd">        negative labels. </span>
</span><span id="L-563"><a href="#L-563"><span class="linenos">563</span></a><span class="sd">        - A Margin Loss value of 1.25 indicates that, on average, positive labels are ranked 1.25 </span>
</span><span id="L-564"><a href="#L-564"><span class="linenos">564</span></a><span class="sd">          positions below negative labels.</span>
</span><span id="L-565"><a href="#L-565"><span class="linenos">565</span></a><span class="sd">        - Lower values are preferable as they suggest that positive labels are ranked closer to the top </span>
</span><span id="L-566"><a href="#L-566"><span class="linenos">566</span></a><span class="sd">          compared to negative labels.</span>
</span><span id="L-567"><a href="#L-567"><span class="linenos">567</span></a>
</span><span id="L-568"><a href="#L-568"><span class="linenos">568</span></a><span class="sd">    5. **Ranking Error**</span>
</span><span id="L-569"><a href="#L-569"><span class="linenos">569</span></a><span class="sd">        Definition: Calculates the sum of squared differences between the predicted and true rankings. </span>
</span><span id="L-570"><a href="#L-570"><span class="linenos">570</span></a><span class="sd">        - A value of 9.5 indicates the total magnitude of the ranking errors.</span>
</span><span id="L-571"><a href="#L-571"><span class="linenos">571</span></a><span class="sd">        - Lower values are better, indicating that the predicted ranking is closer to the true ranking.</span>
</span><span id="L-572"><a href="#L-572"><span class="linenos">572</span></a>
</span><span id="L-573"><a href="#L-573"><span class="linenos">573</span></a><span class="sd">    6. **Ranking Loss**</span>
</span><span id="L-574"><a href="#L-574"><span class="linenos">574</span></a><span class="sd">        Definition: Measures the fraction of label pairs where the ranking is incorrect. </span>
</span><span id="L-575"><a href="#L-575"><span class="linenos">575</span></a><span class="sd">        - A value of approximately 0.67 indicates that about 67% of label pairs are ranked incorrectly.</span>
</span><span id="L-576"><a href="#L-576"><span class="linenos">576</span></a><span class="sd">        - Lower values are preferred, indicating that the majority of label pairs are ranked correctly.</span>
</span><span id="L-577"><a href="#L-577"><span class="linenos">577</span></a>
</span><span id="L-578"><a href="#L-578"><span class="linenos">578</span></a><span class="sd">    References:</span>
</span><span id="L-579"><a href="#L-579"><span class="linenos">579</span></a><span class="sd">    ----------</span>
</span><span id="L-580"><a href="#L-580"><span class="linenos">580</span></a><span class="sd">    - The metrics used are commonly referenced in multi-label ranking evaluation literature and libraries.</span>
</span><span id="L-581"><a href="#L-581"><span class="linenos">581</span></a><span class="sd">    - For detailed explanations, see the respective methods in the `ms` (multi-label metrics) library </span>
</span><span id="L-582"><a href="#L-582"><span class="linenos">582</span></a><span class="sd">    documentation and scikit-learn documentation for `label_ranking_loss` and `coverage_error`.</span>
</span><span id="L-583"><a href="#L-583"><span class="linenos">583</span></a>
</span><span id="L-584"><a href="#L-584"><span class="linenos">584</span></a><span class="sd">    Examples:</span>
</span><span id="L-585"><a href="#L-585"><span class="linenos">585</span></a><span class="sd">    ----------</span>
</span><span id="L-586"><a href="#L-586"><span class="linenos">586</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_ranking_measures(true_labels, pred_scores)</span>
</span><span id="L-587"><a href="#L-587"><span class="linenos">587</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="L-588"><a href="#L-588"><span class="linenos">588</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="L-589"><a href="#L-589"><span class="linenos">589</span></a>    
</span><span id="L-590"><a href="#L-590"><span class="linenos">590</span></a>    <span class="c1"># Compute the various ranking metrics</span>
</span><span id="L-591"><a href="#L-591"><span class="linenos">591</span></a>    <span class="n">average_precision</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_average_precision</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="L-592"><a href="#L-592"><span class="linenos">592</span></a>    <span class="n">precision_atk</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_precision_at_k</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="L-593"><a href="#L-593"><span class="linenos">593</span></a>    <span class="n">coverage</span> <span class="o">=</span> <span class="n">coverage_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="L-594"><a href="#L-594"><span class="linenos">594</span></a>    <span class="n">iserror</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_is_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="L-595"><a href="#L-595"><span class="linenos">595</span></a>    <span class="n">margin_loss</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_margin_loss</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="L-596"><a href="#L-596"><span class="linenos">596</span></a>    <span class="n">ranking_error</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_ranking_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="L-597"><a href="#L-597"><span class="linenos">597</span></a>    <span class="n">ranking_loss</span> <span class="o">=</span> <span class="n">label_ranking_loss</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="L-598"><a href="#L-598"><span class="linenos">598</span></a>
</span><span id="L-599"><a href="#L-599"><span class="linenos">599</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="L-600"><a href="#L-600"><span class="linenos">600</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>    
</span><span id="L-601"><a href="#L-601"><span class="linenos">601</span></a>        <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="n">average_precision</span><span class="p">,</span>
</span><span id="L-602"><a href="#L-602"><span class="linenos">602</span></a>        <span class="s1">&#39;coverage&#39;</span><span class="p">:</span> <span class="n">coverage</span><span class="p">,</span>
</span><span id="L-603"><a href="#L-603"><span class="linenos">603</span></a>        <span class="s1">&#39;is_error&#39;</span><span class="p">:</span> <span class="n">iserror</span><span class="p">,</span>
</span><span id="L-604"><a href="#L-604"><span class="linenos">604</span></a>        <span class="s1">&#39;margin_loss&#39;</span><span class="p">:</span> <span class="n">margin_loss</span><span class="p">,</span>
</span><span id="L-605"><a href="#L-605"><span class="linenos">605</span></a>        <span class="s1">&#39;precision_atk&#39;</span><span class="p">:</span> <span class="n">precision_atk</span><span class="p">,</span>
</span><span id="L-606"><a href="#L-606"><span class="linenos">606</span></a>        <span class="s1">&#39;ranking_error&#39;</span><span class="p">:</span> <span class="n">ranking_error</span><span class="p">,</span>
</span><span id="L-607"><a href="#L-607"><span class="linenos">607</span></a>        <span class="s1">&#39;ranking_loss&#39;</span><span class="p">:</span> <span class="n">ranking_loss</span>    
</span><span id="L-608"><a href="#L-608"><span class="linenos">608</span></a>    <span class="p">}</span>
</span><span id="L-609"><a href="#L-609"><span class="linenos">609</span></a>
</span><span id="L-610"><a href="#L-610"><span class="linenos">610</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="L-611"><a href="#L-611"><span class="linenos">611</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="L-612"><a href="#L-612"><span class="linenos">612</span></a>    
</span><span id="L-613"><a href="#L-613"><span class="linenos">613</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="L-614"><a href="#L-614"><span class="linenos">614</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="L-615"><a href="#L-615"><span class="linenos">615</span></a>
</span><span id="L-616"><a href="#L-616"><span class="linenos">616</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span></pre></div>


            </section>
                <section id="system">
                    <div class="attr variable">
            <span class="name">system</span>        =
<span class="default_value">&#39;Windows&#39;</span>

        
    </div>
    <a class="headerlink" href="#system"></a>
    
    

                </section>
                <section id="current_directory">
                    <div class="attr variable">
            <span class="name">current_directory</span>        =
<span class="default_value">&#39;C:\\Users\\Cissa\\Documents\\MultiLabelEvaluationMetrics\\src&#39;</span>

        
    </div>
    <a class="headerlink" href="#current_directory"></a>
    
    

                </section>
                <section id="multilabel_label_problem_measures">
                            <input id="multilabel_label_problem_measures-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">multilabel_label_problem_measures</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">true_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>,</span><span class="param">	<span class="n">pred_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span></span><span class="return-annotation">) -> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>:</span></span>

                <label class="view-source-button" for="multilabel_label_problem_measures-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#multilabel_label_problem_measures"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="multilabel_label_problem_measures-82"><a href="#multilabel_label_problem_measures-82"><span class="linenos"> 82</span></a><span class="k">def</span> <span class="nf">multilabel_label_problem_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="multilabel_label_problem_measures-83"><a href="#multilabel_label_problem_measures-83"><span class="linenos"> 83</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="multilabel_label_problem_measures-84"><a href="#multilabel_label_problem_measures-84"><span class="linenos"> 84</span></a><span class="sd">    Calculates measures for label prediction problems in multi-label classification.</span>
</span><span id="multilabel_label_problem_measures-85"><a href="#multilabel_label_problem_measures-85"><span class="linenos"> 85</span></a>
</span><span id="multilabel_label_problem_measures-86"><a href="#multilabel_label_problem_measures-86"><span class="linenos"> 86</span></a><span class="sd">    Parameters:</span>
</span><span id="multilabel_label_problem_measures-87"><a href="#multilabel_label_problem_measures-87"><span class="linenos"> 87</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_label_problem_measures-88"><a href="#multilabel_label_problem_measures-88"><span class="linenos"> 88</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="multilabel_label_problem_measures-89"><a href="#multilabel_label_problem_measures-89"><span class="linenos"> 89</span></a><span class="sd">    pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</span>
</span><span id="multilabel_label_problem_measures-90"><a href="#multilabel_label_problem_measures-90"><span class="linenos"> 90</span></a>
</span><span id="multilabel_label_problem_measures-91"><a href="#multilabel_label_problem_measures-91"><span class="linenos"> 91</span></a><span class="sd">    Returns:</span>
</span><span id="multilabel_label_problem_measures-92"><a href="#multilabel_label_problem_measures-92"><span class="linenos"> 92</span></a><span class="sd">    -------</span>
</span><span id="multilabel_label_problem_measures-93"><a href="#multilabel_label_problem_measures-93"><span class="linenos"> 93</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="multilabel_label_problem_measures-94"><a href="#multilabel_label_problem_measures-94"><span class="linenos"> 94</span></a><span class="sd">        A DataFrame containing all the calculated metrics.</span>
</span><span id="multilabel_label_problem_measures-95"><a href="#multilabel_label_problem_measures-95"><span class="linenos"> 95</span></a>
</span><span id="multilabel_label_problem_measures-96"><a href="#multilabel_label_problem_measures-96"><span class="linenos"> 96</span></a><span class="sd">    Metrics Calculated:</span>
</span><span id="multilabel_label_problem_measures-97"><a href="#multilabel_label_problem_measures-97"><span class="linenos"> 97</span></a><span class="sd">    -------------------</span>
</span><span id="multilabel_label_problem_measures-98"><a href="#multilabel_label_problem_measures-98"><span class="linenos"> 98</span></a><span class="sd">    - Constant Label Problem (CLP)</span>
</span><span id="multilabel_label_problem_measures-99"><a href="#multilabel_label_problem_measures-99"><span class="linenos"> 99</span></a><span class="sd">    - Wrong Label Problem (WLP)</span>
</span><span id="multilabel_label_problem_measures-100"><a href="#multilabel_label_problem_measures-100"><span class="linenos">100</span></a><span class="sd">    - Missing Label Problem (MLP)</span>
</span><span id="multilabel_label_problem_measures-101"><a href="#multilabel_label_problem_measures-101"><span class="linenos">101</span></a>
</span><span id="multilabel_label_problem_measures-102"><a href="#multilabel_label_problem_measures-102"><span class="linenos">102</span></a><span class="sd">    Interpretation:</span>
</span><span id="multilabel_label_problem_measures-103"><a href="#multilabel_label_problem_measures-103"><span class="linenos">103</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_label_problem_measures-104"><a href="#multilabel_label_problem_measures-104"><span class="linenos">104</span></a><span class="sd">    1. **Wrong Label Problem (WLP)**</span>
</span><span id="multilabel_label_problem_measures-105"><a href="#multilabel_label_problem_measures-105"><span class="linenos">105</span></a><span class="sd">        Definition: Measures the number of labels that are predicted but should not be. The ideal value is zero.        </span>
</span><span id="multilabel_label_problem_measures-106"><a href="#multilabel_label_problem_measures-106"><span class="linenos">106</span></a><span class="sd">        - **Low WLP**: Indicates fewer incorrect predictions of labels.</span>
</span><span id="multilabel_label_problem_measures-107"><a href="#multilabel_label_problem_measures-107"><span class="linenos">107</span></a><span class="sd">        - **High WLP**: Indicates that the classifier often predicts incorrect labels.</span>
</span><span id="multilabel_label_problem_measures-108"><a href="#multilabel_label_problem_measures-108"><span class="linenos">108</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="multilabel_label_problem_measures-109"><a href="#multilabel_label_problem_measures-109"><span class="linenos">109</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="multilabel_label_problem_measures-110"><a href="#multilabel_label_problem_measures-110"><span class="linenos">110</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="multilabel_label_problem_measures-111"><a href="#multilabel_label_problem_measures-111"><span class="linenos">111</span></a>
</span><span id="multilabel_label_problem_measures-112"><a href="#multilabel_label_problem_measures-112"><span class="linenos">112</span></a><span class="sd">    2. **Missing Label Problem (MLP)**</span>
</span><span id="multilabel_label_problem_measures-113"><a href="#multilabel_label_problem_measures-113"><span class="linenos">113</span></a><span class="sd">        Definition: Measures the proportion of labels that should have been predicted but were not. The ideal value is zero.        </span>
</span><span id="multilabel_label_problem_measures-114"><a href="#multilabel_label_problem_measures-114"><span class="linenos">114</span></a><span class="sd">        - **Low MLP**: Indicates that most of the relevant labels are predicted.</span>
</span><span id="multilabel_label_problem_measures-115"><a href="#multilabel_label_problem_measures-115"><span class="linenos">115</span></a><span class="sd">        - **High MLP**: Indicates that many relevant labels are missing in the predictions.</span>
</span><span id="multilabel_label_problem_measures-116"><a href="#multilabel_label_problem_measures-116"><span class="linenos">116</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="multilabel_label_problem_measures-117"><a href="#multilabel_label_problem_measures-117"><span class="linenos">117</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="multilabel_label_problem_measures-118"><a href="#multilabel_label_problem_measures-118"><span class="linenos">118</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="multilabel_label_problem_measures-119"><a href="#multilabel_label_problem_measures-119"><span class="linenos">119</span></a>
</span><span id="multilabel_label_problem_measures-120"><a href="#multilabel_label_problem_measures-120"><span class="linenos">120</span></a><span class="sd">    3. **Constant Label Problem (CLP)**</span>
</span><span id="multilabel_label_problem_measures-121"><a href="#multilabel_label_problem_measures-121"><span class="linenos">121</span></a><span class="sd">        Definition: Measures the occurrence where the same label is predicted for all instances. The ideal value is zero.        </span>
</span><span id="multilabel_label_problem_measures-122"><a href="#multilabel_label_problem_measures-122"><span class="linenos">122</span></a><span class="sd">        - **Low CLP**: Indicates that predictions vary and are more closely aligned with true labels.</span>
</span><span id="multilabel_label_problem_measures-123"><a href="#multilabel_label_problem_measures-123"><span class="linenos">123</span></a><span class="sd">        - **High CLP**: Indicates that the classifier predicts the same label for all instances.</span>
</span><span id="multilabel_label_problem_measures-124"><a href="#multilabel_label_problem_measures-124"><span class="linenos">124</span></a><span class="sd">        - **Reference**: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing </span>
</span><span id="multilabel_label_problem_measures-125"><a href="#multilabel_label_problem_measures-125"><span class="linenos">125</span></a><span class="sd">        multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. </span>
</span><span id="multilabel_label_problem_measures-126"><a href="#multilabel_label_problem_measures-126"><span class="linenos">126</span></a><span class="sd">        DOI: 10.1111/exsy.12304</span>
</span><span id="multilabel_label_problem_measures-127"><a href="#multilabel_label_problem_measures-127"><span class="linenos">127</span></a><span class="sd">  </span>
</span><span id="multilabel_label_problem_measures-128"><a href="#multilabel_label_problem_measures-128"><span class="linenos">128</span></a>
</span><span id="multilabel_label_problem_measures-129"><a href="#multilabel_label_problem_measures-129"><span class="linenos">129</span></a><span class="sd">    Example Usage:</span>
</span><span id="multilabel_label_problem_measures-130"><a href="#multilabel_label_problem_measures-130"><span class="linenos">130</span></a><span class="sd">    --------------</span>
</span><span id="multilabel_label_problem_measures-131"><a href="#multilabel_label_problem_measures-131"><span class="linenos">131</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_label_problem_measures(true_labels, pred_labels)</span>
</span><span id="multilabel_label_problem_measures-132"><a href="#multilabel_label_problem_measures-132"><span class="linenos">132</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="multilabel_label_problem_measures-133"><a href="#multilabel_label_problem_measures-133"><span class="linenos">133</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="multilabel_label_problem_measures-134"><a href="#multilabel_label_problem_measures-134"><span class="linenos">134</span></a>
</span><span id="multilabel_label_problem_measures-135"><a href="#multilabel_label_problem_measures-135"><span class="linenos">135</span></a>    <span class="n">matrix_confusion</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">mlem_confusion_matrix</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="multilabel_label_problem_measures-136"><a href="#multilabel_label_problem_measures-136"><span class="linenos">136</span></a>
</span><span id="multilabel_label_problem_measures-137"><a href="#multilabel_label_problem_measures-137"><span class="linenos">137</span></a>    <span class="n">clp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_clp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="multilabel_label_problem_measures-138"><a href="#multilabel_label_problem_measures-138"><span class="linenos">138</span></a>    <span class="n">mlp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_mlp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="multilabel_label_problem_measures-139"><a href="#multilabel_label_problem_measures-139"><span class="linenos">139</span></a>    <span class="n">wlp</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_wlp</span><span class="p">(</span><span class="n">matrix_confusion</span><span class="p">)</span>
</span><span id="multilabel_label_problem_measures-140"><a href="#multilabel_label_problem_measures-140"><span class="linenos">140</span></a>
</span><span id="multilabel_label_problem_measures-141"><a href="#multilabel_label_problem_measures-141"><span class="linenos">141</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="multilabel_label_problem_measures-142"><a href="#multilabel_label_problem_measures-142"><span class="linenos">142</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>    
</span><span id="multilabel_label_problem_measures-143"><a href="#multilabel_label_problem_measures-143"><span class="linenos">143</span></a>        <span class="s1">&#39;clp&#39;</span><span class="p">:</span> <span class="n">clp</span><span class="p">,</span>
</span><span id="multilabel_label_problem_measures-144"><a href="#multilabel_label_problem_measures-144"><span class="linenos">144</span></a>        <span class="s1">&#39;mlp&#39;</span><span class="p">:</span> <span class="n">mlp</span><span class="p">,</span>
</span><span id="multilabel_label_problem_measures-145"><a href="#multilabel_label_problem_measures-145"><span class="linenos">145</span></a>        <span class="s1">&#39;wlp&#39;</span><span class="p">:</span> <span class="n">wlp</span>
</span><span id="multilabel_label_problem_measures-146"><a href="#multilabel_label_problem_measures-146"><span class="linenos">146</span></a>    <span class="p">}</span>
</span><span id="multilabel_label_problem_measures-147"><a href="#multilabel_label_problem_measures-147"><span class="linenos">147</span></a>
</span><span id="multilabel_label_problem_measures-148"><a href="#multilabel_label_problem_measures-148"><span class="linenos">148</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="multilabel_label_problem_measures-149"><a href="#multilabel_label_problem_measures-149"><span class="linenos">149</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="multilabel_label_problem_measures-150"><a href="#multilabel_label_problem_measures-150"><span class="linenos">150</span></a>
</span><span id="multilabel_label_problem_measures-151"><a href="#multilabel_label_problem_measures-151"><span class="linenos">151</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="multilabel_label_problem_measures-152"><a href="#multilabel_label_problem_measures-152"><span class="linenos">152</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="multilabel_label_problem_measures-153"><a href="#multilabel_label_problem_measures-153"><span class="linenos">153</span></a>
</span><span id="multilabel_label_problem_measures-154"><a href="#multilabel_label_problem_measures-154"><span class="linenos">154</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span></pre></div>


            <div class="docstring"><p>Calculates measures for label prediction problems in multi-label classification.</p>

<h2 id="parameters">Parameters:</h2>

<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.
pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</p>

<h2 id="returns">Returns:</h2>

<p>pd.DataFrame
    A DataFrame containing all the calculated metrics.</p>

<h2 id="metrics-calculated">Metrics Calculated:</h2>

<ul>
<li>Constant Label Problem (CLP)</li>
<li>Wrong Label Problem (WLP)</li>
<li>Missing Label Problem (MLP)</li>
</ul>

<h2 id="interpretation">Interpretation:</h2>

<ol>
<li><p><strong>Wrong Label Problem (WLP)</strong>
Definition: Measures the number of labels that are predicted but should not be. The ideal value is zero.        </p>

<ul>
<li><strong>Low WLP</strong>: Indicates fewer incorrect predictions of labels.</li>
<li><strong>High WLP</strong>: Indicates that the classifier often predicts incorrect labels.</li>
<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing 
multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. 
DOI: 10.1111/exsy.12304</li>
</ul></li>
<li><p><strong>Missing Label Problem (MLP)</strong>
Definition: Measures the proportion of labels that should have been predicted but were not. The ideal value is zero.        </p>

<ul>
<li><strong>Low MLP</strong>: Indicates that most of the relevant labels are predicted.</li>
<li><strong>High MLP</strong>: Indicates that many relevant labels are missing in the predictions.</li>
<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing 
multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. 
DOI: 10.1111/exsy.12304</li>
</ul></li>
<li><p><strong>Constant Label Problem (CLP)</strong>
Definition: Measures the occurrence where the same label is predicted for all instances. The ideal value is zero.        </p>

<ul>
<li><strong>Low CLP</strong>: Indicates that predictions vary and are more closely aligned with true labels.</li>
<li><strong>High CLP</strong>: Indicates that the classifier predicts the same label for all instances.</li>
<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing 
multilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. 
DOI: 10.1111/exsy.12304</li>
</ul></li>
</ol>

<h2 id="example-usage">Example Usage:</h2>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">result_df</span> <span class="o">=</span> <span class="n">multilabel_label_problem_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">result_df</span><span class="p">)</span>
</code></pre>
</div>
</div>


                </section>
                <section id="multilabel_bipartition_measures">
                            <input id="multilabel_bipartition_measures-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">multilabel_bipartition_measures</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">true_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>,</span><span class="param">	<span class="n">pred_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span></span><span class="return-annotation">) -> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>:</span></span>

                <label class="view-source-button" for="multilabel_bipartition_measures-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#multilabel_bipartition_measures"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="multilabel_bipartition_measures-161"><a href="#multilabel_bipartition_measures-161"><span class="linenos">161</span></a><span class="k">def</span> <span class="nf">multilabel_bipartition_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="multilabel_bipartition_measures-162"><a href="#multilabel_bipartition_measures-162"><span class="linenos">162</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="multilabel_bipartition_measures-163"><a href="#multilabel_bipartition_measures-163"><span class="linenos">163</span></a><span class="sd">    Calculates various evaluation metrics for multi-label classification.</span>
</span><span id="multilabel_bipartition_measures-164"><a href="#multilabel_bipartition_measures-164"><span class="linenos">164</span></a>
</span><span id="multilabel_bipartition_measures-165"><a href="#multilabel_bipartition_measures-165"><span class="linenos">165</span></a><span class="sd">    Parameters:</span>
</span><span id="multilabel_bipartition_measures-166"><a href="#multilabel_bipartition_measures-166"><span class="linenos">166</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_bipartition_measures-167"><a href="#multilabel_bipartition_measures-167"><span class="linenos">167</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="multilabel_bipartition_measures-168"><a href="#multilabel_bipartition_measures-168"><span class="linenos">168</span></a><span class="sd">    pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</span>
</span><span id="multilabel_bipartition_measures-169"><a href="#multilabel_bipartition_measures-169"><span class="linenos">169</span></a>
</span><span id="multilabel_bipartition_measures-170"><a href="#multilabel_bipartition_measures-170"><span class="linenos">170</span></a><span class="sd">    Returns:</span>
</span><span id="multilabel_bipartition_measures-171"><a href="#multilabel_bipartition_measures-171"><span class="linenos">171</span></a><span class="sd">    -------</span>
</span><span id="multilabel_bipartition_measures-172"><a href="#multilabel_bipartition_measures-172"><span class="linenos">172</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="multilabel_bipartition_measures-173"><a href="#multilabel_bipartition_measures-173"><span class="linenos">173</span></a><span class="sd">        A DataFrame containing all the calculated metrics.</span>
</span><span id="multilabel_bipartition_measures-174"><a href="#multilabel_bipartition_measures-174"><span class="linenos">174</span></a>
</span><span id="multilabel_bipartition_measures-175"><a href="#multilabel_bipartition_measures-175"><span class="linenos">175</span></a><span class="sd">    Metrics Calculated:</span>
</span><span id="multilabel_bipartition_measures-176"><a href="#multilabel_bipartition_measures-176"><span class="linenos">176</span></a><span class="sd">    -------------------</span>
</span><span id="multilabel_bipartition_measures-177"><a href="#multilabel_bipartition_measures-177"><span class="linenos">177</span></a><span class="sd">    - Accuracy</span>
</span><span id="multilabel_bipartition_measures-178"><a href="#multilabel_bipartition_measures-178"><span class="linenos">178</span></a><span class="sd">    - Hamming Loss</span>
</span><span id="multilabel_bipartition_measures-179"><a href="#multilabel_bipartition_measures-179"><span class="linenos">179</span></a><span class="sd">    - Zero-One Loss</span>
</span><span id="multilabel_bipartition_measures-180"><a href="#multilabel_bipartition_measures-180"><span class="linenos">180</span></a><span class="sd">    - F1 Score (macro, micro, weighted, samples)</span>
</span><span id="multilabel_bipartition_measures-181"><a href="#multilabel_bipartition_measures-181"><span class="linenos">181</span></a><span class="sd">    - Precision (macro, micro, weighted, samples)</span>
</span><span id="multilabel_bipartition_measures-182"><a href="#multilabel_bipartition_measures-182"><span class="linenos">182</span></a><span class="sd">    - Recall (macro, micro, weighted, samples)</span>
</span><span id="multilabel_bipartition_measures-183"><a href="#multilabel_bipartition_measures-183"><span class="linenos">183</span></a><span class="sd">    - Precision Recall F1 Support (macro, micro, weighted, samples)</span>
</span><span id="multilabel_bipartition_measures-184"><a href="#multilabel_bipartition_measures-184"><span class="linenos">184</span></a><span class="sd">    - Jaccard Score (macro, micro, weighted, samples)</span>
</span><span id="multilabel_bipartition_measures-185"><a href="#multilabel_bipartition_measures-185"><span class="linenos">185</span></a>
</span><span id="multilabel_bipartition_measures-186"><a href="#multilabel_bipartition_measures-186"><span class="linenos">186</span></a><span class="sd">    Interpretation:</span>
</span><span id="multilabel_bipartition_measures-187"><a href="#multilabel_bipartition_measures-187"><span class="linenos">187</span></a><span class="sd">    ----------------</span>
</span><span id="multilabel_bipartition_measures-188"><a href="#multilabel_bipartition_measures-188"><span class="linenos">188</span></a><span class="sd">    1. **Accuracy**</span>
</span><span id="multilabel_bipartition_measures-189"><a href="#multilabel_bipartition_measures-189"><span class="linenos">189</span></a><span class="sd">        Definition: The proportion of correctly predicted labels (both positive and negative) over </span>
</span><span id="multilabel_bipartition_measures-190"><a href="#multilabel_bipartition_measures-190"><span class="linenos">190</span></a><span class="sd">        the total number of labels.</span>
</span><span id="multilabel_bipartition_measures-191"><a href="#multilabel_bipartition_measures-191"><span class="linenos">191</span></a><span class="sd">        - **High Accuracy**: Indicates that the classifier correctly predicted a high proportion of labels.</span>
</span><span id="multilabel_bipartition_measures-192"><a href="#multilabel_bipartition_measures-192"><span class="linenos">192</span></a><span class="sd">        - **Low Accuracy**: Indicates that the classifier made many incorrect predictions.</span>
</span><span id="multilabel_bipartition_measures-193"><a href="#multilabel_bipartition_measures-193"><span class="linenos">193</span></a><span class="sd">        - **Reference**: [Wikipedia: Accuracy and Precision](https://en.wikipedia.org/wiki/Accuracy_and_precision)</span>
</span><span id="multilabel_bipartition_measures-194"><a href="#multilabel_bipartition_measures-194"><span class="linenos">194</span></a>
</span><span id="multilabel_bipartition_measures-195"><a href="#multilabel_bipartition_measures-195"><span class="linenos">195</span></a><span class="sd">    2. **Hamming Loss**</span>
</span><span id="multilabel_bipartition_measures-196"><a href="#multilabel_bipartition_measures-196"><span class="linenos">196</span></a><span class="sd">        Definition: The fraction of labels that are incorrectly predicted, either due to false</span>
</span><span id="multilabel_bipartition_measures-197"><a href="#multilabel_bipartition_measures-197"><span class="linenos">197</span></a><span class="sd">        positives or false negatives, normalized by the total number of labels.    </span>
</span><span id="multilabel_bipartition_measures-198"><a href="#multilabel_bipartition_measures-198"><span class="linenos">198</span></a><span class="sd">        - **Low Hamming Loss**: Indicates fewer incorrect predictions.</span>
</span><span id="multilabel_bipartition_measures-199"><a href="#multilabel_bipartition_measures-199"><span class="linenos">199</span></a><span class="sd">        - **High Hamming Loss**: Indicates many incorrect predictions.</span>
</span><span id="multilabel_bipartition_measures-200"><a href="#multilabel_bipartition_measures-200"><span class="linenos">200</span></a><span class="sd">        - **Reference**: [Hamming Loss on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html)</span>
</span><span id="multilabel_bipartition_measures-201"><a href="#multilabel_bipartition_measures-201"><span class="linenos">201</span></a>
</span><span id="multilabel_bipartition_measures-202"><a href="#multilabel_bipartition_measures-202"><span class="linenos">202</span></a><span class="sd">    3. **Subset Accuracy**</span>
</span><span id="multilabel_bipartition_measures-203"><a href="#multilabel_bipartition_measures-203"><span class="linenos">203</span></a><span class="sd">        Definition: The proportion of instances for which the classifier predicted all the labels </span>
</span><span id="multilabel_bipartition_measures-204"><a href="#multilabel_bipartition_measures-204"><span class="linenos">204</span></a><span class="sd">        exactly right (i.e., the predicted label set matches the true label set exactly).</span>
</span><span id="multilabel_bipartition_measures-205"><a href="#multilabel_bipartition_measures-205"><span class="linenos">205</span></a><span class="sd">        - **High Subset Accuracy**: Indicates that the classifier correctly predicted all labels for </span>
</span><span id="multilabel_bipartition_measures-206"><a href="#multilabel_bipartition_measures-206"><span class="linenos">206</span></a><span class="sd">          many instances.</span>
</span><span id="multilabel_bipartition_measures-207"><a href="#multilabel_bipartition_measures-207"><span class="linenos">207</span></a><span class="sd">        - **Low Subset Accuracy**: Indicates that the classifier often missed some labels or included </span>
</span><span id="multilabel_bipartition_measures-208"><a href="#multilabel_bipartition_measures-208"><span class="linenos">208</span></a><span class="sd">          incorrect labels.</span>
</span><span id="multilabel_bipartition_measures-209"><a href="#multilabel_bipartition_measures-209"><span class="linenos">209</span></a><span class="sd">        - **Reference**: [Subset Accuracy on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)</span>
</span><span id="multilabel_bipartition_measures-210"><a href="#multilabel_bipartition_measures-210"><span class="linenos">210</span></a>
</span><span id="multilabel_bipartition_measures-211"><a href="#multilabel_bipartition_measures-211"><span class="linenos">211</span></a><span class="sd">    4. **Zero-One Loss**</span>
</span><span id="multilabel_bipartition_measures-212"><a href="#multilabel_bipartition_measures-212"><span class="linenos">212</span></a><span class="sd">        Definition: The fraction of instances where the classifier’s prediction does not match the </span>
</span><span id="multilabel_bipartition_measures-213"><a href="#multilabel_bipartition_measures-213"><span class="linenos">213</span></a><span class="sd">        true label set (i.e., the prediction is not an exact match).    </span>
</span><span id="multilabel_bipartition_measures-214"><a href="#multilabel_bipartition_measures-214"><span class="linenos">214</span></a><span class="sd">        - **Low Zero-One Loss**: Indicates that the classifier makes fewer predictions that do not match </span>
</span><span id="multilabel_bipartition_measures-215"><a href="#multilabel_bipartition_measures-215"><span class="linenos">215</span></a><span class="sd">          the true labels exactly.</span>
</span><span id="multilabel_bipartition_measures-216"><a href="#multilabel_bipartition_measures-216"><span class="linenos">216</span></a><span class="sd">        - **High Zero-One Loss**: Indicates that the classifier often makes incorrect predictions.</span>
</span><span id="multilabel_bipartition_measures-217"><a href="#multilabel_bipartition_measures-217"><span class="linenos">217</span></a><span class="sd">        - **Reference**: [Zero-One Loss on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html)</span>
</span><span id="multilabel_bipartition_measures-218"><a href="#multilabel_bipartition_measures-218"><span class="linenos">218</span></a>
</span><span id="multilabel_bipartition_measures-219"><a href="#multilabel_bipartition_measures-219"><span class="linenos">219</span></a><span class="sd">    5. **Precision (Macro)**</span>
</span><span id="multilabel_bipartition_measures-220"><a href="#multilabel_bipartition_measures-220"><span class="linenos">220</span></a><span class="sd">        Definition: The average precision score calculated for each label independently and then </span>
</span><span id="multilabel_bipartition_measures-221"><a href="#multilabel_bipartition_measures-221"><span class="linenos">221</span></a><span class="sd">        averaged, treating all labels equally.    </span>
</span><span id="multilabel_bipartition_measures-222"><a href="#multilabel_bipartition_measures-222"><span class="linenos">222</span></a><span class="sd">        - **High Macro Precision**: Indicates good performance across all labels individually.</span>
</span><span id="multilabel_bipartition_measures-223"><a href="#multilabel_bipartition_measures-223"><span class="linenos">223</span></a><span class="sd">        - **Low Macro Precision**: Indicates poor performance on some labels.</span>
</span><span id="multilabel_bipartition_measures-224"><a href="#multilabel_bipartition_measures-224"><span class="linenos">224</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="multilabel_bipartition_measures-225"><a href="#multilabel_bipartition_measures-225"><span class="linenos">225</span></a>
</span><span id="multilabel_bipartition_measures-226"><a href="#multilabel_bipartition_measures-226"><span class="linenos">226</span></a><span class="sd">    6. **Precision (Micro)**</span>
</span><span id="multilabel_bipartition_measures-227"><a href="#multilabel_bipartition_measures-227"><span class="linenos">227</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives </span>
</span><span id="multilabel_bipartition_measures-228"><a href="#multilabel_bipartition_measures-228"><span class="linenos">228</span></a><span class="sd">        and false positives, aggregated across all labels.    </span>
</span><span id="multilabel_bipartition_measures-229"><a href="#multilabel_bipartition_measures-229"><span class="linenos">229</span></a><span class="sd">        - **High Micro Precision**: Indicates good overall performance when considering all labels collectively.</span>
</span><span id="multilabel_bipartition_measures-230"><a href="#multilabel_bipartition_measures-230"><span class="linenos">230</span></a><span class="sd">        - **Low Micro Precision**: Indicates many false positives relative to true positives.</span>
</span><span id="multilabel_bipartition_measures-231"><a href="#multilabel_bipartition_measures-231"><span class="linenos">231</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="multilabel_bipartition_measures-232"><a href="#multilabel_bipartition_measures-232"><span class="linenos">232</span></a>
</span><span id="multilabel_bipartition_measures-233"><a href="#multilabel_bipartition_measures-233"><span class="linenos">233</span></a><span class="sd">    7. **Precision (Weighted)**</span>
</span><span id="multilabel_bipartition_measures-234"><a href="#multilabel_bipartition_measures-234"><span class="linenos">234</span></a><span class="sd">        Definition: The precision score calculated for each label, weighted by the number of true </span>
</span><span id="multilabel_bipartition_measures-235"><a href="#multilabel_bipartition_measures-235"><span class="linenos">235</span></a><span class="sd">        instances for each label, and then averaged.</span>
</span><span id="multilabel_bipartition_measures-236"><a href="#multilabel_bipartition_measures-236"><span class="linenos">236</span></a><span class="sd">        - **High Weighted Precision**: Indicates good performance when accounting for the number of </span>
</span><span id="multilabel_bipartition_measures-237"><a href="#multilabel_bipartition_measures-237"><span class="linenos">237</span></a><span class="sd">          instances for each label.</span>
</span><span id="multilabel_bipartition_measures-238"><a href="#multilabel_bipartition_measures-238"><span class="linenos">238</span></a><span class="sd">        - **Low Weighted Precision**: Indicates varying performance across labels.</span>
</span><span id="multilabel_bipartition_measures-239"><a href="#multilabel_bipartition_measures-239"><span class="linenos">239</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="multilabel_bipartition_measures-240"><a href="#multilabel_bipartition_measures-240"><span class="linenos">240</span></a>
</span><span id="multilabel_bipartition_measures-241"><a href="#multilabel_bipartition_measures-241"><span class="linenos">241</span></a><span class="sd">    8. **Precision (Samples)**</span>
</span><span id="multilabel_bipartition_measures-242"><a href="#multilabel_bipartition_measures-242"><span class="linenos">242</span></a><span class="sd">        Definition: The precision score computed for each instance individually, then averaged.    </span>
</span><span id="multilabel_bipartition_measures-243"><a href="#multilabel_bipartition_measures-243"><span class="linenos">243</span></a><span class="sd">        - **High Sample Precision**: Indicates good performance on average across different instances.</span>
</span><span id="multilabel_bipartition_measures-244"><a href="#multilabel_bipartition_measures-244"><span class="linenos">244</span></a><span class="sd">        - **Low Sample Precision**: Indicates that the classifier often makes incorrect predictions for some instances.</span>
</span><span id="multilabel_bipartition_measures-245"><a href="#multilabel_bipartition_measures-245"><span class="linenos">245</span></a><span class="sd">        - **Reference**: [Precision Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)</span>
</span><span id="multilabel_bipartition_measures-246"><a href="#multilabel_bipartition_measures-246"><span class="linenos">246</span></a>
</span><span id="multilabel_bipartition_measures-247"><a href="#multilabel_bipartition_measures-247"><span class="linenos">247</span></a><span class="sd">    9. **Recall (Macro)**</span>
</span><span id="multilabel_bipartition_measures-248"><a href="#multilabel_bipartition_measures-248"><span class="linenos">248</span></a><span class="sd">        Definition: The average recall score calculated for each label independently and then </span>
</span><span id="multilabel_bipartition_measures-249"><a href="#multilabel_bipartition_measures-249"><span class="linenos">249</span></a><span class="sd">        averaged, treating all labels equally.    </span>
</span><span id="multilabel_bipartition_measures-250"><a href="#multilabel_bipartition_measures-250"><span class="linenos">250</span></a><span class="sd">        - **High Macro Recall**: Indicates good identification of relevant labels across all labels individually.</span>
</span><span id="multilabel_bipartition_measures-251"><a href="#multilabel_bipartition_measures-251"><span class="linenos">251</span></a><span class="sd">        - **Low Macro Recall**: Indicates that the classifier misses many relevant labels.</span>
</span><span id="multilabel_bipartition_measures-252"><a href="#multilabel_bipartition_measures-252"><span class="linenos">252</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="multilabel_bipartition_measures-253"><a href="#multilabel_bipartition_measures-253"><span class="linenos">253</span></a>
</span><span id="multilabel_bipartition_measures-254"><a href="#multilabel_bipartition_measures-254"><span class="linenos">254</span></a><span class="sd">    10. **Recall (Micro)**</span>
</span><span id="multilabel_bipartition_measures-255"><a href="#multilabel_bipartition_measures-255"><span class="linenos">255</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives </span>
</span><span id="multilabel_bipartition_measures-256"><a href="#multilabel_bipartition_measures-256"><span class="linenos">256</span></a><span class="sd">        and false negatives, aggregated across all labels.    </span>
</span><span id="multilabel_bipartition_measures-257"><a href="#multilabel_bipartition_measures-257"><span class="linenos">257</span></a><span class="sd">        - **High Micro Recall**: Indicates good overall identification of relevant labels.</span>
</span><span id="multilabel_bipartition_measures-258"><a href="#multilabel_bipartition_measures-258"><span class="linenos">258</span></a><span class="sd">        - **Low Micro Recall**: Indicates that the classifier misses many relevant labels.</span>
</span><span id="multilabel_bipartition_measures-259"><a href="#multilabel_bipartition_measures-259"><span class="linenos">259</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="multilabel_bipartition_measures-260"><a href="#multilabel_bipartition_measures-260"><span class="linenos">260</span></a>
</span><span id="multilabel_bipartition_measures-261"><a href="#multilabel_bipartition_measures-261"><span class="linenos">261</span></a><span class="sd">    11. **Recall (Weighted)**</span>
</span><span id="multilabel_bipartition_measures-262"><a href="#multilabel_bipartition_measures-262"><span class="linenos">262</span></a><span class="sd">        Definition: The recall score calculated for each label, weighted by the number of true </span>
</span><span id="multilabel_bipartition_measures-263"><a href="#multilabel_bipartition_measures-263"><span class="linenos">263</span></a><span class="sd">        instances for each label, and then averaged.    </span>
</span><span id="multilabel_bipartition_measures-264"><a href="#multilabel_bipartition_measures-264"><span class="linenos">264</span></a><span class="sd">        - **High Weighted Recall**: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="multilabel_bipartition_measures-265"><a href="#multilabel_bipartition_measures-265"><span class="linenos">265</span></a><span class="sd">        - **Low Weighted Recall**: Indicates varying performance in identifying relevant labels.</span>
</span><span id="multilabel_bipartition_measures-266"><a href="#multilabel_bipartition_measures-266"><span class="linenos">266</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="multilabel_bipartition_measures-267"><a href="#multilabel_bipartition_measures-267"><span class="linenos">267</span></a>
</span><span id="multilabel_bipartition_measures-268"><a href="#multilabel_bipartition_measures-268"><span class="linenos">268</span></a><span class="sd">    12. **Recall (Samples)**</span>
</span><span id="multilabel_bipartition_measures-269"><a href="#multilabel_bipartition_measures-269"><span class="linenos">269</span></a><span class="sd">        Definition: The recall score computed for each instance individually, then averaged.    </span>
</span><span id="multilabel_bipartition_measures-270"><a href="#multilabel_bipartition_measures-270"><span class="linenos">270</span></a><span class="sd">        - **High Sample Recall**: Indicates good identification of relevant labels on average across instances.</span>
</span><span id="multilabel_bipartition_measures-271"><a href="#multilabel_bipartition_measures-271"><span class="linenos">271</span></a><span class="sd">        - **Low Sample Recall**: Indicates that the classifier misses many relevant labels for some instances.</span>
</span><span id="multilabel_bipartition_measures-272"><a href="#multilabel_bipartition_measures-272"><span class="linenos">272</span></a><span class="sd">        - **Reference**: [Recall Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)</span>
</span><span id="multilabel_bipartition_measures-273"><a href="#multilabel_bipartition_measures-273"><span class="linenos">273</span></a>
</span><span id="multilabel_bipartition_measures-274"><a href="#multilabel_bipartition_measures-274"><span class="linenos">274</span></a><span class="sd">    13. **F1 Score (Macro)**</span>
</span><span id="multilabel_bipartition_measures-275"><a href="#multilabel_bipartition_measures-275"><span class="linenos">275</span></a><span class="sd">        Definition: The average F1 score calculated for each label independently and then averaged, </span>
</span><span id="multilabel_bipartition_measures-276"><a href="#multilabel_bipartition_measures-276"><span class="linenos">276</span></a><span class="sd">        treating all labels equally.    </span>
</span><span id="multilabel_bipartition_measures-277"><a href="#multilabel_bipartition_measures-277"><span class="linenos">277</span></a><span class="sd">        - **High Macro F1**: Indicates a good balance between precision and recall across all labels.</span>
</span><span id="multilabel_bipartition_measures-278"><a href="#multilabel_bipartition_measures-278"><span class="linenos">278</span></a><span class="sd">        - **Low Macro F1**: Indicates poor balance between precision and recall.</span>
</span><span id="multilabel_bipartition_measures-279"><a href="#multilabel_bipartition_measures-279"><span class="linenos">279</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="multilabel_bipartition_measures-280"><a href="#multilabel_bipartition_measures-280"><span class="linenos">280</span></a>
</span><span id="multilabel_bipartition_measures-281"><a href="#multilabel_bipartition_measures-281"><span class="linenos">281</span></a><span class="sd">    14. **F1 Score (Micro)**</span>
</span><span id="multilabel_bipartition_measures-282"><a href="#multilabel_bipartition_measures-282"><span class="linenos">282</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives, </span>
</span><span id="multilabel_bipartition_measures-283"><a href="#multilabel_bipartition_measures-283"><span class="linenos">283</span></a><span class="sd">        false positives, and false negatives, aggregated across all labels.</span>
</span><span id="multilabel_bipartition_measures-284"><a href="#multilabel_bipartition_measures-284"><span class="linenos">284</span></a><span class="sd">        - **High Micro F1**: Indicates good overall balance between precision and recall.</span>
</span><span id="multilabel_bipartition_measures-285"><a href="#multilabel_bipartition_measures-285"><span class="linenos">285</span></a><span class="sd">        - **Low Micro F1**: Indicates poor overall balance between precision and recall.</span>
</span><span id="multilabel_bipartition_measures-286"><a href="#multilabel_bipartition_measures-286"><span class="linenos">286</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="multilabel_bipartition_measures-287"><a href="#multilabel_bipartition_measures-287"><span class="linenos">287</span></a>
</span><span id="multilabel_bipartition_measures-288"><a href="#multilabel_bipartition_measures-288"><span class="linenos">288</span></a><span class="sd">    15. **F1 Score (Weighted)**</span>
</span><span id="multilabel_bipartition_measures-289"><a href="#multilabel_bipartition_measures-289"><span class="linenos">289</span></a><span class="sd">        Definition: The F1 score calculated for each label, weighted by the number of true instances</span>
</span><span id="multilabel_bipartition_measures-290"><a href="#multilabel_bipartition_measures-290"><span class="linenos">290</span></a><span class="sd">          for each label, and then averaged.    </span>
</span><span id="multilabel_bipartition_measures-291"><a href="#multilabel_bipartition_measures-291"><span class="linenos">291</span></a><span class="sd">        - **High Weighted F1**: Indicates good performance considering the number of instances for each label.</span>
</span><span id="multilabel_bipartition_measures-292"><a href="#multilabel_bipartition_measures-292"><span class="linenos">292</span></a><span class="sd">        - **Low Weighted F1**: Indicates varying performance across labels.</span>
</span><span id="multilabel_bipartition_measures-293"><a href="#multilabel_bipartition_measures-293"><span class="linenos">293</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="multilabel_bipartition_measures-294"><a href="#multilabel_bipartition_measures-294"><span class="linenos">294</span></a>
</span><span id="multilabel_bipartition_measures-295"><a href="#multilabel_bipartition_measures-295"><span class="linenos">295</span></a><span class="sd">    16. **F1 Score (Samples)**</span>
</span><span id="multilabel_bipartition_measures-296"><a href="#multilabel_bipartition_measures-296"><span class="linenos">296</span></a><span class="sd">        Definition: The F1 score computed for each instance individually, then averaged.</span>
</span><span id="multilabel_bipartition_measures-297"><a href="#multilabel_bipartition_measures-297"><span class="linenos">297</span></a><span class="sd">        - **High Sample F1**: Indicates good balance between precision and recall for each instance.</span>
</span><span id="multilabel_bipartition_measures-298"><a href="#multilabel_bipartition_measures-298"><span class="linenos">298</span></a><span class="sd">        - **Low Sample F1**: Indicates that the balance between precision and recall varies significantly across instances.</span>
</span><span id="multilabel_bipartition_measures-299"><a href="#multilabel_bipartition_measures-299"><span class="linenos">299</span></a><span class="sd">        - **Reference**: [F1 Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)</span>
</span><span id="multilabel_bipartition_measures-300"><a href="#multilabel_bipartition_measures-300"><span class="linenos">300</span></a>
</span><span id="multilabel_bipartition_measures-301"><a href="#multilabel_bipartition_measures-301"><span class="linenos">301</span></a><span class="sd">    17. **Jaccard Score (Macro)**</span>
</span><span id="multilabel_bipartition_measures-302"><a href="#multilabel_bipartition_measures-302"><span class="linenos">302</span></a><span class="sd">        Definition: The average Jaccard score computed for each label independently and then </span>
</span><span id="multilabel_bipartition_measures-303"><a href="#multilabel_bipartition_measures-303"><span class="linenos">303</span></a><span class="sd">        averaged, treating all labels equally.</span>
</span><span id="multilabel_bipartition_measures-304"><a href="#multilabel_bipartition_measures-304"><span class="linenos">304</span></a><span class="sd">        - **High Macro Jaccard Score**: Indicates good performance across all labels individually.</span>
</span><span id="multilabel_bipartition_measures-305"><a href="#multilabel_bipartition_measures-305"><span class="linenos">305</span></a><span class="sd">        - **Low Macro Jaccard Score**: Indicates poor performance on some labels.</span>
</span><span id="multilabel_bipartition_measures-306"><a href="#multilabel_bipartition_measures-306"><span class="linenos">306</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="multilabel_bipartition_measures-307"><a href="#multilabel_bipartition_measures-307"><span class="linenos">307</span></a>
</span><span id="multilabel_bipartition_measures-308"><a href="#multilabel_bipartition_measures-308"><span class="linenos">308</span></a><span class="sd">    18. **Jaccard Score (Micro)**</span>
</span><span id="multilabel_bipartition_measures-309"><a href="#multilabel_bipartition_measures-309"><span class="linenos">309</span></a><span class="sd">        Definition: The total number of true positives divided by the total number of true positives,</span>
</span><span id="multilabel_bipartition_measures-310"><a href="#multilabel_bipartition_measures-310"><span class="linenos">310</span></a><span class="sd">        false positives, and false negatives, aggregated across all labels.</span>
</span><span id="multilabel_bipartition_measures-311"><a href="#multilabel_bipartition_measures-311"><span class="linenos">311</span></a><span class="sd">        - **High Micro Jaccard Score**: Indicates good overall performance in terms of similarity and diversity.</span>
</span><span id="multilabel_bipartition_measures-312"><a href="#multilabel_bipartition_measures-312"><span class="linenos">312</span></a><span class="sd">        - **Low Micro Jaccard Score**: Indicates poor performance in capturing similarities and differences across labels.</span>
</span><span id="multilabel_bipartition_measures-313"><a href="#multilabel_bipartition_measures-313"><span class="linenos">313</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="multilabel_bipartition_measures-314"><a href="#multilabel_bipartition_measures-314"><span class="linenos">314</span></a>
</span><span id="multilabel_bipartition_measures-315"><a href="#multilabel_bipartition_measures-315"><span class="linenos">315</span></a><span class="sd">    19. **Jaccard Score (Weighted)**</span>
</span><span id="multilabel_bipartition_measures-316"><a href="#multilabel_bipartition_measures-316"><span class="linenos">316</span></a><span class="sd">        Definition: The Jaccard score calculated for each label, weighted by the number of true </span>
</span><span id="multilabel_bipartition_measures-317"><a href="#multilabel_bipartition_measures-317"><span class="linenos">317</span></a><span class="sd">        instances for each label, and then averaged.</span>
</span><span id="multilabel_bipartition_measures-318"><a href="#multilabel_bipartition_measures-318"><span class="linenos">318</span></a><span class="sd">        - **High Weighted Jaccard Score**: Indicates good performance considering the number of instances for each label.</span>
</span><span id="multilabel_bipartition_measures-319"><a href="#multilabel_bipartition_measures-319"><span class="linenos">319</span></a><span class="sd">        - **Low Weighted Jaccard Score**: Indicates varying performance across labels.</span>
</span><span id="multilabel_bipartition_measures-320"><a href="#multilabel_bipartition_measures-320"><span class="linenos">320</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="multilabel_bipartition_measures-321"><a href="#multilabel_bipartition_measures-321"><span class="linenos">321</span></a>
</span><span id="multilabel_bipartition_measures-322"><a href="#multilabel_bipartition_measures-322"><span class="linenos">322</span></a><span class="sd">    20. **Jaccard Score (Samples)**</span>
</span><span id="multilabel_bipartition_measures-323"><a href="#multilabel_bipartition_measures-323"><span class="linenos">323</span></a><span class="sd">        Definition: The Jaccard score computed for each instance individually, then averaged.</span>
</span><span id="multilabel_bipartition_measures-324"><a href="#multilabel_bipartition_measures-324"><span class="linenos">324</span></a><span class="sd">        - **High Sample Jaccard Score**: Indicates good performance on average for each instance.</span>
</span><span id="multilabel_bipartition_measures-325"><a href="#multilabel_bipartition_measures-325"><span class="linenos">325</span></a><span class="sd">        - **Low Sample Jaccard Score**: Indicates varying performance across instances.</span>
</span><span id="multilabel_bipartition_measures-326"><a href="#multilabel_bipartition_measures-326"><span class="linenos">326</span></a><span class="sd">        - **Reference**: [Jaccard Score on scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)</span>
</span><span id="multilabel_bipartition_measures-327"><a href="#multilabel_bipartition_measures-327"><span class="linenos">327</span></a>
</span><span id="multilabel_bipartition_measures-328"><a href="#multilabel_bipartition_measures-328"><span class="linenos">328</span></a><span class="sd">    Example Usage:</span>
</span><span id="multilabel_bipartition_measures-329"><a href="#multilabel_bipartition_measures-329"><span class="linenos">329</span></a><span class="sd">    --------------</span>
</span><span id="multilabel_bipartition_measures-330"><a href="#multilabel_bipartition_measures-330"><span class="linenos">330</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_bipartition_measures(true_labels, pred_labels)</span>
</span><span id="multilabel_bipartition_measures-331"><a href="#multilabel_bipartition_measures-331"><span class="linenos">331</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="multilabel_bipartition_measures-332"><a href="#multilabel_bipartition_measures-332"><span class="linenos">332</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="multilabel_bipartition_measures-333"><a href="#multilabel_bipartition_measures-333"><span class="linenos">333</span></a>
</span><span id="multilabel_bipartition_measures-334"><a href="#multilabel_bipartition_measures-334"><span class="linenos">334</span></a>    <span class="c1"># Basic metrics</span>
</span><span id="multilabel_bipartition_measures-335"><a href="#multilabel_bipartition_measures-335"><span class="linenos">335</span></a>    <span class="n">accuracy_mlem</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_accuracy</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-336"><a href="#multilabel_bipartition_measures-336"><span class="linenos">336</span></a>    <span class="n">hamming_l</span> <span class="o">=</span> <span class="n">hamming_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">true_labels</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred_labels</span><span class="p">))</span>    
</span><span id="multilabel_bipartition_measures-337"><a href="#multilabel_bipartition_measures-337"><span class="linenos">337</span></a>    <span class="n">zol</span> <span class="o">=</span> <span class="n">zero_one_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">true_labels</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred_labels</span><span class="p">))</span>    
</span><span id="multilabel_bipartition_measures-338"><a href="#multilabel_bipartition_measures-338"><span class="linenos">338</span></a>    <span class="n">sa</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_subset_accuracy</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-339"><a href="#multilabel_bipartition_measures-339"><span class="linenos">339</span></a>
</span><span id="multilabel_bipartition_measures-340"><a href="#multilabel_bipartition_measures-340"><span class="linenos">340</span></a>    <span class="c1"># Precision Scores</span>
</span><span id="multilabel_bipartition_measures-341"><a href="#multilabel_bipartition_measures-341"><span class="linenos">341</span></a>    <span class="n">precision_macro</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>    
</span><span id="multilabel_bipartition_measures-342"><a href="#multilabel_bipartition_measures-342"><span class="linenos">342</span></a>    <span class="n">precision_micro</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-343"><a href="#multilabel_bipartition_measures-343"><span class="linenos">343</span></a>    <span class="n">precision_weighted</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-344"><a href="#multilabel_bipartition_measures-344"><span class="linenos">344</span></a>    <span class="n">precision_samples</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-345"><a href="#multilabel_bipartition_measures-345"><span class="linenos">345</span></a>    <span class="n">precision_none</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-346"><a href="#multilabel_bipartition_measures-346"><span class="linenos">346</span></a>    
</span><span id="multilabel_bipartition_measures-347"><a href="#multilabel_bipartition_measures-347"><span class="linenos">347</span></a>    <span class="c1"># Recall Scores</span>
</span><span id="multilabel_bipartition_measures-348"><a href="#multilabel_bipartition_measures-348"><span class="linenos">348</span></a>    <span class="n">recall_macro</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>  
</span><span id="multilabel_bipartition_measures-349"><a href="#multilabel_bipartition_measures-349"><span class="linenos">349</span></a>    <span class="n">recall_micro</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-350"><a href="#multilabel_bipartition_measures-350"><span class="linenos">350</span></a>    <span class="n">recall_weighted</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-351"><a href="#multilabel_bipartition_measures-351"><span class="linenos">351</span></a>    <span class="n">recall_samples</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-352"><a href="#multilabel_bipartition_measures-352"><span class="linenos">352</span></a>    <span class="n">recall_none</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-353"><a href="#multilabel_bipartition_measures-353"><span class="linenos">353</span></a>       
</span><span id="multilabel_bipartition_measures-354"><a href="#multilabel_bipartition_measures-354"><span class="linenos">354</span></a>    <span class="c1"># F1 Scores</span>
</span><span id="multilabel_bipartition_measures-355"><a href="#multilabel_bipartition_measures-355"><span class="linenos">355</span></a>    <span class="n">f1_macro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-356"><a href="#multilabel_bipartition_measures-356"><span class="linenos">356</span></a>    <span class="n">f1_micro</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-357"><a href="#multilabel_bipartition_measures-357"><span class="linenos">357</span></a>    <span class="n">f1_weighted</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-358"><a href="#multilabel_bipartition_measures-358"><span class="linenos">358</span></a>    <span class="n">f1_samples</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-359"><a href="#multilabel_bipartition_measures-359"><span class="linenos">359</span></a>    <span class="n">f1_none</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s1">&#39;warn&#39;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-360"><a href="#multilabel_bipartition_measures-360"><span class="linenos">360</span></a>
</span><span id="multilabel_bipartition_measures-361"><a href="#multilabel_bipartition_measures-361"><span class="linenos">361</span></a>    <span class="c1"># Jaccard Scores</span>
</span><span id="multilabel_bipartition_measures-362"><a href="#multilabel_bipartition_measures-362"><span class="linenos">362</span></a>    <span class="n">jaccard_macro</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-363"><a href="#multilabel_bipartition_measures-363"><span class="linenos">363</span></a>    <span class="n">jaccard_micro</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-364"><a href="#multilabel_bipartition_measures-364"><span class="linenos">364</span></a>    <span class="n">jaccard_weighted</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-365"><a href="#multilabel_bipartition_measures-365"><span class="linenos">365</span></a>    <span class="n">jaccard_samples</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="multilabel_bipartition_measures-366"><a href="#multilabel_bipartition_measures-366"><span class="linenos">366</span></a>    <span class="n">jaccard_none</span> <span class="o">=</span> <span class="n">jaccard_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="multilabel_bipartition_measures-367"><a href="#multilabel_bipartition_measures-367"><span class="linenos">367</span></a>
</span><span id="multilabel_bipartition_measures-368"><a href="#multilabel_bipartition_measures-368"><span class="linenos">368</span></a>    <span class="c1"># Precision, Recall, F1, and Support Scores</span>
</span><span id="multilabel_bipartition_measures-369"><a href="#multilabel_bipartition_measures-369"><span class="linenos">369</span></a>    <span class="n">rpf_macro</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-370"><a href="#multilabel_bipartition_measures-370"><span class="linenos">370</span></a>    <span class="n">rpf_micro</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-371"><a href="#multilabel_bipartition_measures-371"><span class="linenos">371</span></a>    <span class="n">rpf_weighted</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>
</span><span id="multilabel_bipartition_measures-372"><a href="#multilabel_bipartition_measures-372"><span class="linenos">372</span></a>    <span class="n">rpf_samples</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="multilabel_bipartition_measures-373"><a href="#multilabel_bipartition_measures-373"><span class="linenos">373</span></a>    <span class="n">rpf_none</span> <span class="o">=</span> <span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="s2">&quot;warn&quot;</span><span class="p">)</span>    
</span><span id="multilabel_bipartition_measures-374"><a href="#multilabel_bipartition_measures-374"><span class="linenos">374</span></a>
</span><span id="multilabel_bipartition_measures-375"><a href="#multilabel_bipartition_measures-375"><span class="linenos">375</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="multilabel_bipartition_measures-376"><a href="#multilabel_bipartition_measures-376"><span class="linenos">376</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="multilabel_bipartition_measures-377"><a href="#multilabel_bipartition_measures-377"><span class="linenos">377</span></a>        <span class="s1">&#39;accuracy&#39;</span><span class="p">:</span> <span class="n">accuracy_mlem</span><span class="p">,</span>        
</span><span id="multilabel_bipartition_measures-378"><a href="#multilabel_bipartition_measures-378"><span class="linenos">378</span></a>        <span class="s1">&#39;f1_macro&#39;</span><span class="p">:</span> <span class="n">f1_macro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-379"><a href="#multilabel_bipartition_measures-379"><span class="linenos">379</span></a>        <span class="s1">&#39;f1_micro&#39;</span><span class="p">:</span> <span class="n">f1_micro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-380"><a href="#multilabel_bipartition_measures-380"><span class="linenos">380</span></a>        <span class="s1">&#39;f1_weighted&#39;</span><span class="p">:</span> <span class="n">f1_weighted</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-381"><a href="#multilabel_bipartition_measures-381"><span class="linenos">381</span></a>        <span class="s1">&#39;f1_samples&#39;</span><span class="p">:</span> <span class="n">f1_samples</span><span class="p">,</span> 
</span><span id="multilabel_bipartition_measures-382"><a href="#multilabel_bipartition_measures-382"><span class="linenos">382</span></a>        <span class="s1">&#39;hamming_loss&#39;</span><span class="p">:</span> <span class="n">hamming_l</span><span class="p">,</span>              
</span><span id="multilabel_bipartition_measures-383"><a href="#multilabel_bipartition_measures-383"><span class="linenos">383</span></a>        <span class="s1">&#39;jaccard_macro&#39;</span><span class="p">:</span> <span class="n">jaccard_macro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-384"><a href="#multilabel_bipartition_measures-384"><span class="linenos">384</span></a>        <span class="s1">&#39;jaccard_micro&#39;</span><span class="p">:</span> <span class="n">jaccard_micro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-385"><a href="#multilabel_bipartition_measures-385"><span class="linenos">385</span></a>        <span class="s1">&#39;jaccard_weighted&#39;</span><span class="p">:</span> <span class="n">jaccard_weighted</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-386"><a href="#multilabel_bipartition_measures-386"><span class="linenos">386</span></a>        <span class="s1">&#39;jaccard_samples&#39;</span><span class="p">:</span> <span class="n">jaccard_samples</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-387"><a href="#multilabel_bipartition_measures-387"><span class="linenos">387</span></a>        <span class="s1">&#39;precision_macro&#39;</span><span class="p">:</span> <span class="n">precision_macro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-388"><a href="#multilabel_bipartition_measures-388"><span class="linenos">388</span></a>        <span class="s1">&#39;precision_micro&#39;</span><span class="p">:</span> <span class="n">precision_micro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-389"><a href="#multilabel_bipartition_measures-389"><span class="linenos">389</span></a>        <span class="s1">&#39;precision_weighted&#39;</span><span class="p">:</span> <span class="n">precision_weighted</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-390"><a href="#multilabel_bipartition_measures-390"><span class="linenos">390</span></a>        <span class="s1">&#39;precision_samples&#39;</span><span class="p">:</span> <span class="n">precision_samples</span><span class="p">,</span>        
</span><span id="multilabel_bipartition_measures-391"><a href="#multilabel_bipartition_measures-391"><span class="linenos">391</span></a>        <span class="s1">&#39;recall_macro&#39;</span><span class="p">:</span> <span class="n">recall_macro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-392"><a href="#multilabel_bipartition_measures-392"><span class="linenos">392</span></a>        <span class="s1">&#39;recall_micro&#39;</span><span class="p">:</span> <span class="n">recall_micro</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-393"><a href="#multilabel_bipartition_measures-393"><span class="linenos">393</span></a>        <span class="s1">&#39;recall_weighted&#39;</span><span class="p">:</span> <span class="n">recall_weighted</span><span class="p">,</span>
</span><span id="multilabel_bipartition_measures-394"><a href="#multilabel_bipartition_measures-394"><span class="linenos">394</span></a>        <span class="s1">&#39;recall_samples&#39;</span><span class="p">:</span> <span class="n">recall_samples</span><span class="p">,</span>  
</span><span id="multilabel_bipartition_measures-395"><a href="#multilabel_bipartition_measures-395"><span class="linenos">395</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_macro&#39;: rpf_macro,</span>
</span><span id="multilabel_bipartition_measures-396"><a href="#multilabel_bipartition_measures-396"><span class="linenos">396</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_micro&#39;: rpf_micro,</span>
</span><span id="multilabel_bipartition_measures-397"><a href="#multilabel_bipartition_measures-397"><span class="linenos">397</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_weighted&#39;: rpf_weighted,</span>
</span><span id="multilabel_bipartition_measures-398"><a href="#multilabel_bipartition_measures-398"><span class="linenos">398</span></a>        <span class="c1">#&#39;precision_recall_fscore_support_samples&#39;: rpf_samples,</span>
</span><span id="multilabel_bipartition_measures-399"><a href="#multilabel_bipartition_measures-399"><span class="linenos">399</span></a>        <span class="s1">&#39;zero_one_loss&#39;</span><span class="p">:</span> <span class="n">zol</span>     
</span><span id="multilabel_bipartition_measures-400"><a href="#multilabel_bipartition_measures-400"><span class="linenos">400</span></a>    <span class="p">}</span>
</span><span id="multilabel_bipartition_measures-401"><a href="#multilabel_bipartition_measures-401"><span class="linenos">401</span></a>
</span><span id="multilabel_bipartition_measures-402"><a href="#multilabel_bipartition_measures-402"><span class="linenos">402</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="multilabel_bipartition_measures-403"><a href="#multilabel_bipartition_measures-403"><span class="linenos">403</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="multilabel_bipartition_measures-404"><a href="#multilabel_bipartition_measures-404"><span class="linenos">404</span></a>
</span><span id="multilabel_bipartition_measures-405"><a href="#multilabel_bipartition_measures-405"><span class="linenos">405</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="multilabel_bipartition_measures-406"><a href="#multilabel_bipartition_measures-406"><span class="linenos">406</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="multilabel_bipartition_measures-407"><a href="#multilabel_bipartition_measures-407"><span class="linenos">407</span></a>
</span><span id="multilabel_bipartition_measures-408"><a href="#multilabel_bipartition_measures-408"><span class="linenos">408</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span></pre></div>


            <div class="docstring"><p>Calculates various evaluation metrics for multi-label classification.</p>

<h2 id="parameters">Parameters:</h2>

<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.
pred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</p>

<h2 id="returns">Returns:</h2>

<p>pd.DataFrame
    A DataFrame containing all the calculated metrics.</p>

<h2 id="metrics-calculated">Metrics Calculated:</h2>

<ul>
<li>Accuracy</li>
<li>Hamming Loss</li>
<li>Zero-One Loss</li>
<li>F1 Score (macro, micro, weighted, samples)</li>
<li>Precision (macro, micro, weighted, samples)</li>
<li>Recall (macro, micro, weighted, samples)</li>
<li>Precision Recall F1 Support (macro, micro, weighted, samples)</li>
<li>Jaccard Score (macro, micro, weighted, samples)</li>
</ul>

<h2 id="interpretation">Interpretation:</h2>

<ol>
<li><p><strong>Accuracy</strong>
Definition: The proportion of correctly predicted labels (both positive and negative) over 
the total number of labels.</p>

<ul>
<li><strong>High Accuracy</strong>: Indicates that the classifier correctly predicted a high proportion of labels.</li>
<li><strong>Low Accuracy</strong>: Indicates that the classifier made many incorrect predictions.</li>
<li><strong>Reference</strong>: <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">Wikipedia: Accuracy and Precision</a></li>
</ul></li>
<li><p><strong>Hamming Loss</strong>
Definition: The fraction of labels that are incorrectly predicted, either due to false
positives or false negatives, normalized by the total number of labels.    </p>

<ul>
<li><strong>Low Hamming Loss</strong>: Indicates fewer incorrect predictions.</li>
<li><strong>High Hamming Loss</strong>: Indicates many incorrect predictions.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html">Hamming Loss on scikit-learn</a></li>
</ul></li>
<li><p><strong>Subset Accuracy</strong>
Definition: The proportion of instances for which the classifier predicted all the labels 
exactly right (i.e., the predicted label set matches the true label set exactly).</p>

<ul>
<li><strong>High Subset Accuracy</strong>: Indicates that the classifier correctly predicted all labels for 
many instances.</li>
<li><strong>Low Subset Accuracy</strong>: Indicates that the classifier often missed some labels or included 
incorrect labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html">Subset Accuracy on scikit-learn</a></li>
</ul></li>
<li><p><strong>Zero-One Loss</strong>
Definition: The fraction of instances where the classifier’s prediction does not match the 
true label set (i.e., the prediction is not an exact match).    </p>

<ul>
<li><strong>Low Zero-One Loss</strong>: Indicates that the classifier makes fewer predictions that do not match 
the true labels exactly.</li>
<li><strong>High Zero-One Loss</strong>: Indicates that the classifier often makes incorrect predictions.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html">Zero-One Loss on scikit-learn</a></li>
</ul></li>
<li><p><strong>Precision (Macro)</strong>
Definition: The average precision score calculated for each label independently and then 
averaged, treating all labels equally.    </p>

<ul>
<li><strong>High Macro Precision</strong>: Indicates good performance across all labels individually.</li>
<li><strong>Low Macro Precision</strong>: Indicates poor performance on some labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">Precision Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Precision (Micro)</strong>
Definition: The total number of true positives divided by the total number of true positives 
and false positives, aggregated across all labels.    </p>

<ul>
<li><strong>High Micro Precision</strong>: Indicates good overall performance when considering all labels collectively.</li>
<li><strong>Low Micro Precision</strong>: Indicates many false positives relative to true positives.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">Precision Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Precision (Weighted)</strong>
Definition: The precision score calculated for each label, weighted by the number of true 
instances for each label, and then averaged.</p>

<ul>
<li><strong>High Weighted Precision</strong>: Indicates good performance when accounting for the number of 
instances for each label.</li>
<li><strong>Low Weighted Precision</strong>: Indicates varying performance across labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">Precision Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Precision (Samples)</strong>
Definition: The precision score computed for each instance individually, then averaged.    </p>

<ul>
<li><strong>High Sample Precision</strong>: Indicates good performance on average across different instances.</li>
<li><strong>Low Sample Precision</strong>: Indicates that the classifier often makes incorrect predictions for some instances.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html">Precision Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Recall (Macro)</strong>
Definition: The average recall score calculated for each label independently and then 
averaged, treating all labels equally.    </p>

<ul>
<li><strong>High Macro Recall</strong>: Indicates good identification of relevant labels across all labels individually.</li>
<li><strong>Low Macro Recall</strong>: Indicates that the classifier misses many relevant labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">Recall Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Recall (Micro)</strong>
Definition: The total number of true positives divided by the total number of true positives 
and false negatives, aggregated across all labels.    </p>

<ul>
<li><strong>High Micro Recall</strong>: Indicates good overall identification of relevant labels.</li>
<li><strong>Low Micro Recall</strong>: Indicates that the classifier misses many relevant labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">Recall Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Recall (Weighted)</strong>
Definition: The recall score calculated for each label, weighted by the number of true 
instances for each label, and then averaged.    </p>

<ul>
<li><strong>High Weighted Recall</strong>: Indicates good performance when considering the number of instances for each label.</li>
<li><strong>Low Weighted Recall</strong>: Indicates varying performance in identifying relevant labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">Recall Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Recall (Samples)</strong>
Definition: The recall score computed for each instance individually, then averaged.    </p>

<ul>
<li><strong>High Sample Recall</strong>: Indicates good identification of relevant labels on average across instances.</li>
<li><strong>Low Sample Recall</strong>: Indicates that the classifier misses many relevant labels for some instances.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html">Recall Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>F1 Score (Macro)</strong>
Definition: The average F1 score calculated for each label independently and then averaged, 
treating all labels equally.    </p>

<ul>
<li><strong>High Macro F1</strong>: Indicates a good balance between precision and recall across all labels.</li>
<li><strong>Low Macro F1</strong>: Indicates poor balance between precision and recall.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>F1 Score (Micro)</strong>
Definition: The total number of true positives divided by the total number of true positives, 
false positives, and false negatives, aggregated across all labels.</p>

<ul>
<li><strong>High Micro F1</strong>: Indicates good overall balance between precision and recall.</li>
<li><strong>Low Micro F1</strong>: Indicates poor overall balance between precision and recall.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>F1 Score (Weighted)</strong>
Definition: The F1 score calculated for each label, weighted by the number of true instances
  for each label, and then averaged.    </p>

<ul>
<li><strong>High Weighted F1</strong>: Indicates good performance considering the number of instances for each label.</li>
<li><strong>Low Weighted F1</strong>: Indicates varying performance across labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>F1 Score (Samples)</strong>
Definition: The F1 score computed for each instance individually, then averaged.</p>

<ul>
<li><strong>High Sample F1</strong>: Indicates good balance between precision and recall for each instance.</li>
<li><strong>Low Sample F1</strong>: Indicates that the balance between precision and recall varies significantly across instances.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">F1 Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Jaccard Score (Macro)</strong>
Definition: The average Jaccard score computed for each label independently and then 
averaged, treating all labels equally.</p>

<ul>
<li><strong>High Macro Jaccard Score</strong>: Indicates good performance across all labels individually.</li>
<li><strong>Low Macro Jaccard Score</strong>: Indicates poor performance on some labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html">Jaccard Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Jaccard Score (Micro)</strong>
Definition: The total number of true positives divided by the total number of true positives,
false positives, and false negatives, aggregated across all labels.</p>

<ul>
<li><strong>High Micro Jaccard Score</strong>: Indicates good overall performance in terms of similarity and diversity.</li>
<li><strong>Low Micro Jaccard Score</strong>: Indicates poor performance in capturing similarities and differences across labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html">Jaccard Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Jaccard Score (Weighted)</strong>
Definition: The Jaccard score calculated for each label, weighted by the number of true 
instances for each label, and then averaged.</p>

<ul>
<li><strong>High Weighted Jaccard Score</strong>: Indicates good performance considering the number of instances for each label.</li>
<li><strong>Low Weighted Jaccard Score</strong>: Indicates varying performance across labels.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html">Jaccard Score on scikit-learn</a></li>
</ul></li>
<li><p><strong>Jaccard Score (Samples)</strong>
Definition: The Jaccard score computed for each instance individually, then averaged.</p>

<ul>
<li><strong>High Sample Jaccard Score</strong>: Indicates good performance on average for each instance.</li>
<li><strong>Low Sample Jaccard Score</strong>: Indicates varying performance across instances.</li>
<li><strong>Reference</strong>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html">Jaccard Score on scikit-learn</a></li>
</ul></li>
</ol>

<h2 id="example-usage">Example Usage:</h2>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">result_df</span> <span class="o">=</span> <span class="n">multilabel_bipartition_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">result_df</span><span class="p">)</span>
</code></pre>
</div>
</div>


                </section>
                <section id="multilabel_curves_measures">
                            <input id="multilabel_curves_measures-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">multilabel_curves_measures</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">true_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>,</span><span class="param">	<span class="n">pred_scores</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span></span><span class="return-annotation">) -> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>:</span></span>

                <label class="view-source-button" for="multilabel_curves_measures-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#multilabel_curves_measures"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="multilabel_curves_measures-416"><a href="#multilabel_curves_measures-416"><span class="linenos">416</span></a><span class="k">def</span> <span class="nf">multilabel_curves_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="multilabel_curves_measures-417"><a href="#multilabel_curves_measures-417"><span class="linenos">417</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="multilabel_curves_measures-418"><a href="#multilabel_curves_measures-418"><span class="linenos">418</span></a><span class="sd">    Calculates various evaluation metrics related to ranking curves for multi-label classification.</span>
</span><span id="multilabel_curves_measures-419"><a href="#multilabel_curves_measures-419"><span class="linenos">419</span></a>
</span><span id="multilabel_curves_measures-420"><a href="#multilabel_curves_measures-420"><span class="linenos">420</span></a><span class="sd">    Parameters:</span>
</span><span id="multilabel_curves_measures-421"><a href="#multilabel_curves_measures-421"><span class="linenos">421</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_curves_measures-422"><a href="#multilabel_curves_measures-422"><span class="linenos">422</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.</span>
</span><span id="multilabel_curves_measures-423"><a href="#multilabel_curves_measures-423"><span class="linenos">423</span></a><span class="sd">    pred_scores (pd.DataFrame): The DataFrame containing the predicted probabilities for each label.</span>
</span><span id="multilabel_curves_measures-424"><a href="#multilabel_curves_measures-424"><span class="linenos">424</span></a>
</span><span id="multilabel_curves_measures-425"><a href="#multilabel_curves_measures-425"><span class="linenos">425</span></a><span class="sd">    Returns:</span>
</span><span id="multilabel_curves_measures-426"><a href="#multilabel_curves_measures-426"><span class="linenos">426</span></a><span class="sd">    -------</span>
</span><span id="multilabel_curves_measures-427"><a href="#multilabel_curves_measures-427"><span class="linenos">427</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="multilabel_curves_measures-428"><a href="#multilabel_curves_measures-428"><span class="linenos">428</span></a><span class="sd">        A DataFrame containing the computed curve-based metrics.</span>
</span><span id="multilabel_curves_measures-429"><a href="#multilabel_curves_measures-429"><span class="linenos">429</span></a>
</span><span id="multilabel_curves_measures-430"><a href="#multilabel_curves_measures-430"><span class="linenos">430</span></a><span class="sd">    Metrics Computed:</span>
</span><span id="multilabel_curves_measures-431"><a href="#multilabel_curves_measures-431"><span class="linenos">431</span></a><span class="sd">    ------------------</span>
</span><span id="multilabel_curves_measures-432"><a href="#multilabel_curves_measures-432"><span class="linenos">432</span></a><span class="sd">    - Average Precision (AP) Score (Macro, Micro, Weighted, Samples)</span>
</span><span id="multilabel_curves_measures-433"><a href="#multilabel_curves_measures-433"><span class="linenos">433</span></a><span class="sd">    - ROC AUC Score (Macro, Micro, Weighted, Samples)</span>
</span><span id="multilabel_curves_measures-434"><a href="#multilabel_curves_measures-434"><span class="linenos">434</span></a>
</span><span id="multilabel_curves_measures-435"><a href="#multilabel_curves_measures-435"><span class="linenos">435</span></a><span class="sd">    Interpretation:</span>
</span><span id="multilabel_curves_measures-436"><a href="#multilabel_curves_measures-436"><span class="linenos">436</span></a><span class="sd">    ----------------</span>
</span><span id="multilabel_curves_measures-437"><a href="#multilabel_curves_measures-437"><span class="linenos">437</span></a><span class="sd">    1. **Average Precision (AP) Score**</span>
</span><span id="multilabel_curves_measures-438"><a href="#multilabel_curves_measures-438"><span class="linenos">438</span></a><span class="sd">        Definition: Measures the quality of the ranking of predicted probabilities. It summarizes the </span>
</span><span id="multilabel_curves_measures-439"><a href="#multilabel_curves_measures-439"><span class="linenos">439</span></a><span class="sd">        precision-recall curve by calculating the average precision over all instances.</span>
</span><span id="multilabel_curves_measures-440"><a href="#multilabel_curves_measures-440"><span class="linenos">440</span></a><span class="sd">        - **AP Macro**: The average precision score calculated for each label independently and then </span>
</span><span id="multilabel_curves_measures-441"><a href="#multilabel_curves_measures-441"><span class="linenos">441</span></a><span class="sd">          averaged, treating all labels equally.</span>
</span><span id="multilabel_curves_measures-442"><a href="#multilabel_curves_measures-442"><span class="linenos">442</span></a><span class="sd">          - High AP Macro: Indicates good performance across all labels, regardless of class imbalance.</span>
</span><span id="multilabel_curves_measures-443"><a href="#multilabel_curves_measures-443"><span class="linenos">443</span></a><span class="sd">        - **AP Micro**: The average precision score calculated by aggregating the contributions of all labels </span>
</span><span id="multilabel_curves_measures-444"><a href="#multilabel_curves_measures-444"><span class="linenos">444</span></a><span class="sd">          to compute the average precision.</span>
</span><span id="multilabel_curves_measures-445"><a href="#multilabel_curves_measures-445"><span class="linenos">445</span></a><span class="sd">          - High AP Micro: Indicates good overall performance when considering the aggregate precision.</span>
</span><span id="multilabel_curves_measures-446"><a href="#multilabel_curves_measures-446"><span class="linenos">446</span></a><span class="sd">        - **AP Weighted**: The average precision score calculated for each label, weighted by the number of </span>
</span><span id="multilabel_curves_measures-447"><a href="#multilabel_curves_measures-447"><span class="linenos">447</span></a><span class="sd">          true instances for each label, and then averaged.</span>
</span><span id="multilabel_curves_measures-448"><a href="#multilabel_curves_measures-448"><span class="linenos">448</span></a><span class="sd">          - High AP Weighted: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="multilabel_curves_measures-449"><a href="#multilabel_curves_measures-449"><span class="linenos">449</span></a><span class="sd">        - **AP Samples**: The average precision score computed for each instance individually and then averaged.</span>
</span><span id="multilabel_curves_measures-450"><a href="#multilabel_curves_measures-450"><span class="linenos">450</span></a><span class="sd">          - High AP Samples: Indicates good performance on average across different instances.</span>
</span><span id="multilabel_curves_measures-451"><a href="#multilabel_curves_measures-451"><span class="linenos">451</span></a>
</span><span id="multilabel_curves_measures-452"><a href="#multilabel_curves_measures-452"><span class="linenos">452</span></a><span class="sd">    2. **ROC AUC Score**</span>
</span><span id="multilabel_curves_measures-453"><a href="#multilabel_curves_measures-453"><span class="linenos">453</span></a><span class="sd">        Definition: Measures the area under the Receiver Operating Characteristic (ROC) curve, summarizing the </span>
</span><span id="multilabel_curves_measures-454"><a href="#multilabel_curves_measures-454"><span class="linenos">454</span></a><span class="sd">        trade-off between true positive rate and false positive rate.</span>
</span><span id="multilabel_curves_measures-455"><a href="#multilabel_curves_measures-455"><span class="linenos">455</span></a><span class="sd">        - **ROC AUC Macro**: The ROC AUC score calculated for each label independently and then averaged, </span>
</span><span id="multilabel_curves_measures-456"><a href="#multilabel_curves_measures-456"><span class="linenos">456</span></a><span class="sd">          treating all labels equally.</span>
</span><span id="multilabel_curves_measures-457"><a href="#multilabel_curves_measures-457"><span class="linenos">457</span></a><span class="sd">          - High ROC AUC Macro: Indicates good performance across all labels, regardless of class imbalance.</span>
</span><span id="multilabel_curves_measures-458"><a href="#multilabel_curves_measures-458"><span class="linenos">458</span></a><span class="sd">        - **ROC AUC Micro**: The ROC AUC score calculated by aggregating the contributions of all labels to </span>
</span><span id="multilabel_curves_measures-459"><a href="#multilabel_curves_measures-459"><span class="linenos">459</span></a><span class="sd">          compute the average ROC AUC.</span>
</span><span id="multilabel_curves_measures-460"><a href="#multilabel_curves_measures-460"><span class="linenos">460</span></a><span class="sd">          - High ROC AUC Micro: Indicates good overall performance when considering the aggregate true positive </span>
</span><span id="multilabel_curves_measures-461"><a href="#multilabel_curves_measures-461"><span class="linenos">461</span></a><span class="sd">            rate and false positive rate.</span>
</span><span id="multilabel_curves_measures-462"><a href="#multilabel_curves_measures-462"><span class="linenos">462</span></a><span class="sd">        - **ROC AUC Weighted**: The ROC AUC score calculated for each label, weighted by the number of true </span>
</span><span id="multilabel_curves_measures-463"><a href="#multilabel_curves_measures-463"><span class="linenos">463</span></a><span class="sd">          instances for each label, and then averaged.</span>
</span><span id="multilabel_curves_measures-464"><a href="#multilabel_curves_measures-464"><span class="linenos">464</span></a><span class="sd">          - High ROC AUC Weighted: Indicates good performance when considering the number of instances for each label.</span>
</span><span id="multilabel_curves_measures-465"><a href="#multilabel_curves_measures-465"><span class="linenos">465</span></a><span class="sd">        - **ROC AUC Samples**: The ROC AUC score computed for each instance individually and then averaged.</span>
</span><span id="multilabel_curves_measures-466"><a href="#multilabel_curves_measures-466"><span class="linenos">466</span></a><span class="sd">          - High ROC AUC Samples: Indicates good performance on average across different instances.</span>
</span><span id="multilabel_curves_measures-467"><a href="#multilabel_curves_measures-467"><span class="linenos">467</span></a>
</span><span id="multilabel_curves_measures-468"><a href="#multilabel_curves_measures-468"><span class="linenos">468</span></a><span class="sd">    Example Usage:</span>
</span><span id="multilabel_curves_measures-469"><a href="#multilabel_curves_measures-469"><span class="linenos">469</span></a><span class="sd">    --------------</span>
</span><span id="multilabel_curves_measures-470"><a href="#multilabel_curves_measures-470"><span class="linenos">470</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_curves_measures(true_labels, pred_scores)</span>
</span><span id="multilabel_curves_measures-471"><a href="#multilabel_curves_measures-471"><span class="linenos">471</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="multilabel_curves_measures-472"><a href="#multilabel_curves_measures-472"><span class="linenos">472</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="multilabel_curves_measures-473"><a href="#multilabel_curves_measures-473"><span class="linenos">473</span></a>
</span><span id="multilabel_curves_measures-474"><a href="#multilabel_curves_measures-474"><span class="linenos">474</span></a>    <span class="c1"># Average Precision Scores</span>
</span><span id="multilabel_curves_measures-475"><a href="#multilabel_curves_measures-475"><span class="linenos">475</span></a>    <span class="n">average_precision_macro</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-476"><a href="#multilabel_curves_measures-476"><span class="linenos">476</span></a>    <span class="n">average_precision_micro</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-477"><a href="#multilabel_curves_measures-477"><span class="linenos">477</span></a>    <span class="n">average_precision_weighted</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-478"><a href="#multilabel_curves_measures-478"><span class="linenos">478</span></a>    <span class="n">average_precision_samples</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>    
</span><span id="multilabel_curves_measures-479"><a href="#multilabel_curves_measures-479"><span class="linenos">479</span></a>    
</span><span id="multilabel_curves_measures-480"><a href="#multilabel_curves_measures-480"><span class="linenos">480</span></a>    <span class="c1"># ROC AUC Scores</span>
</span><span id="multilabel_curves_measures-481"><a href="#multilabel_curves_measures-481"><span class="linenos">481</span></a>    <span class="n">roc_auc_macro</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-482"><a href="#multilabel_curves_measures-482"><span class="linenos">482</span></a>    <span class="n">roc_auc_micro</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-483"><a href="#multilabel_curves_measures-483"><span class="linenos">483</span></a>    <span class="n">roc_auc_weighted</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
</span><span id="multilabel_curves_measures-484"><a href="#multilabel_curves_measures-484"><span class="linenos">484</span></a>    <span class="n">roc_auc_samples</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>      
</span><span id="multilabel_curves_measures-485"><a href="#multilabel_curves_measures-485"><span class="linenos">485</span></a>
</span><span id="multilabel_curves_measures-486"><a href="#multilabel_curves_measures-486"><span class="linenos">486</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="multilabel_curves_measures-487"><a href="#multilabel_curves_measures-487"><span class="linenos">487</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="multilabel_curves_measures-488"><a href="#multilabel_curves_measures-488"><span class="linenos">488</span></a>        <span class="s1">&#39;auprc_macro&#39;</span><span class="p">:</span> <span class="n">average_precision_macro</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-489"><a href="#multilabel_curves_measures-489"><span class="linenos">489</span></a>        <span class="s1">&#39;auprc_micro&#39;</span><span class="p">:</span> <span class="n">average_precision_micro</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-490"><a href="#multilabel_curves_measures-490"><span class="linenos">490</span></a>        <span class="s1">&#39;auprc_weighted&#39;</span><span class="p">:</span> <span class="n">average_precision_weighted</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-491"><a href="#multilabel_curves_measures-491"><span class="linenos">491</span></a>        <span class="s1">&#39;auprc_samples&#39;</span><span class="p">:</span> <span class="n">average_precision_samples</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-492"><a href="#multilabel_curves_measures-492"><span class="linenos">492</span></a>        <span class="s1">&#39;roc_auc_macro&#39;</span><span class="p">:</span> <span class="n">roc_auc_macro</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-493"><a href="#multilabel_curves_measures-493"><span class="linenos">493</span></a>        <span class="s1">&#39;roc_auc_micro&#39;</span><span class="p">:</span> <span class="n">roc_auc_micro</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-494"><a href="#multilabel_curves_measures-494"><span class="linenos">494</span></a>        <span class="s1">&#39;roc_auc_weighted&#39;</span><span class="p">:</span> <span class="n">roc_auc_weighted</span><span class="p">,</span>
</span><span id="multilabel_curves_measures-495"><a href="#multilabel_curves_measures-495"><span class="linenos">495</span></a>        <span class="s1">&#39;roc_auc_samples&#39;</span><span class="p">:</span> <span class="n">roc_auc_samples</span>
</span><span id="multilabel_curves_measures-496"><a href="#multilabel_curves_measures-496"><span class="linenos">496</span></a>    <span class="p">}</span>
</span><span id="multilabel_curves_measures-497"><a href="#multilabel_curves_measures-497"><span class="linenos">497</span></a>
</span><span id="multilabel_curves_measures-498"><a href="#multilabel_curves_measures-498"><span class="linenos">498</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="multilabel_curves_measures-499"><a href="#multilabel_curves_measures-499"><span class="linenos">499</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="multilabel_curves_measures-500"><a href="#multilabel_curves_measures-500"><span class="linenos">500</span></a>
</span><span id="multilabel_curves_measures-501"><a href="#multilabel_curves_measures-501"><span class="linenos">501</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="multilabel_curves_measures-502"><a href="#multilabel_curves_measures-502"><span class="linenos">502</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="multilabel_curves_measures-503"><a href="#multilabel_curves_measures-503"><span class="linenos">503</span></a>
</span><span id="multilabel_curves_measures-504"><a href="#multilabel_curves_measures-504"><span class="linenos">504</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span></pre></div>


            <div class="docstring"><p>Calculates various evaluation metrics related to ranking curves for multi-label classification.</p>

<h2 id="parameters">Parameters:</h2>

<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.
pred_scores (pd.DataFrame): The DataFrame containing the predicted probabilities for each label.</p>

<h2 id="returns">Returns:</h2>

<p>pd.DataFrame
    A DataFrame containing the computed curve-based metrics.</p>

<h2 id="metrics-computed">Metrics Computed:</h2>

<ul>
<li>Average Precision (AP) Score (Macro, Micro, Weighted, Samples)</li>
<li>ROC AUC Score (Macro, Micro, Weighted, Samples)</li>
</ul>

<h2 id="interpretation">Interpretation:</h2>

<ol>
<li><p><strong>Average Precision (AP) Score</strong>
Definition: Measures the quality of the ranking of predicted probabilities. It summarizes the 
precision-recall curve by calculating the average precision over all instances.</p>

<ul>
<li><strong>AP Macro</strong>: The average precision score calculated for each label independently and then 
averaged, treating all labels equally.
<ul>
<li>High AP Macro: Indicates good performance across all labels, regardless of class imbalance.</li>
</ul></li>
<li><strong>AP Micro</strong>: The average precision score calculated by aggregating the contributions of all labels 
to compute the average precision.
<ul>
<li>High AP Micro: Indicates good overall performance when considering the aggregate precision.</li>
</ul></li>
<li><strong>AP Weighted</strong>: The average precision score calculated for each label, weighted by the number of 
true instances for each label, and then averaged.
<ul>
<li>High AP Weighted: Indicates good performance when considering the number of instances for each label.</li>
</ul></li>
<li><strong>AP Samples</strong>: The average precision score computed for each instance individually and then averaged.
<ul>
<li>High AP Samples: Indicates good performance on average across different instances.</li>
</ul></li>
</ul></li>
<li><p><strong>ROC AUC Score</strong>
Definition: Measures the area under the Receiver Operating Characteristic (ROC) curve, summarizing the 
trade-off between true positive rate and false positive rate.</p>

<ul>
<li><strong>ROC AUC Macro</strong>: The ROC AUC score calculated for each label independently and then averaged, 
treating all labels equally.
<ul>
<li>High ROC AUC Macro: Indicates good performance across all labels, regardless of class imbalance.</li>
</ul></li>
<li><strong>ROC AUC Micro</strong>: The ROC AUC score calculated by aggregating the contributions of all labels to 
compute the average ROC AUC.
<ul>
<li>High ROC AUC Micro: Indicates good overall performance when considering the aggregate true positive 
rate and false positive rate.</li>
</ul></li>
<li><strong>ROC AUC Weighted</strong>: The ROC AUC score calculated for each label, weighted by the number of true 
instances for each label, and then averaged.
<ul>
<li>High ROC AUC Weighted: Indicates good performance when considering the number of instances for each label.</li>
</ul></li>
<li><strong>ROC AUC Samples</strong>: The ROC AUC score computed for each instance individually and then averaged.
<ul>
<li>High ROC AUC Samples: Indicates good performance on average across different instances.</li>
</ul></li>
</ul></li>
</ol>

<h2 id="example-usage">Example Usage:</h2>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">result_df</span> <span class="o">=</span> <span class="n">multilabel_curves_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">result_df</span><span class="p">)</span>
</code></pre>
</div>
</div>


                </section>
                <section id="multilabel_ranking_measures">
                            <input id="multilabel_ranking_measures-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">multilabel_ranking_measures</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">true_labels</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>,</span><span class="param">	<span class="n">pred_scores</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span></span><span class="return-annotation">) -> <span class="n">pandas</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">DataFrame</span>:</span></span>

                <label class="view-source-button" for="multilabel_ranking_measures-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#multilabel_ranking_measures"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="multilabel_ranking_measures-512"><a href="#multilabel_ranking_measures-512"><span class="linenos">512</span></a><span class="k">def</span> <span class="nf">multilabel_ranking_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
</span><span id="multilabel_ranking_measures-513"><a href="#multilabel_ranking_measures-513"><span class="linenos">513</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="multilabel_ranking_measures-514"><a href="#multilabel_ranking_measures-514"><span class="linenos">514</span></a><span class="sd">    Calculates various ranking-based evaluation metrics for multi-label classification.</span>
</span><span id="multilabel_ranking_measures-515"><a href="#multilabel_ranking_measures-515"><span class="linenos">515</span></a>
</span><span id="multilabel_ranking_measures-516"><a href="#multilabel_ranking_measures-516"><span class="linenos">516</span></a><span class="sd">    Parameters:</span>
</span><span id="multilabel_ranking_measures-517"><a href="#multilabel_ranking_measures-517"><span class="linenos">517</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_ranking_measures-518"><a href="#multilabel_ranking_measures-518"><span class="linenos">518</span></a><span class="sd">    true_labels (pd.DataFrame): The DataFrame containing the true binary labels for each instance.</span>
</span><span id="multilabel_ranking_measures-519"><a href="#multilabel_ranking_measures-519"><span class="linenos">519</span></a><span class="sd">    pred_scores (pd.DataFrame): The DataFrame containing the predicted scores for each label.</span>
</span><span id="multilabel_ranking_measures-520"><a href="#multilabel_ranking_measures-520"><span class="linenos">520</span></a>
</span><span id="multilabel_ranking_measures-521"><a href="#multilabel_ranking_measures-521"><span class="linenos">521</span></a><span class="sd">    Returns:</span>
</span><span id="multilabel_ranking_measures-522"><a href="#multilabel_ranking_measures-522"><span class="linenos">522</span></a><span class="sd">    -------</span>
</span><span id="multilabel_ranking_measures-523"><a href="#multilabel_ranking_measures-523"><span class="linenos">523</span></a><span class="sd">    pd.DataFrame</span>
</span><span id="multilabel_ranking_measures-524"><a href="#multilabel_ranking_measures-524"><span class="linenos">524</span></a><span class="sd">        A DataFrame containing the computed ranking-based metrics.</span>
</span><span id="multilabel_ranking_measures-525"><a href="#multilabel_ranking_measures-525"><span class="linenos">525</span></a>
</span><span id="multilabel_ranking_measures-526"><a href="#multilabel_ranking_measures-526"><span class="linenos">526</span></a><span class="sd">    Metrics Computed:</span>
</span><span id="multilabel_ranking_measures-527"><a href="#multilabel_ranking_measures-527"><span class="linenos">527</span></a><span class="sd">    ------------------</span>
</span><span id="multilabel_ranking_measures-528"><a href="#multilabel_ranking_measures-528"><span class="linenos">528</span></a><span class="sd">    - Average Precision</span>
</span><span id="multilabel_ranking_measures-529"><a href="#multilabel_ranking_measures-529"><span class="linenos">529</span></a><span class="sd">    - Coverage Error</span>
</span><span id="multilabel_ranking_measures-530"><a href="#multilabel_ranking_measures-530"><span class="linenos">530</span></a><span class="sd">    - Is Error</span>
</span><span id="multilabel_ranking_measures-531"><a href="#multilabel_ranking_measures-531"><span class="linenos">531</span></a><span class="sd">    - Margin Loss</span>
</span><span id="multilabel_ranking_measures-532"><a href="#multilabel_ranking_measures-532"><span class="linenos">532</span></a><span class="sd">    - Ranking Error</span>
</span><span id="multilabel_ranking_measures-533"><a href="#multilabel_ranking_measures-533"><span class="linenos">533</span></a><span class="sd">    - Ranking Loss</span>
</span><span id="multilabel_ranking_measures-534"><a href="#multilabel_ranking_measures-534"><span class="linenos">534</span></a>
</span><span id="multilabel_ranking_measures-535"><a href="#multilabel_ranking_measures-535"><span class="linenos">535</span></a><span class="sd">    Interpretation:</span>
</span><span id="multilabel_ranking_measures-536"><a href="#multilabel_ranking_measures-536"><span class="linenos">536</span></a><span class="sd">    ----------------</span>
</span><span id="multilabel_ranking_measures-537"><a href="#multilabel_ranking_measures-537"><span class="linenos">537</span></a><span class="sd">    1. **Average Precision**</span>
</span><span id="multilabel_ranking_measures-538"><a href="#multilabel_ranking_measures-538"><span class="linenos">538</span></a><span class="sd">        Definition: Measures the quality of the ranking of predicted labels. It is the average of the </span>
</span><span id="multilabel_ranking_measures-539"><a href="#multilabel_ranking_measures-539"><span class="linenos">539</span></a><span class="sd">        precision scores calculated at each position in the ranked list of predictions, weighted by </span>
</span><span id="multilabel_ranking_measures-540"><a href="#multilabel_ranking_measures-540"><span class="linenos">540</span></a><span class="sd">        the number of relevant items found.</span>
</span><span id="multilabel_ranking_measures-541"><a href="#multilabel_ranking_measures-541"><span class="linenos">541</span></a><span class="sd">        - A value of 1.0 indicates perfect ranking where all relevant labels are ranked above all </span>
</span><span id="multilabel_ranking_measures-542"><a href="#multilabel_ranking_measures-542"><span class="linenos">542</span></a><span class="sd">          irrelevant labels for every instance.</span>
</span><span id="multilabel_ranking_measures-543"><a href="#multilabel_ranking_measures-543"><span class="linenos">543</span></a><span class="sd">        - Lower values indicate that the model is not effectively ranking all relevant labels before </span>
</span><span id="multilabel_ranking_measures-544"><a href="#multilabel_ranking_measures-544"><span class="linenos">544</span></a><span class="sd">          irrelevant ones.</span>
</span><span id="multilabel_ranking_measures-545"><a href="#multilabel_ranking_measures-545"><span class="linenos">545</span></a>
</span><span id="multilabel_ranking_measures-546"><a href="#multilabel_ranking_measures-546"><span class="linenos">546</span></a><span class="sd">    2. **Coverage Error**</span>
</span><span id="multilabel_ranking_measures-547"><a href="#multilabel_ranking_measures-547"><span class="linenos">547</span></a><span class="sd">        Definition: Measures the average number of labels that need to be checked before finding all </span>
</span><span id="multilabel_ranking_measures-548"><a href="#multilabel_ranking_measures-548"><span class="linenos">548</span></a><span class="sd">        relevant labels for each instance.</span>
</span><span id="multilabel_ranking_measures-549"><a href="#multilabel_ranking_measures-549"><span class="linenos">549</span></a><span class="sd">        - A value of 3.5 indicates that, on average, you need to check 3.5 labels to find all relevant </span>
</span><span id="multilabel_ranking_measures-550"><a href="#multilabel_ranking_measures-550"><span class="linenos">550</span></a><span class="sd">          labels.</span>
</span><span id="multilabel_ranking_measures-551"><a href="#multilabel_ranking_measures-551"><span class="linenos">551</span></a><span class="sd">        - Lower values are preferable as they suggest that fewer labels need to be checked to find all </span>
</span><span id="multilabel_ranking_measures-552"><a href="#multilabel_ranking_measures-552"><span class="linenos">552</span></a><span class="sd">          relevant ones, indicating better model performance.</span>
</span><span id="multilabel_ranking_measures-553"><a href="#multilabel_ranking_measures-553"><span class="linenos">553</span></a>
</span><span id="multilabel_ranking_measures-554"><a href="#multilabel_ranking_measures-554"><span class="linenos">554</span></a><span class="sd">    3. **Is Error**</span>
</span><span id="multilabel_ranking_measures-555"><a href="#multilabel_ranking_measures-555"><span class="linenos">555</span></a><span class="sd">        Definition: Indicates whether there is any discrepancy between the predicted ranking and the true </span>
</span><span id="multilabel_ranking_measures-556"><a href="#multilabel_ranking_measures-556"><span class="linenos">556</span></a><span class="sd">        ranking. </span>
</span><span id="multilabel_ranking_measures-557"><a href="#multilabel_ranking_measures-557"><span class="linenos">557</span></a><span class="sd">        - A value of 1.0 suggests that there is an error in the ranking, meaning that the predicted </span>
</span><span id="multilabel_ranking_measures-558"><a href="#multilabel_ranking_measures-558"><span class="linenos">558</span></a><span class="sd">          ranking does not match the true ranking exactly.</span>
</span><span id="multilabel_ranking_measures-559"><a href="#multilabel_ranking_measures-559"><span class="linenos">559</span></a><span class="sd">        - A value of 0.0 indicates that the predicted ranking matches the true ranking exactly.</span>
</span><span id="multilabel_ranking_measures-560"><a href="#multilabel_ranking_measures-560"><span class="linenos">560</span></a>
</span><span id="multilabel_ranking_measures-561"><a href="#multilabel_ranking_measures-561"><span class="linenos">561</span></a><span class="sd">    4. **Margin Loss**</span>
</span><span id="multilabel_ranking_measures-562"><a href="#multilabel_ranking_measures-562"><span class="linenos">562</span></a><span class="sd">        Definition: Measures the average number of positions by which positive labels are ranked below </span>
</span><span id="multilabel_ranking_measures-563"><a href="#multilabel_ranking_measures-563"><span class="linenos">563</span></a><span class="sd">        negative labels. </span>
</span><span id="multilabel_ranking_measures-564"><a href="#multilabel_ranking_measures-564"><span class="linenos">564</span></a><span class="sd">        - A Margin Loss value of 1.25 indicates that, on average, positive labels are ranked 1.25 </span>
</span><span id="multilabel_ranking_measures-565"><a href="#multilabel_ranking_measures-565"><span class="linenos">565</span></a><span class="sd">          positions below negative labels.</span>
</span><span id="multilabel_ranking_measures-566"><a href="#multilabel_ranking_measures-566"><span class="linenos">566</span></a><span class="sd">        - Lower values are preferable as they suggest that positive labels are ranked closer to the top </span>
</span><span id="multilabel_ranking_measures-567"><a href="#multilabel_ranking_measures-567"><span class="linenos">567</span></a><span class="sd">          compared to negative labels.</span>
</span><span id="multilabel_ranking_measures-568"><a href="#multilabel_ranking_measures-568"><span class="linenos">568</span></a>
</span><span id="multilabel_ranking_measures-569"><a href="#multilabel_ranking_measures-569"><span class="linenos">569</span></a><span class="sd">    5. **Ranking Error**</span>
</span><span id="multilabel_ranking_measures-570"><a href="#multilabel_ranking_measures-570"><span class="linenos">570</span></a><span class="sd">        Definition: Calculates the sum of squared differences between the predicted and true rankings. </span>
</span><span id="multilabel_ranking_measures-571"><a href="#multilabel_ranking_measures-571"><span class="linenos">571</span></a><span class="sd">        - A value of 9.5 indicates the total magnitude of the ranking errors.</span>
</span><span id="multilabel_ranking_measures-572"><a href="#multilabel_ranking_measures-572"><span class="linenos">572</span></a><span class="sd">        - Lower values are better, indicating that the predicted ranking is closer to the true ranking.</span>
</span><span id="multilabel_ranking_measures-573"><a href="#multilabel_ranking_measures-573"><span class="linenos">573</span></a>
</span><span id="multilabel_ranking_measures-574"><a href="#multilabel_ranking_measures-574"><span class="linenos">574</span></a><span class="sd">    6. **Ranking Loss**</span>
</span><span id="multilabel_ranking_measures-575"><a href="#multilabel_ranking_measures-575"><span class="linenos">575</span></a><span class="sd">        Definition: Measures the fraction of label pairs where the ranking is incorrect. </span>
</span><span id="multilabel_ranking_measures-576"><a href="#multilabel_ranking_measures-576"><span class="linenos">576</span></a><span class="sd">        - A value of approximately 0.67 indicates that about 67% of label pairs are ranked incorrectly.</span>
</span><span id="multilabel_ranking_measures-577"><a href="#multilabel_ranking_measures-577"><span class="linenos">577</span></a><span class="sd">        - Lower values are preferred, indicating that the majority of label pairs are ranked correctly.</span>
</span><span id="multilabel_ranking_measures-578"><a href="#multilabel_ranking_measures-578"><span class="linenos">578</span></a>
</span><span id="multilabel_ranking_measures-579"><a href="#multilabel_ranking_measures-579"><span class="linenos">579</span></a><span class="sd">    References:</span>
</span><span id="multilabel_ranking_measures-580"><a href="#multilabel_ranking_measures-580"><span class="linenos">580</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_ranking_measures-581"><a href="#multilabel_ranking_measures-581"><span class="linenos">581</span></a><span class="sd">    - The metrics used are commonly referenced in multi-label ranking evaluation literature and libraries.</span>
</span><span id="multilabel_ranking_measures-582"><a href="#multilabel_ranking_measures-582"><span class="linenos">582</span></a><span class="sd">    - For detailed explanations, see the respective methods in the `ms` (multi-label metrics) library </span>
</span><span id="multilabel_ranking_measures-583"><a href="#multilabel_ranking_measures-583"><span class="linenos">583</span></a><span class="sd">    documentation and scikit-learn documentation for `label_ranking_loss` and `coverage_error`.</span>
</span><span id="multilabel_ranking_measures-584"><a href="#multilabel_ranking_measures-584"><span class="linenos">584</span></a>
</span><span id="multilabel_ranking_measures-585"><a href="#multilabel_ranking_measures-585"><span class="linenos">585</span></a><span class="sd">    Examples:</span>
</span><span id="multilabel_ranking_measures-586"><a href="#multilabel_ranking_measures-586"><span class="linenos">586</span></a><span class="sd">    ----------</span>
</span><span id="multilabel_ranking_measures-587"><a href="#multilabel_ranking_measures-587"><span class="linenos">587</span></a><span class="sd">    &gt;&gt;&gt; result_df = multilabel_ranking_measures(true_labels, pred_scores)</span>
</span><span id="multilabel_ranking_measures-588"><a href="#multilabel_ranking_measures-588"><span class="linenos">588</span></a><span class="sd">    &gt;&gt;&gt; print(result_df)</span>
</span><span id="multilabel_ranking_measures-589"><a href="#multilabel_ranking_measures-589"><span class="linenos">589</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="multilabel_ranking_measures-590"><a href="#multilabel_ranking_measures-590"><span class="linenos">590</span></a>    
</span><span id="multilabel_ranking_measures-591"><a href="#multilabel_ranking_measures-591"><span class="linenos">591</span></a>    <span class="c1"># Compute the various ranking metrics</span>
</span><span id="multilabel_ranking_measures-592"><a href="#multilabel_ranking_measures-592"><span class="linenos">592</span></a>    <span class="n">average_precision</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_average_precision</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="multilabel_ranking_measures-593"><a href="#multilabel_ranking_measures-593"><span class="linenos">593</span></a>    <span class="n">precision_atk</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_precision_at_k</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="multilabel_ranking_measures-594"><a href="#multilabel_ranking_measures-594"><span class="linenos">594</span></a>    <span class="n">coverage</span> <span class="o">=</span> <span class="n">coverage_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="multilabel_ranking_measures-595"><a href="#multilabel_ranking_measures-595"><span class="linenos">595</span></a>    <span class="n">iserror</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_is_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
</span><span id="multilabel_ranking_measures-596"><a href="#multilabel_ranking_measures-596"><span class="linenos">596</span></a>    <span class="n">margin_loss</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_margin_loss</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="multilabel_ranking_measures-597"><a href="#multilabel_ranking_measures-597"><span class="linenos">597</span></a>    <span class="n">ranking_error</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">mlem_ranking_error</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="multilabel_ranking_measures-598"><a href="#multilabel_ranking_measures-598"><span class="linenos">598</span></a>    <span class="n">ranking_loss</span> <span class="o">=</span> <span class="n">label_ranking_loss</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>       
</span><span id="multilabel_ranking_measures-599"><a href="#multilabel_ranking_measures-599"><span class="linenos">599</span></a>
</span><span id="multilabel_ranking_measures-600"><a href="#multilabel_ranking_measures-600"><span class="linenos">600</span></a>    <span class="c1"># Store all metrics in a dictionary</span>
</span><span id="multilabel_ranking_measures-601"><a href="#multilabel_ranking_measures-601"><span class="linenos">601</span></a>    <span class="n">metrics_dict</span> <span class="o">=</span> <span class="p">{</span>    
</span><span id="multilabel_ranking_measures-602"><a href="#multilabel_ranking_measures-602"><span class="linenos">602</span></a>        <span class="s1">&#39;average_precision&#39;</span><span class="p">:</span> <span class="n">average_precision</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-603"><a href="#multilabel_ranking_measures-603"><span class="linenos">603</span></a>        <span class="s1">&#39;coverage&#39;</span><span class="p">:</span> <span class="n">coverage</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-604"><a href="#multilabel_ranking_measures-604"><span class="linenos">604</span></a>        <span class="s1">&#39;is_error&#39;</span><span class="p">:</span> <span class="n">iserror</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-605"><a href="#multilabel_ranking_measures-605"><span class="linenos">605</span></a>        <span class="s1">&#39;margin_loss&#39;</span><span class="p">:</span> <span class="n">margin_loss</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-606"><a href="#multilabel_ranking_measures-606"><span class="linenos">606</span></a>        <span class="s1">&#39;precision_atk&#39;</span><span class="p">:</span> <span class="n">precision_atk</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-607"><a href="#multilabel_ranking_measures-607"><span class="linenos">607</span></a>        <span class="s1">&#39;ranking_error&#39;</span><span class="p">:</span> <span class="n">ranking_error</span><span class="p">,</span>
</span><span id="multilabel_ranking_measures-608"><a href="#multilabel_ranking_measures-608"><span class="linenos">608</span></a>        <span class="s1">&#39;ranking_loss&#39;</span><span class="p">:</span> <span class="n">ranking_loss</span>    
</span><span id="multilabel_ranking_measures-609"><a href="#multilabel_ranking_measures-609"><span class="linenos">609</span></a>    <span class="p">}</span>
</span><span id="multilabel_ranking_measures-610"><a href="#multilabel_ranking_measures-610"><span class="linenos">610</span></a>
</span><span id="multilabel_ranking_measures-611"><a href="#multilabel_ranking_measures-611"><span class="linenos">611</span></a>    <span class="c1"># Convert dictionary to DataFrame</span>
</span><span id="multilabel_ranking_measures-612"><a href="#multilabel_ranking_measures-612"><span class="linenos">612</span></a>    <span class="c1"># metrics_df = pd.DataFrame([metrics_dict])</span>
</span><span id="multilabel_ranking_measures-613"><a href="#multilabel_ranking_measures-613"><span class="linenos">613</span></a>    
</span><span id="multilabel_ranking_measures-614"><a href="#multilabel_ranking_measures-614"><span class="linenos">614</span></a>    <span class="c1"># Converter o dicionário em um DataFrame com colunas &quot;Measure&quot; e &quot;Value&quot;</span>
</span><span id="multilabel_ranking_measures-615"><a href="#multilabel_ranking_measures-615"><span class="linenos">615</span></a>    <span class="n">metrics_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">metrics_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Measure&#39;</span><span class="p">,</span> <span class="s1">&#39;Value&#39;</span><span class="p">])</span>
</span><span id="multilabel_ranking_measures-616"><a href="#multilabel_ranking_measures-616"><span class="linenos">616</span></a>
</span><span id="multilabel_ranking_measures-617"><a href="#multilabel_ranking_measures-617"><span class="linenos">617</span></a>    <span class="k">return</span> <span class="n">metrics_df</span>
</span></pre></div>


            <div class="docstring"><p>Calculates various ranking-based evaluation metrics for multi-label classification.</p>

<h2 id="parameters">Parameters:</h2>

<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels for each instance.
pred_scores (pd.DataFrame): The DataFrame containing the predicted scores for each label.</p>

<h2 id="returns">Returns:</h2>

<p>pd.DataFrame
    A DataFrame containing the computed ranking-based metrics.</p>

<h2 id="metrics-computed">Metrics Computed:</h2>

<ul>
<li>Average Precision</li>
<li>Coverage Error</li>
<li>Is Error</li>
<li>Margin Loss</li>
<li>Ranking Error</li>
<li>Ranking Loss</li>
</ul>

<h2 id="interpretation">Interpretation:</h2>

<ol>
<li><p><strong>Average Precision</strong>
Definition: Measures the quality of the ranking of predicted labels. It is the average of the 
precision scores calculated at each position in the ranked list of predictions, weighted by 
the number of relevant items found.</p>

<ul>
<li>A value of 1.0 indicates perfect ranking where all relevant labels are ranked above all 
irrelevant labels for every instance.</li>
<li>Lower values indicate that the model is not effectively ranking all relevant labels before 
irrelevant ones.</li>
</ul></li>
<li><p><strong>Coverage Error</strong>
Definition: Measures the average number of labels that need to be checked before finding all 
relevant labels for each instance.</p>

<ul>
<li>A value of 3.5 indicates that, on average, you need to check 3.5 labels to find all relevant 
labels.</li>
<li>Lower values are preferable as they suggest that fewer labels need to be checked to find all 
relevant ones, indicating better model performance.</li>
</ul></li>
<li><p><strong>Is Error</strong>
Definition: Indicates whether there is any discrepancy between the predicted ranking and the true 
ranking. </p>

<ul>
<li>A value of 1.0 suggests that there is an error in the ranking, meaning that the predicted 
ranking does not match the true ranking exactly.</li>
<li>A value of 0.0 indicates that the predicted ranking matches the true ranking exactly.</li>
</ul></li>
<li><p><strong>Margin Loss</strong>
Definition: Measures the average number of positions by which positive labels are ranked below 
negative labels. </p>

<ul>
<li>A Margin Loss value of 1.25 indicates that, on average, positive labels are ranked 1.25 
positions below negative labels.</li>
<li>Lower values are preferable as they suggest that positive labels are ranked closer to the top 
compared to negative labels.</li>
</ul></li>
<li><p><strong>Ranking Error</strong>
Definition: Calculates the sum of squared differences between the predicted and true rankings. </p>

<ul>
<li>A value of 9.5 indicates the total magnitude of the ranking errors.</li>
<li>Lower values are better, indicating that the predicted ranking is closer to the true ranking.</li>
</ul></li>
<li><p><strong>Ranking Loss</strong>
Definition: Measures the fraction of label pairs where the ranking is incorrect. </p>

<ul>
<li>A value of approximately 0.67 indicates that about 67% of label pairs are ranked incorrectly.</li>
<li>Lower values are preferred, indicating that the majority of label pairs are ranked correctly.</li>
</ul></li>
</ol>

<h2 id="references">References:</h2>

<ul>
<li>The metrics used are commonly referenced in multi-label ranking evaluation literature and libraries.</li>
<li>For detailed explanations, see the respective methods in the <code>ms</code> (multi-label metrics) library 
documentation and scikit-learn documentation for <code>label_ranking_loss</code> and <code>coverage_error</code>.</li>
</ul>

<h2 id="examples">Examples:</h2>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">result_df</span> <span class="o">=</span> <span class="n">multilabel_ranking_measures</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">pred_scores</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">result_df</span><span class="p">)</span>
</code></pre>
</div>
</div>


                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>