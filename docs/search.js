window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "src", "modulename": "src", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.confusion_matrix", "modulename": "src.confusion_matrix", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.confusion_matrix.system", "modulename": "src.confusion_matrix", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Windows&#x27;"}, {"fullname": "src.confusion_matrix.current_directory", "modulename": "src.confusion_matrix", "qualname": "current_directory", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;C:\\\\Users\\\\Cissa\\\\Documents\\\\MultiLabelEvaluationMetrics\\\\src&#x27;"}, {"fullname": "src.confusion_matrix.get_all_measures_names", "modulename": "src.confusion_matrix", "qualname": "get_all_measures_names", "kind": "function", "doc": "<p>Returns a dictionary with hierarchical measure names for multi-label classification.</p>\n\n<p>The dictionary is organized into categories, each containing a list of measure names.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>dict\n    A dictionary with keys representing categories and values as lists of measure names.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">measures</span> <span class=\"o\">=</span> <span class=\"n\">get_all_measures_names</span><span class=\"p\">()</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">measures</span><span class=\"p\">[</span><span class=\"s1\">&#39;macro-based&#39;</span><span class=\"p\">])</span>\n<span class=\"go\">[&#39;macro-auprc&#39;, &#39;macro-F1&#39;, &#39;macro-precision&#39;, &#39;macro-recall&#39;, &#39;macro-jaccard&#39;, &#39;macro-roc_auc&#39;]</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.number_of_instances", "modulename": "src.confusion_matrix", "qualname": "number_of_instances", "kind": "function", "doc": "<p>Returns the total number of instances (rows) in a dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>dataset (pd.DataFrame): The DataFrame for which the number of instances will be counted.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>int: The total number of instances (rows) in the dataset.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;A&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"s1\">&#39;B&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">get_number_of_instances</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"go\">3</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.number_of_labels", "modulename": "src.confusion_matrix", "qualname": "number_of_labels", "kind": "function", "doc": "<p>Returns the total number of labels (columns) in a dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>int: The total number of labels (columns) in the dataset.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">get_number_of_labels</span><span class=\"p\">(</span><span class=\"n\">df_labels</span><span class=\"p\">)</span>\n<span class=\"go\">2</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.positive_instances", "modulename": "src.confusion_matrix", "qualname": "positive_instances", "kind": "function", "doc": "<p>Returns the count of positive instances for each label in a dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>dataset (pd.DataFrame): The DataFrame where each column represents a label and contains binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the count of positive instances for each label (i.e., the sum of each column).</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">get_positive_instances</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.negative_instances", "modulename": "src.confusion_matrix", "qualname": "negative_instances", "kind": "function", "doc": "<p>Returns the count of negative instances for each label in a dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>dataset (pd.DataFrame): The DataFrame where each column represents a label and contains binary values (0 or 1).\npositive_instances (pd.Series): A Series containing the count of positive instances for each label.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the count of negative instances for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">positive_instances</span> <span class=\"o\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sum</span><span class=\"p\">()</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">get_negative_instances</span><span class=\"p\">(</span><span class=\"n\">df</span><span class=\"p\">,</span> <span class=\"n\">positive_instances</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    1</span>\n<span class=\"go\">Label2    1</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span>, </span><span class=\"param\"><span class=\"n\">positive_instances</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_true_1", "modulename": "src.confusion_matrix", "qualname": "mlem_true_1", "kind": "function", "doc": "<p>Returns a DataFrame indicating where the true labels are equal to 1.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame of the same shape as <code>true_labels</code>, where cells with value 1 are retained as 1, and others are set to 0.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_true_labels_equal_to_one</span><span class=\"p\">(</span><span class=\"n\">df_labels</span><span class=\"p\">)</span>\n<span class=\"go\">   Label1  Label2</span>\n<span class=\"go\">0       1       0</span>\n<span class=\"go\">1       0       1</span>\n<span class=\"go\">2       1       1</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_true_0", "modulename": "src.confusion_matrix", "qualname": "mlem_true_0", "kind": "function", "doc": "<p>Returns a DataFrame indicating where the true labels are equal to 0.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame of the same shape as <code>true_labels</code>, where cells with value 0 are retained as 1, and others are set to 0.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_0</span><span class=\"p\">(</span><span class=\"n\">df_labels</span><span class=\"p\">)</span>\n<span class=\"go\">   Label1  Label2</span>\n<span class=\"go\">0       0       1</span>\n<span class=\"go\">1       1       0</span>\n<span class=\"go\">2       0       0</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_pred_1", "modulename": "src.confusion_matrix", "qualname": "mlem_pred_1", "kind": "function", "doc": "<p>Returns a DataFrame indicating where the predicted labels are equal to 1.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>pred_labels (pd.DataFrame): The DataFrame containing the predicted labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame of the same shape as <code>pred_labels</code>, where cells with value 1 are retained as 1, and others are set to 0.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_pred</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_1</span><span class=\"p\">(</span><span class=\"n\">df_pred</span><span class=\"p\">)</span>\n<span class=\"go\">   Label1  Label2</span>\n<span class=\"go\">0       1       0</span>\n<span class=\"go\">1       0       1</span>\n<span class=\"go\">2       1       1</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_pred_0", "modulename": "src.confusion_matrix", "qualname": "mlem_pred_0", "kind": "function", "doc": "<p>Returns a DataFrame indicating where the predicted labels are equal to 0.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>pred_labels (pd.DataFrame): The DataFrame containing the predicted labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame of the same shape as <code>pred_labels</code>, where cells with value 0 are retained as 1, and others are set to 0.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_pred</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_0</span><span class=\"p\">(</span><span class=\"n\">df_pred</span><span class=\"p\">)</span>\n<span class=\"go\">   Label1  Label2</span>\n<span class=\"go\">0       0       1</span>\n<span class=\"go\">1       1       0</span>\n<span class=\"go\">2       0       0</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_total_true_1", "modulename": "src.confusion_matrix", "qualname": "mlem_total_true_1", "kind": "function", "doc": "<p>Returns a Series containing the total count of true labels equal to 1 for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 1, and 0s elsewhere.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the total count of true labels equal to 1 for each label (i.e., the sum of each column).</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_true_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">total_true_1</span><span class=\"p\">(</span><span class=\"n\">df_true_1</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_total_true_0", "modulename": "src.confusion_matrix", "qualname": "mlem_total_true_0", "kind": "function", "doc": "<p>Returns a Series containing the total count of true labels equal to 0 for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 0, and 0s elsewhere.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the total count of true labels equal to 0 for each label (i.e., the sum of each column).</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_true_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">total_true_0</span><span class=\"p\">(</span><span class=\"n\">df_true_0</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_total_pred_1", "modulename": "src.confusion_matrix", "qualname": "mlem_total_pred_1", "kind": "function", "doc": "<p>Returns a Series containing the total count of predicted labels equal to 1 for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>pred_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 1, and 0s elsewhere.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the total count of predicted labels equal to 1 for each label (i.e., the sum of each column).</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_pred_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">total_pred_1</span><span class=\"p\">(</span><span class=\"n\">df_pred_1</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred_1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_total_pred_0", "modulename": "src.confusion_matrix", "qualname": "mlem_total_pred_0", "kind": "function", "doc": "<p>Returns a Series containing the total count of predicted labels equal to 0 for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>pred_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 0, and 0s elsewhere.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series containing the total count of predicted labels equal to 0 for each label (i.e., the sum of each column).</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]}</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">df_pred_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">total_pred_0</span><span class=\"p\">(</span><span class=\"n\">df_pred_0</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred_0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_tpi", "modulename": "src.confusion_matrix", "qualname": "mlem_tpi", "kind": "function", "doc": "<p>Calculate True Positives (TP): The model predicted 1 and the correct response is 1.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 1.\npred_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 1.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame indicating True Positives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_tp</span><span class=\"p\">(</span><span class=\"n\">true_1</span><span class=\"p\">,</span> <span class=\"n\">pred_1</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    1</span>\n<span class=\"go\">Label2    1</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_1</span>, </span><span class=\"param\"><span class=\"n\">pred_1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_tni", "modulename": "src.confusion_matrix", "qualname": "mlem_tni", "kind": "function", "doc": "<p>Calculate True Negatives (TN): The model predicted 0 and the correct response is 0.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 0.\npred_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 0.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame indicating True Negatives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_tn</span><span class=\"p\">(</span><span class=\"n\">true_0</span><span class=\"p\">,</span> <span class=\"n\">pred_0</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    1</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_0</span>, </span><span class=\"param\"><span class=\"n\">pred_0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_fpi", "modulename": "src.confusion_matrix", "qualname": "mlem_fpi", "kind": "function", "doc": "<p>Calculate False Positives (FP): The model predicted 1 and the correct response is 0.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 0.\npred_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 1.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame indicating False Positives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_fp</span><span class=\"p\">(</span><span class=\"n\">true_0</span><span class=\"p\">,</span> <span class=\"n\">pred_1</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    1</span>\n<span class=\"go\">Label2    0</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_0</span>, </span><span class=\"param\"><span class=\"n\">pred_1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_fni", "modulename": "src.confusion_matrix", "qualname": "mlem_fni", "kind": "function", "doc": "<p>Calculate False Negatives (FN): The model predicted 0 and the correct response is 1.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_1 (pd.DataFrame): A DataFrame where cells with value 1 indicate true labels equal to 1.\npred_0 (pd.DataFrame): A DataFrame where cells with value 1 indicate predicted labels equal to 0.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame indicating False Negatives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_1</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_0</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_fn</span><span class=\"p\">(</span><span class=\"n\">true_1</span><span class=\"p\">,</span> <span class=\"n\">pred_0</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    1</span>\n<span class=\"go\">Label2    1</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_1</span>, </span><span class=\"param\"><span class=\"n\">pred_0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_tpl", "modulename": "src.confusion_matrix", "qualname": "mlem_tpl", "kind": "function", "doc": "<p>Calculate the total number of True Positives (TP) for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>TPi (pd.DataFrame): A DataFrame where cells with value 1 indicate True Positives.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series with the total number of True Positives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">TPi</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_tp_totals</span><span class=\"p\">(</span><span class=\"n\">TPi</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">TPi</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_fpl", "modulename": "src.confusion_matrix", "qualname": "mlem_fpl", "kind": "function", "doc": "<p>Calculate the total number of False Positives (FP) for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>FPi (pd.DataFrame): A DataFrame where cells with value 1 indicate False Positives.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series with the total number of False Positives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">FPi</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_fp_totals</span><span class=\"p\">(</span><span class=\"n\">FPi</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">FPi</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_fnl", "modulename": "src.confusion_matrix", "qualname": "mlem_fnl", "kind": "function", "doc": "<p>Calculate the total number of False Negatives (FN) for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>FNi (pd.DataFrame): A DataFrame where cells with value 1 indicate False Negatives.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series with the total number of False Negatives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">FNi</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_fn_totals</span><span class=\"p\">(</span><span class=\"n\">FNi</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">FNi</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_tnl", "modulename": "src.confusion_matrix", "qualname": "mlem_tnl", "kind": "function", "doc": "<p>Calculate the total number of True Negatives (TN) for each label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>TNi (pd.DataFrame): A DataFrame where cells with value 1 indicate True Negatives.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.Series: A Series with the total number of True Negatives for each label.</p>\n\n<p>Example:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">TNi</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span><span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_tn_totals</span><span class=\"p\">(</span><span class=\"n\">TNi</span><span class=\"p\">)</span>\n<span class=\"go\">Label1    2</span>\n<span class=\"go\">Label2    2</span>\n<span class=\"go\">dtype: int64</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">TNi</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.confusion_matrix.mlem_confusion_matrix", "modulename": "src.confusion_matrix", "qualname": "mlem_confusion_matrix", "kind": "function", "doc": "<p>Compute the multi-label confusion matrix and various performance metrics.</p>\n\n<p>This function calculates the confusion matrix components and derived metrics for multi-label classification.\nIt generates a DataFrame containing the counts of True Positives (TP), True Negatives (TN), \nFalse Positives (FP), and False Negatives (FN) for each label. Additionally, it computes the \npercentage of correctly and incorrectly classified labels, and provides totals for columns and rows.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels : pd.DataFrame\n    A DataFrame containing the true binary labels.\npred_labels : pd.DataFrame\n    A DataFrame containing the predicted binary labels.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame\n    A DataFrame with confusion matrix components, percentages, and totals.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span>, </span><span class=\"param\"><span class=\"n\">pred_labels</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.evaluation", "modulename": "src.evaluation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.evaluation.system", "modulename": "src.evaluation", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Windows&#x27;"}, {"fullname": "src.evaluation.current_directory", "modulename": "src.evaluation", "qualname": "current_directory", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;C:\\\\Users\\\\Cissa\\\\Documents\\\\MultiLabelEvaluationMetrics\\\\src&#x27;"}, {"fullname": "src.evaluation.multilabel_label_problem_measures", "modulename": "src.evaluation", "qualname": "multilabel_label_problem_measures", "kind": "function", "doc": "<p>Calculates measures for label prediction problems in multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.\npred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing all the calculated metrics.</p>\n\n<h2 id=\"metrics-calculated\">Metrics Calculated:</h2>\n\n<ul>\n<li>Constant Label Problem (CLP)</li>\n<li>Wrong Label Problem (WLP)</li>\n<li>Missing Label Problem (MLP)</li>\n</ul>\n\n<h2 id=\"interpretation\">Interpretation:</h2>\n\n<ol>\n<li><p><strong>Wrong Label Problem (WLP)</strong>\nDefinition: Measures the number of labels that are predicted but should not be. The ideal value is zero.        </p>\n\n<ul>\n<li><strong>Low WLP</strong>: Indicates fewer incorrect predictions of labels.</li>\n<li><strong>High WLP</strong>: Indicates that the classifier often predicts incorrect labels.</li>\n<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing \nmultilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. \nDOI: 10.1111/exsy.12304</li>\n</ul></li>\n<li><p><strong>Missing Label Problem (MLP)</strong>\nDefinition: Measures the proportion of labels that should have been predicted but were not. The ideal value is zero.        </p>\n\n<ul>\n<li><strong>Low MLP</strong>: Indicates that most of the relevant labels are predicted.</li>\n<li><strong>High MLP</strong>: Indicates that many relevant labels are missing in the predictions.</li>\n<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing \nmultilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. \nDOI: 10.1111/exsy.12304</li>\n</ul></li>\n<li><p><strong>Constant Label Problem (CLP)</strong>\nDefinition: Measures the occurrence where the same label is predicted for all instances. The ideal value is zero.        </p>\n\n<ul>\n<li><strong>Low CLP</strong>: Indicates that predictions vary and are more closely aligned with true labels.</li>\n<li><strong>High CLP</strong>: Indicates that the classifier predicts the same label for all instances.</li>\n<li><strong>Reference</strong>: Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). Enhancing \nmultilabel classification for food truck recommendation. Expert Systems. Wiley-Blackwell. \nDOI: 10.1111/exsy.12304</li>\n</ul></li>\n</ol>\n\n<h2 id=\"example-usage\">Example Usage:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">result_df</span> <span class=\"o\">=</span> <span class=\"n\">multilabel_label_problem_measures</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_labels</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result_df</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.evaluation.multilabel_bipartition_measures", "modulename": "src.evaluation", "qualname": "multilabel_bipartition_measures", "kind": "function", "doc": "<p>Calculates various evaluation metrics for multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.\npred_labels (pd.DataFrame): The DataFrame containing the predicted binary labels (0 or 1) for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing all the calculated metrics.</p>\n\n<h2 id=\"metrics-calculated\">Metrics Calculated:</h2>\n\n<ul>\n<li>Accuracy</li>\n<li>Hamming Loss</li>\n<li>Zero-One Loss</li>\n<li>F1 Score (macro, micro, weighted, samples)</li>\n<li>Precision (macro, micro, weighted, samples)</li>\n<li>Recall (macro, micro, weighted, samples)</li>\n<li>Precision Recall F1 Support (macro, micro, weighted, samples)</li>\n<li>Jaccard Score (macro, micro, weighted, samples)</li>\n</ul>\n\n<h2 id=\"interpretation\">Interpretation:</h2>\n\n<ol>\n<li><p><strong>Accuracy</strong>\nDefinition: The proportion of correctly predicted labels (both positive and negative) over \nthe total number of labels.</p>\n\n<ul>\n<li><strong>High Accuracy</strong>: Indicates that the classifier correctly predicted a high proportion of labels.</li>\n<li><strong>Low Accuracy</strong>: Indicates that the classifier made many incorrect predictions.</li>\n<li><strong>Reference</strong>: <a href=\"https://en.wikipedia.org/wiki/Accuracy_and_precision\">Wikipedia: Accuracy and Precision</a></li>\n</ul></li>\n<li><p><strong>Hamming Loss</strong>\nDefinition: The fraction of labels that are incorrectly predicted, either due to false\npositives or false negatives, normalized by the total number of labels.    </p>\n\n<ul>\n<li><strong>Low Hamming Loss</strong>: Indicates fewer incorrect predictions.</li>\n<li><strong>High Hamming Loss</strong>: Indicates many incorrect predictions.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html\">Hamming Loss on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Subset Accuracy</strong>\nDefinition: The proportion of instances for which the classifier predicted all the labels \nexactly right (i.e., the predicted label set matches the true label set exactly).</p>\n\n<ul>\n<li><strong>High Subset Accuracy</strong>: Indicates that the classifier correctly predicted all labels for \nmany instances.</li>\n<li><strong>Low Subset Accuracy</strong>: Indicates that the classifier often missed some labels or included \nincorrect labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">Subset Accuracy on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Zero-One Loss</strong>\nDefinition: The fraction of instances where the classifier\u2019s prediction does not match the \ntrue label set (i.e., the prediction is not an exact match).    </p>\n\n<ul>\n<li><strong>Low Zero-One Loss</strong>: Indicates that the classifier makes fewer predictions that do not match \nthe true labels exactly.</li>\n<li><strong>High Zero-One Loss</strong>: Indicates that the classifier often makes incorrect predictions.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html\">Zero-One Loss on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Precision (Macro)</strong>\nDefinition: The average precision score calculated for each label independently and then \naveraged, treating all labels equally.    </p>\n\n<ul>\n<li><strong>High Macro Precision</strong>: Indicates good performance across all labels individually.</li>\n<li><strong>Low Macro Precision</strong>: Indicates poor performance on some labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">Precision Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Precision (Micro)</strong>\nDefinition: The total number of true positives divided by the total number of true positives \nand false positives, aggregated across all labels.    </p>\n\n<ul>\n<li><strong>High Micro Precision</strong>: Indicates good overall performance when considering all labels collectively.</li>\n<li><strong>Low Micro Precision</strong>: Indicates many false positives relative to true positives.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">Precision Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Precision (Weighted)</strong>\nDefinition: The precision score calculated for each label, weighted by the number of true \ninstances for each label, and then averaged.</p>\n\n<ul>\n<li><strong>High Weighted Precision</strong>: Indicates good performance when accounting for the number of \ninstances for each label.</li>\n<li><strong>Low Weighted Precision</strong>: Indicates varying performance across labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">Precision Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Precision (Samples)</strong>\nDefinition: The precision score computed for each instance individually, then averaged.    </p>\n\n<ul>\n<li><strong>High Sample Precision</strong>: Indicates good performance on average across different instances.</li>\n<li><strong>Low Sample Precision</strong>: Indicates that the classifier often makes incorrect predictions for some instances.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\">Precision Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Recall (Macro)</strong>\nDefinition: The average recall score calculated for each label independently and then \naveraged, treating all labels equally.    </p>\n\n<ul>\n<li><strong>High Macro Recall</strong>: Indicates good identification of relevant labels across all labels individually.</li>\n<li><strong>Low Macro Recall</strong>: Indicates that the classifier misses many relevant labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\">Recall Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Recall (Micro)</strong>\nDefinition: The total number of true positives divided by the total number of true positives \nand false negatives, aggregated across all labels.    </p>\n\n<ul>\n<li><strong>High Micro Recall</strong>: Indicates good overall identification of relevant labels.</li>\n<li><strong>Low Micro Recall</strong>: Indicates that the classifier misses many relevant labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\">Recall Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Recall (Weighted)</strong>\nDefinition: The recall score calculated for each label, weighted by the number of true \ninstances for each label, and then averaged.    </p>\n\n<ul>\n<li><strong>High Weighted Recall</strong>: Indicates good performance when considering the number of instances for each label.</li>\n<li><strong>Low Weighted Recall</strong>: Indicates varying performance in identifying relevant labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\">Recall Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Recall (Samples)</strong>\nDefinition: The recall score computed for each instance individually, then averaged.    </p>\n\n<ul>\n<li><strong>High Sample Recall</strong>: Indicates good identification of relevant labels on average across instances.</li>\n<li><strong>Low Sample Recall</strong>: Indicates that the classifier misses many relevant labels for some instances.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\">Recall Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>F1 Score (Macro)</strong>\nDefinition: The average F1 score calculated for each label independently and then averaged, \ntreating all labels equally.    </p>\n\n<ul>\n<li><strong>High Macro F1</strong>: Indicates a good balance between precision and recall across all labels.</li>\n<li><strong>Low Macro F1</strong>: Indicates poor balance between precision and recall.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>F1 Score (Micro)</strong>\nDefinition: The total number of true positives divided by the total number of true positives, \nfalse positives, and false negatives, aggregated across all labels.</p>\n\n<ul>\n<li><strong>High Micro F1</strong>: Indicates good overall balance between precision and recall.</li>\n<li><strong>Low Micro F1</strong>: Indicates poor overall balance between precision and recall.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>F1 Score (Weighted)</strong>\nDefinition: The F1 score calculated for each label, weighted by the number of true instances\n  for each label, and then averaged.    </p>\n\n<ul>\n<li><strong>High Weighted F1</strong>: Indicates good performance considering the number of instances for each label.</li>\n<li><strong>Low Weighted F1</strong>: Indicates varying performance across labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>F1 Score (Samples)</strong>\nDefinition: The F1 score computed for each instance individually, then averaged.</p>\n\n<ul>\n<li><strong>High Sample F1</strong>: Indicates good balance between precision and recall for each instance.</li>\n<li><strong>Low Sample F1</strong>: Indicates that the balance between precision and recall varies significantly across instances.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Jaccard Score (Macro)</strong>\nDefinition: The average Jaccard score computed for each label independently and then \naveraged, treating all labels equally.</p>\n\n<ul>\n<li><strong>High Macro Jaccard Score</strong>: Indicates good performance across all labels individually.</li>\n<li><strong>Low Macro Jaccard Score</strong>: Indicates poor performance on some labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\">Jaccard Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Jaccard Score (Micro)</strong>\nDefinition: The total number of true positives divided by the total number of true positives,\nfalse positives, and false negatives, aggregated across all labels.</p>\n\n<ul>\n<li><strong>High Micro Jaccard Score</strong>: Indicates good overall performance in terms of similarity and diversity.</li>\n<li><strong>Low Micro Jaccard Score</strong>: Indicates poor performance in capturing similarities and differences across labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\">Jaccard Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Jaccard Score (Weighted)</strong>\nDefinition: The Jaccard score calculated for each label, weighted by the number of true \ninstances for each label, and then averaged.</p>\n\n<ul>\n<li><strong>High Weighted Jaccard Score</strong>: Indicates good performance considering the number of instances for each label.</li>\n<li><strong>Low Weighted Jaccard Score</strong>: Indicates varying performance across labels.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\">Jaccard Score on scikit-learn</a></li>\n</ul></li>\n<li><p><strong>Jaccard Score (Samples)</strong>\nDefinition: The Jaccard score computed for each instance individually, then averaged.</p>\n\n<ul>\n<li><strong>High Sample Jaccard Score</strong>: Indicates good performance on average for each instance.</li>\n<li><strong>Low Sample Jaccard Score</strong>: Indicates varying performance across instances.</li>\n<li><strong>Reference</strong>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\">Jaccard Score on scikit-learn</a></li>\n</ul></li>\n</ol>\n\n<h2 id=\"example-usage\">Example Usage:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">result_df</span> <span class=\"o\">=</span> <span class=\"n\">multilabel_bipartition_measures</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_labels</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result_df</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.evaluation.multilabel_curves_measures", "modulename": "src.evaluation", "qualname": "multilabel_curves_measures", "kind": "function", "doc": "<p>Calculates various evaluation metrics related to ranking curves for multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels (0 or 1) for each instance.\npred_scores (pd.DataFrame): The DataFrame containing the predicted probabilities for each label.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing the computed curve-based metrics.</p>\n\n<h2 id=\"metrics-computed\">Metrics Computed:</h2>\n\n<ul>\n<li>Average Precision (AP) Score (Macro, Micro, Weighted, Samples)</li>\n<li>ROC AUC Score (Macro, Micro, Weighted, Samples)</li>\n</ul>\n\n<h2 id=\"interpretation\">Interpretation:</h2>\n\n<ol>\n<li><p><strong>Average Precision (AP) Score</strong>\nDefinition: Measures the quality of the ranking of predicted probabilities. It summarizes the \nprecision-recall curve by calculating the average precision over all instances.</p>\n\n<ul>\n<li><strong>AP Macro</strong>: The average precision score calculated for each label independently and then \naveraged, treating all labels equally.\n<ul>\n<li>High AP Macro: Indicates good performance across all labels, regardless of class imbalance.</li>\n</ul></li>\n<li><strong>AP Micro</strong>: The average precision score calculated by aggregating the contributions of all labels \nto compute the average precision.\n<ul>\n<li>High AP Micro: Indicates good overall performance when considering the aggregate precision.</li>\n</ul></li>\n<li><strong>AP Weighted</strong>: The average precision score calculated for each label, weighted by the number of \ntrue instances for each label, and then averaged.\n<ul>\n<li>High AP Weighted: Indicates good performance when considering the number of instances for each label.</li>\n</ul></li>\n<li><strong>AP Samples</strong>: The average precision score computed for each instance individually and then averaged.\n<ul>\n<li>High AP Samples: Indicates good performance on average across different instances.</li>\n</ul></li>\n</ul></li>\n<li><p><strong>ROC AUC Score</strong>\nDefinition: Measures the area under the Receiver Operating Characteristic (ROC) curve, summarizing the \ntrade-off between true positive rate and false positive rate.</p>\n\n<ul>\n<li><strong>ROC AUC Macro</strong>: The ROC AUC score calculated for each label independently and then averaged, \ntreating all labels equally.\n<ul>\n<li>High ROC AUC Macro: Indicates good performance across all labels, regardless of class imbalance.</li>\n</ul></li>\n<li><strong>ROC AUC Micro</strong>: The ROC AUC score calculated by aggregating the contributions of all labels to \ncompute the average ROC AUC.\n<ul>\n<li>High ROC AUC Micro: Indicates good overall performance when considering the aggregate true positive \nrate and false positive rate.</li>\n</ul></li>\n<li><strong>ROC AUC Weighted</strong>: The ROC AUC score calculated for each label, weighted by the number of true \ninstances for each label, and then averaged.\n<ul>\n<li>High ROC AUC Weighted: Indicates good performance when considering the number of instances for each label.</li>\n</ul></li>\n<li><strong>ROC AUC Samples</strong>: The ROC AUC score computed for each instance individually and then averaged.\n<ul>\n<li>High ROC AUC Samples: Indicates good performance on average across different instances.</li>\n</ul></li>\n</ul></li>\n</ol>\n\n<h2 id=\"example-usage\">Example Usage:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">result_df</span> <span class=\"o\">=</span> <span class=\"n\">multilabel_curves_measures</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result_df</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.evaluation.multilabel_ranking_measures", "modulename": "src.evaluation", "qualname": "multilabel_ranking_measures", "kind": "function", "doc": "<p>Calculates various ranking-based evaluation metrics for multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true binary labels for each instance.\npred_scores (pd.DataFrame): The DataFrame containing the predicted scores for each label.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame\n    A DataFrame containing the computed ranking-based metrics.</p>\n\n<h2 id=\"metrics-computed\">Metrics Computed:</h2>\n\n<ul>\n<li>Average Precision</li>\n<li>Coverage Error</li>\n<li>Is Error</li>\n<li>Margin Loss</li>\n<li>Ranking Error</li>\n<li>Ranking Loss</li>\n</ul>\n\n<h2 id=\"interpretation\">Interpretation:</h2>\n\n<ol>\n<li><p><strong>Average Precision</strong>\nDefinition: Measures the quality of the ranking of predicted labels. It is the average of the \nprecision scores calculated at each position in the ranked list of predictions, weighted by \nthe number of relevant items found.</p>\n\n<ul>\n<li>A value of 1.0 indicates perfect ranking where all relevant labels are ranked above all \nirrelevant labels for every instance.</li>\n<li>Lower values indicate that the model is not effectively ranking all relevant labels before \nirrelevant ones.</li>\n</ul></li>\n<li><p><strong>Coverage Error</strong>\nDefinition: Measures the average number of labels that need to be checked before finding all \nrelevant labels for each instance.</p>\n\n<ul>\n<li>A value of 3.5 indicates that, on average, you need to check 3.5 labels to find all relevant \nlabels.</li>\n<li>Lower values are preferable as they suggest that fewer labels need to be checked to find all \nrelevant ones, indicating better model performance.</li>\n</ul></li>\n<li><p><strong>Is Error</strong>\nDefinition: Indicates whether there is any discrepancy between the predicted ranking and the true \nranking. </p>\n\n<ul>\n<li>A value of 1.0 suggests that there is an error in the ranking, meaning that the predicted \nranking does not match the true ranking exactly.</li>\n<li>A value of 0.0 indicates that the predicted ranking matches the true ranking exactly.</li>\n</ul></li>\n<li><p><strong>Margin Loss</strong>\nDefinition: Measures the average number of positions by which positive labels are ranked below \nnegative labels. </p>\n\n<ul>\n<li>A Margin Loss value of 1.25 indicates that, on average, positive labels are ranked 1.25 \npositions below negative labels.</li>\n<li>Lower values are preferable as they suggest that positive labels are ranked closer to the top \ncompared to negative labels.</li>\n</ul></li>\n<li><p><strong>Ranking Error</strong>\nDefinition: Calculates the sum of squared differences between the predicted and true rankings. </p>\n\n<ul>\n<li>A value of 9.5 indicates the total magnitude of the ranking errors.</li>\n<li>Lower values are better, indicating that the predicted ranking is closer to the true ranking.</li>\n</ul></li>\n<li><p><strong>Ranking Loss</strong>\nDefinition: Measures the fraction of label pairs where the ranking is incorrect. </p>\n\n<ul>\n<li>A value of approximately 0.67 indicates that about 67% of label pairs are ranked incorrectly.</li>\n<li>Lower values are preferred, indicating that the majority of label pairs are ranked correctly.</li>\n</ul></li>\n</ol>\n\n<h2 id=\"references\">References:</h2>\n\n<ul>\n<li>The metrics used are commonly referenced in multi-label ranking evaluation literature and libraries.</li>\n<li>For detailed explanations, see the respective methods in the <code>ms</code> (multi-label metrics) library \ndocumentation and scikit-learn documentation for <code>label_ranking_loss</code> and <code>coverage_error</code>.</li>\n</ul>\n\n<h2 id=\"examples\">Examples:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">result_df</span> <span class=\"o\">=</span> <span class=\"n\">multilabel_ranking_measures</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result_df</span><span class=\"p\">)</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.main", "modulename": "src.main", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.main.system", "modulename": "src.main", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Windows&#x27;"}, {"fullname": "src.main.current_directory", "modulename": "src.main", "qualname": "current_directory", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;C:\\\\Users\\\\Cissa\\\\Documents\\\\MultiLabelEvaluationMetrics\\\\src&#x27;"}, {"fullname": "src.measures", "modulename": "src.measures", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "src.measures.system", "modulename": "src.measures", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;Windows&#x27;"}, {"fullname": "src.measures.current_directory", "modulename": "src.measures", "qualname": "current_directory", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;C:\\\\Users\\\\Cissa\\\\Documents\\\\MultiLabelEvaluationMetrics\\\\src&#x27;"}, {"fullname": "src.measures.mlem_accuracy", "modulename": "src.measures", "qualname": "mlem_accuracy", "kind": "function", "doc": "<p>Calculate the accuracy for multi-label classification.</p>\n\n<p>Accuracy is defined as the proportion of correctly predicted labels out of the total number of labels.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true labels with binary values (0 or 1).\npred_labels (pd.DataFrame): The DataFrame containing the predicted labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: Accuracy value.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>[1] Gibaja, E., &amp; Ventura, S. (2015). A Tutorial on Multilabel Learning. \nACM Comput. Surv., 47(3), 52:1-52:38.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_subset_accuracy", "modulename": "src.measures", "qualname": "mlem_subset_accuracy", "kind": "function", "doc": "<p>Calculate the Subset Accuracy (Exact Match Ratio) for multi-label classification.</p>\n\n<p>Subset Accuracy measures the fraction of samples where the predicted labels exactly match the true labels.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): The DataFrame containing the true labels with binary values (0 or 1).\npred_labels (pd.DataFrame): The DataFrame containing the predicted labels with binary values (0 or 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: Subset Accuracy value.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>[1] Zhu, S., Ji, X., Xu, W., &amp; Gong, Y. (2005). Multilabelled Classification \nUsing Maximum Entropy Method. In Proceedings of the 28th. Annual International \nACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'05)\n(pp. 274-281).</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_ranking", "modulename": "src.measures", "qualname": "mlem_ranking", "kind": "function", "doc": "<p>Calculate the ranking of scores based on their values in descending order.\nThe ranking is computed row-wise and is based on the inverse of the scores.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>pred_scores (pd.DataFrame): A DataFrame containing the predicted probabilities for each label.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>pd.DataFrame: A DataFrame with the same shape as <code>scores</code> containing the ranks of the scores.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;Label1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;Label2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;Label3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_ranking</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)</span>\n<span class=\"go\">   Label1  Label2  Label3</span>\n<span class=\"go\">0       1       3       2</span>\n<span class=\"go\">1       3       1       2</span>\n<span class=\"go\">2       1       2       3</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_average_precision", "modulename": "src.measures", "qualname": "mlem_average_precision", "kind": "function", "doc": "<p>Calculate the Average Precision for multi-label classification based on true labels and predicted scores.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): A DataFrame with binary values indicating the true labels for each instance.\npred_scores (pd.DataFrame): A DataFrame with predicted scores for each label for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Average Precision score.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>Tsoumakas, K., et al. (2009). Multi-Label Classification with Label Constraints. \nIn Proceedings of the ECML PKDD 2008 Workshop on Preference Learning (PL-08, Antwerp, Belgium), 157-171.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_scores</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">average_precision</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"go\">0.5833333333333334</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_precision_at_k", "modulename": "src.measures", "qualname": "mlem_precision_at_k", "kind": "function", "doc": "<p>Calculate the Precision at k for multi-label classification based on true labels and predicted scores.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): A DataFrame with binary values indicating the true labels for each instance.\npred_scores (pd.DataFrame): A DataFrame with predicted scores for each label for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Precision at k score.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>Schapire, R. E., &amp; Singer, Y. (2000). BoosTexter: A boosting-based system for text categorization. \nMachine Learning, 39(2), 135-168.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_scores</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">precision_at_k</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"go\">0.6111111111111112</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_is_error", "modulename": "src.measures", "qualname": "mlem_is_error", "kind": "function", "doc": "<p>Calculate the Is Error metric to evaluate if the predicted ranking matches the true ranking.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): A DataFrame containing the true labels for each instance.\npred_scores (pd.DataFrame): A DataFrame containing the predicted scores for each label and instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Is Error metric value.</p>\n\n<h2 id=\"raises\">Raises:</h2>\n\n<p>ValueError: If the <code>true_labels</code> or <code>pred_scores</code> arguments are not provided.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>Crammer, K., &amp; Singer, Y. (2003). A Family of Additive Online Algorithms for Category Ranking. \nJournal of Machine Learning Research, 3(6), 1025-1058.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.compute_mloss_for_instance", "modulename": "src.measures", "qualname": "compute_mloss_for_instance", "kind": "function", "doc": "<p>Compute the Margin Loss for a single instance.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels_row (np.array): Binary array indicating the true labels for a single instance.\npred_ranking_row (np.array): Ranking array indicating the predicted ranks for a single instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Margin Loss for the instance.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels_row</span>, </span><span class=\"param\"><span class=\"n\">pred_ranking_row</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_margin_loss", "modulename": "src.measures", "qualname": "mlem_margin_loss", "kind": "function", "doc": "<p>Calculate the Margin Loss metric for multi-label classification.</p>\n\n<p>The Margin Loss metric quantifies the number of positions between positive and negative labels\nin the ranking. It measures the worst-case ranking difference between the highest-ranked positive\nlabel and the lowest-ranked negative label.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): A DataFrame with binary values indicating the true labels for each instance.\npred_scores (pd.DataFrame): A DataFrame with predicted scores for each label for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Margin Loss metric value.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>Loza Mencia, E., &amp; Furnkranz, J. (2010). Efficient Multilabel Classification Algorithms for Large-Scale Problems in the Legal Domain.\nIn Semantic Processing of Legal Texts (pp. 192-215).</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_scores</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">margin_loss</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"go\">0.5</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_ranking_error", "modulename": "src.measures", "qualname": "mlem_ranking_error", "kind": "function", "doc": "<p>Calculate the Ranking Error (RE) metric for multi-label classification.</p>\n\n<p>The Ranking Error metric measures the sum of squared differences in positions \nof predicted ranks versus true ranks. If the predicted ranking matches the true ranking exactly, RE = 0. \nIf the ranking is completely inverted, RE = 1.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>true_labels (pd.DataFrame): A DataFrame with binary values indicating the true labels for each instance.\npred_scores (pd.DataFrame): A DataFrame with predicted scores for each label for each instance.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float: The Ranking Error metric value.</p>\n\n<h2 id=\"references\">References:</h2>\n\n<p>Park, S.-H., &amp; Furnkranz, J. (2008). Multi-Label Classification with Label Constraints. \nProceedings of the ECML PKDD 2008 Workshop on Preference Learning (PL-08, Antwerp, Belgium), 157-171.</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">true_labels</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">pred_scores</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L1&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L2&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L3&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.7</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;L4&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">ranking_error</span><span class=\"p\">(</span><span class=\"n\">true_labels</span><span class=\"p\">,</span> <span class=\"n\">pred_scores</span><span class=\"p\">)</span>\n<span class=\"go\">0.5</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_labels</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span>,</span><span class=\"param\">\t<span class=\"n\">pred_scores</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_clp", "modulename": "src.measures", "qualname": "mlem_clp", "kind": "function", "doc": "<p>Calculate the Constant Label Problem (CLP) for multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>confusion_matrix : pd.DataFrame\n    DataFrame containing the confusion matrix with columns 'TNL' (True Negatives) \n    and 'FNL' (False Negatives) for each label.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    The CLP score, which represents the proportion of labels where TN + FN == 0.</p>\n\n<h2 id=\"reference\">Reference:</h2>\n\n<p>[1] Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). \n    Enhancing multilabel classification for food truck recommendation. \n    Expert Systems. Wiley-Blackwell. DOI: 10.1111/exsy.12304</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">confusion_matrix</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_clp</span><span class=\"p\">(</span><span class=\"n\">confusion_matrix</span><span class=\"p\">)</span>\n<span class=\"go\">0.5</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">confusion_matrix</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_mlp", "modulename": "src.measures", "qualname": "mlem_mlp", "kind": "function", "doc": "<p>Calculate the Missing Label Prediction (MLP) for multi-label classification.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>confusion_matrix : pd.DataFrame\n    DataFrame containing the confusion matrix with columns 'TPI', 'TNI', 'FPI', 'FNI',\n    'TPL', 'TNL', 'FPL', 'FNL'.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    The MLP score, representing the proportion of labels that are never predicted.</p>\n\n<h2 id=\"reference\">Reference:</h2>\n\n<p>[1] Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). \n    Enhancing multilabel classification for food truck recommendation. \n    Expert Systems. Wiley-Blackwell. DOI: 10.1111/exsy.12304</p>\n\n<p>Example:</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">confusion_matrix</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TPI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TNI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FPI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FNI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TPL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FPL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_mlp</span><span class=\"p\">(</span><span class=\"n\">confusion_matrix</span><span class=\"p\">)</span>\n<span class=\"go\">0.5</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">confusion_matrix</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "src.measures.mlem_wlp", "modulename": "src.measures", "qualname": "mlem_wlp", "kind": "function", "doc": "<p>Calculate the Wrong Label Prediction (WLP) for multi-label classification.</p>\n\n<p>WLP measures when a label may be predicted for some instances, but these predictions are always wrong.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>confusion_matrix : pd.DataFrame\n    DataFrame containing the confusion matrix with columns 'TPI', 'TNI', 'FPI', 'FNI',\n    'TPL', 'TNL', 'FPL', 'FNL'.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    The WLP score, representing the proportion of labels where the predictions are always wrong.</p>\n\n<h2 id=\"reference\">Reference:</h2>\n\n<p>[1] Rivolli, A., Soares, C., &amp; Carvalho, A. C. P. de L. F. de. (2018). \n    Enhancing multilabel classification for food truck recommendation. \n    Expert Systems. Wiley-Blackwell. DOI: 10.1111/exsy.12304</p>\n\n<h2 id=\"example\">Example:</h2>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">confusion_matrix</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">({</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TPI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TNI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FPI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FNI&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TPL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;TNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FPL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span>\n<span class=\"gp\">... </span>    <span class=\"s1\">&#39;FNL&#39;</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">]</span>\n<span class=\"gp\">... </span><span class=\"p\">})</span>\n<span class=\"gp\">&gt;&gt;&gt; </span><span class=\"n\">calculate_wlp</span><span class=\"p\">(</span><span class=\"n\">confusion_matrix</span><span class=\"p\">)</span>\n<span class=\"go\">0.5</span>\n</code></pre>\n</div>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">confusion_matrix</span><span class=\"p\">:</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">core</span><span class=\"o\">.</span><span class=\"n\">frame</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();